{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cuda.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glLml3HaUOZz",
        "outputId": "323b1f55-f16f-46e7-ca13-82851d852b21"
      },
      "source": [
        "!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning git://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-uvtjad9v\n",
            "  Running command git clone -q git://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-uvtjad9v\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4306 sha256=c0f1e1c09d63db02920d6aeec4f225e586315c9e99b302c15a1b4833c845c043\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ziub99_b/wheels/c5/2b/c0/87008e795a14bbcdfc7c846a00d06981916331eb980b6c8bdf\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFuj3mrxjujG",
        "outputId": "440cc5bf-0baf-40a5-e2bb-3dfe54cdc3ec"
      },
      "source": [
        " import torch\n",
        " print('__CUDA Device Name:',torch.cuda.get_device_name(0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__CUDA Device Name: Tesla K80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seknzbneUm7B",
        "outputId": "b05ef6c5-5547-4e13-bb67-b4f40e53b5d0"
      },
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDrYVtdYU0NG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d5f32f6-021c-44e7-9fe5-37fb5b143d5b"
      },
      "source": [
        "%%writefile global.cu\n",
        "//matrix multiplication simple approach\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <assert.h>\n",
        "\n",
        "#define BLOCK_SIZE_X 16\n",
        "#define BLOCK_SIZE_Y 16\n",
        "\n",
        "__global__ void gpu_matrix_mult(int *a,int *b, int *c, int m, int n, int k)\n",
        "{ \n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y; \n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int sum = 0;\n",
        "    if( col < k && row < m)\n",
        "    {\n",
        "        for(int i = 0; i < n; i++) \n",
        "        {\n",
        "            sum += a[row * n + i] * b[i * k + col];\n",
        "        }\n",
        "        c[row * k + col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "void cpu_matrix_mult(int *h_a, int *h_b, int *h_result, int m, int n, int k) {\n",
        "    for (int i = 0; i < m; ++i) \n",
        "    {\n",
        "        for (int j = 0; j < k; ++j) \n",
        "        {\n",
        "            int tmp = 0.0;\n",
        "            for (int h = 0; h < n; ++h) \n",
        "            {\n",
        "                tmp += h_a[i * n + h] * h_b[h * k + j];\n",
        "            }\n",
        "            h_result[i * k + j] = tmp;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(int argc, char const *argv[])\n",
        "{\n",
        "\n",
        "  \n",
        "    int m=8192, n=m, k=m;\n",
        " \n",
        "    srand(3333);\n",
        "\n",
        "    // allocate memory in host RAM, h_cc is used to store CPU result\n",
        "    int *h_a, *h_b, *h_c, *h_cc;\n",
        "    h_a = (int *)malloc((sizeof(int)*m*n)) ;\n",
        "    h_b = (int *)malloc((sizeof(int)*m*n)) ;\n",
        "    h_c = (int *)malloc((sizeof(int)*m*n)) ;\n",
        "    h_cc = (int *)malloc((sizeof(int)*m*n)) ;\n",
        "    float gpu_elapsed_time_ms, cpu_elapsed_time_ms;\n",
        "     // random initialize matrix A\n",
        "    for (int i = 0; i < m; ++i) {\n",
        "        for (int j = 0; j < n; ++j) {\n",
        "            h_a[i * n + j] =random()%4;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // random initialize matrix B\n",
        "    for (int i = 0; i < n; ++i) {\n",
        "        for (int j = 0; j < k; ++j) {\n",
        "            h_b[i * k + j] = random()%4;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // some events to count the execution time\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&stop);\n",
        "cudaEventCreate(&start);\n",
        "    // start to count execution time of GPU version\n",
        "    // Allocate memory space on the device \n",
        "    int *d_a, *d_b, *d_c;\n",
        "    cudaMalloc(&d_a, sizeof(int)*m*n);\n",
        "    cudaMalloc(&d_b, sizeof(int)*n*k);\n",
        "    cudaMalloc(&d_c, sizeof(int)*m*k);\n",
        "\n",
        "    // copy matrix A and B from host to device memory\n",
        "    cudaMemcpy(d_a, h_a, sizeof(int)*m*n, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, h_b, sizeof(int)*n*k, cudaMemcpyHostToDevice);\n",
        "\n",
        "    unsigned int grid_rows = m/BLOCK_SIZE_Y;\n",
        "    unsigned int grid_cols = k/BLOCK_SIZE_X;\n",
        "    dim3 dimGrid(grid_cols, grid_rows);\n",
        "    dim3 dimBlock(BLOCK_SIZE_X, BLOCK_SIZE_Y);\n",
        "   \n",
        "    // Launch kernel \n",
        "   \n",
        "    cudaEventRecord(start, 0);\n",
        "    gpu_matrix_mult<<<dimGrid, dimBlock>>>(d_a, d_b, d_c, m, n, k);    \n",
        " cudaEventRecord(stop,0);\n",
        " cudaEventSynchronize(stop);\n",
        "\n",
        "    // compute time elapse on GPU computing\n",
        "    cudaEventElapsedTime(&gpu_elapsed_time_ms, start, stop);\n",
        "    printf(\"Time elapsed on matrix multiplication of %dx%d . %dx%d on GPU: %f ms.\\n\\n\", m, n, n, k, gpu_elapsed_time_ms);\n",
        "cudaMemcpy(h_c, d_c, sizeof(int)*m*k, cudaMemcpyDeviceToHost);\n",
        "    // start the CPU version \n",
        "    cudaEventRecord(start, 0);\n",
        "    //cpu_matrix_mult(h_a, h_b, h_cc, m, n, k);\n",
        "\n",
        "    cudaEventRecord(stop, 0);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&cpu_elapsed_time_ms, start, stop);\n",
        "    printf(\"Time elapsed on matrix multiplication of %dx%d . %dx%d on CPU: %f ms.\\n\\n\", m, n, n, k, cpu_elapsed_time_ms);\n",
        "\n",
        "\n",
        "    // free memory\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_c);\n",
        "    free(h_a);\n",
        "    free(h_b);\n",
        "    free(h_c);\n",
        "    free(h_cc);\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing global.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxZ1FuqvKDzy",
        "outputId": "e2b6ebfa-8edb-4b6f-e660-2acc4c32d673"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Nov 21 15:41:17 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8    30W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBqTHwzhmAoh"
      },
      "source": [
        "!nvcc -o global global.cu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQcLxlLNmFuo",
        "outputId": "3ad25f18-07d6-4b67-ca62-e48a28d04308"
      },
      "source": [
        "!nvprof ./global"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==162== NVPROF is profiling process 162, command: ./global\n",
            "==162== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "Time elapsed on matrix multiplication of 8192x8192 . 8192x8192 on GPU: 0.003872 ms.\n",
            "\n",
            "Time elapsed on matrix multiplication of 8192x8192 . 8192x8192 on CPU: 0.003872 ms.\n",
            "\n",
            "==162== Profiling application: ./global\n",
            "==162== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   69.18%  174.28ms         1  174.28ms  174.28ms  174.28ms  [CUDA memcpy DtoH]\n",
            "                   30.82%  77.633ms         2  38.816ms  38.518ms  39.115ms  [CUDA memcpy HtoD]\n",
            "      API calls:   45.29%  269.39ms         2  134.69ms  1.6600us  269.38ms  cudaEventCreate\n",
            "                   42.62%  253.48ms         3  84.492ms  38.672ms  175.63ms  cudaMemcpy\n",
            "                   11.54%  68.642ms         3  22.881ms  485.48us  34.114ms  cudaFree\n",
            "                    0.28%  1.6389ms         3  546.30us  506.46us  591.92us  cudaMalloc\n",
            "                    0.11%  660.12us         1  660.12us  660.12us  660.12us  cuDeviceGetPCIBusId\n",
            "                    0.09%  525.09us         1  525.09us  525.09us  525.09us  cuDeviceTotalMem\n",
            "                    0.04%  223.44us       101  2.2120us     164ns  84.808us  cuDeviceGetAttribute\n",
            "                    0.02%  132.63us         2  66.315us  9.8530us  122.78us  cudaEventSynchronize\n",
            "                    0.00%  25.283us         1  25.283us  25.283us  25.283us  cuDeviceGetName\n",
            "                    0.00%  22.192us         4  5.5480us  1.7230us  9.3240us  cudaEventRecord\n",
            "                    0.00%  5.1610us         2  2.5800us  2.5290us  2.6320us  cudaEventElapsedTime\n",
            "                    0.00%  1.8850us         3     628ns     201ns  1.0630us  cuDeviceGetCount\n",
            "                    0.00%  1.5820us         2     791ns     338ns  1.2440us  cuDeviceGet\n",
            "                    0.00%  1.0420us         1  1.0420us  1.0420us  1.0420us  cudaLaunchKernel\n",
            "                    0.00%     345ns         1     345ns     345ns     345ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRM1kr6MYy4a",
        "outputId": "dddc09e6-3ff4-4467-f371-ff6a350d3667"
      },
      "source": [
        "%%writefile shared.cu\n",
        "//matrix multiplication w/ shared memory\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <assert.h>\n",
        "\n",
        "#define BLOCK_SIZE 16 // 32x32\n",
        "#define TILE_WIDTH BLOCK_SIZE\n",
        "\n",
        "__global__ void gpu_matrix_mult(int *a,int *b, int *c, int m, int n, int k){ \n",
        "    \n",
        "    __shared__ int A[TILE_WIDTH][TILE_WIDTH];\n",
        "    __shared__ int B[TILE_WIDTH][TILE_WIDTH];\n",
        "\n",
        "    int bx = blockIdx.x;\n",
        "    int by = blockIdx.y;\n",
        "    int tx = threadIdx.x;\n",
        "    int ty = threadIdx.y;\n",
        "    int row = by * blockDim.y + ty;\n",
        "    int col = bx * blockDim.x + tx;\n",
        "\n",
        "    int Csub = 0;\n",
        "\n",
        "    for (int t=0; t<n/TILE_WIDTH; t++)\n",
        "    {\n",
        "      A[ty][tx] = a[row*n + t*TILE_WIDTH + tx];\n",
        "      B[ty][tx] = b[(t*TILE_WIDTH + ty)*k + col];\n",
        "      \n",
        "      __syncthreads();\n",
        "      for (int i=0; i<TILE_WIDTH; i++)\n",
        "      {\n",
        "          Csub += A[ty][i] * B[i][tx];\n",
        "      }\n",
        "      __syncthreads();\n",
        "    }\n",
        "    c[row*k + col] = Csub;\n",
        "}\n",
        "\n",
        "\n",
        "void cpu_matrix_mult(int *h_a, int *h_b, int *h_result, int m, int n, int k) {\n",
        "    for (int i = 0; i < m; ++i) \n",
        "    {\n",
        "        for (int j = 0; j < k; ++j) \n",
        "        {\n",
        "            int tmp = 0.0;\n",
        "            for (int h = 0; h < n; ++h) \n",
        "            {\n",
        "                tmp += h_a[i * n + h] * h_b[h * k + j];\n",
        "            }\n",
        "            h_result[i * k + j] = tmp;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "int main(int argc, char const *argv[])\n",
        "{\n",
        "    int m=8192, n=m, k=m;\n",
        " \n",
        "    srand(3333);\n",
        "\n",
        "    // allocate memory in host RAM, h_cc is used to store CPU result\n",
        "    int *h_a, *h_b, *h_c, *h_cc;\n",
        "    h_a = (int *)malloc((sizeof(int)*m*n)) ;\n",
        "    h_b = (int *)malloc((sizeof(int)*m*n)) ;\n",
        "    h_c = (int *)malloc((sizeof(int)*m*n)) ;\n",
        "    h_cc = (int *)malloc((sizeof(int)*m*n)) ;\n",
        "    float gpu_elapsed_time_ms, cpu_elapsed_time_ms;\n",
        "\n",
        "    // some events to count the execution time\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&stop);\n",
        "cudaEventCreate(&start);\n",
        "    // start to count execution time of GPU version\n",
        "    // Allocate memory space on the device \n",
        "    int *d_a, *d_b, *d_c;\n",
        "    cudaMalloc(&d_a, sizeof(int)*m*n);\n",
        "    cudaMalloc(&d_b, sizeof(int)*n*k);\n",
        "    cudaMalloc(&d_c, sizeof(int)*m*k);\n",
        "\n",
        "    // copy matrix A and B from host to device memory\n",
        "    cudaMemcpy(d_a, h_a, sizeof(int)*m*n, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, h_b, sizeof(int)*n*k, cudaMemcpyHostToDevice);\n",
        "\n",
        "    unsigned int grid_rows = m/BLOCK_SIZE;\n",
        "    unsigned int grid_cols = k/BLOCK_SIZE;\n",
        "    dim3 dimGrid(grid_cols, grid_rows);\n",
        "    dim3 dimBlock(BLOCK_SIZE, BLOCK_SIZE);\n",
        "    \n",
        "    // Launch kernel \n",
        "   \n",
        "    cudaEventRecord(start, 0);\n",
        "    gpu_matrix_mult<<<dimGrid, dimBlock>>>(d_a, d_b, d_c, m, n, k);    \n",
        "\n",
        "    cudaEventRecord(stop,0);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    // compute time elapse on GPU computing\n",
        "    cudaEventElapsedTime(&gpu_elapsed_time_ms, start, stop);\n",
        "    printf(\"Time elapsed on matrix multiplication of %dx%d . %dx%d on GPU: %f ms.\\n\\n\", m, n, n, k, gpu_elapsed_time_ms);\n",
        "cudaMemcpy(h_c, d_c, sizeof(int)*m*k, cudaMemcpyDeviceToHost);\n",
        "    // start the CPU version \n",
        "    cudaEventRecord(start, 0);\n",
        "    //cpu_matrix_mult(h_a, h_b, h_cc, m, n, k);\n",
        "\n",
        "    cudaEventRecord(stop, 0);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&cpu_elapsed_time_ms, start, stop);\n",
        "    printf(\"Time elapsed on matrix multiplication of %dx%d . %dx%d on CPU: %f ms.\\n\\n\", m, n, n, k, cpu_elapsed_time_ms);\n",
        "\n",
        "    // free memory\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_c);\n",
        "    free(h_a);\n",
        "    free(h_b);\n",
        "    free(h_c);\n",
        "    free(h_cc);\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting shared.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sp_HvioW55U"
      },
      "source": [
        "!nvcc -o shared shared.cu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ViQS7ZG1gr5",
        "outputId": "3fba22eb-7447-4585-e124-0a996ce75283"
      },
      "source": [
        "!./shared"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time elapsed on matrix multiplication of 8192x8192 . 8192x8192 on GPU: 0.003680 ms.\n",
            "\n",
            "Time elapsed on matrix multiplication of 8192x8192 . 8192x8192 on CPU: 0.003904 ms.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRri3jIR1tUQ",
        "outputId": "6e1ff239-12a3-4587-c9ba-41f0b6ccc3fc"
      },
      "source": [
        "!/usr/local/cuda-10.0/NsightCompute-1.0/nv-nsight-cu-cli ./shared"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==PROF== Profiling -    1: 0%....50%....100%\n",
            "Time elapsed on matrix multiplication of 1024x1024 . 1024x1024 on GPU: 4336.388184 ms.\n",
            "\n",
            "Time elapsed on matrix multiplication of 1024x1024 . 1024x1024 on CPU: 0.003968 ms.\n",
            "\n",
            "==PROF== Report: profile.nsight-cuprof-report\n",
            "gpu_matrix_mult, 2021-Aug-11 15:34:18\n",
            "Section: GPU Speed Of Light\n",
            "---------------------------------------------------------------------- --------------- ------------------------------\n",
            "Memory Frequency                                                         cycle/nsecond                           1.25\n",
            "SOL FB                                                                               %                           5.21\n",
            "Elapsed Cycles                                                                   cycle                   5,231,258.60\n",
            "SM Frequency                                                             cycle/usecond                         583.60\n",
            "Memory [%]                                                                           %                          48.18\n",
            "Duration                                                                       msecond                           8.96\n",
            "SOL L2                                                                               %                           6.99\n",
            "SOL TEX                                                                              %                          48.41\n",
            "SM [%]                                                                               %                          64.24\n",
            "---------------------------------------------------------------------- --------------- ------------------------------\n",
            "\n",
            "Section: Compute Workload Analysis\n",
            "---------------------------------------------------------------------- --------------- ------------------------------\n",
            "Executed Ipc Active                                                         inst/cycle                           0.82\n",
            "Executed Ipc Elapsed                                                        inst/cycle                           0.82\n",
            "Issued Ipc Active                                                           inst/cycle                           0.82\n",
            "Issue Slots Busy                                                                     %                          20.56\n",
            "SM Busy                                                                              %                          32.33\n",
            "---------------------------------------------------------------------- --------------- ------------------------------\n",
            "\n",
            "Section: Memory Workload Analysis\n",
            "---------------------------------------------------------------------- --------------- ------------------------------\n",
            "Memory Throughput                                                         Gbyte/second                          21.19\n",
            "Mem Busy                                                                             %                          48.18\n",
            "Max Bandwidth                                                                        %                          32.33\n",
            "L2 Hit Rate                                                                          %                          81.99\n",
            "Mem Pipes Busy                                                                       %                          64.24\n",
            "L1 Hit Rate                                                                          %                          87.28\n",
            "---------------------------------------------------------------------- --------------- ------------------------------\n",
            "\n",
            "Section: Scheduler Statistics\n",
            "---------------------------------------------------------------------- --------------- ------------------------------\n",
            "Active Warps Per Scheduler                                                        warp                           7.89\n",
            "Eligible Warps Per Scheduler                                                      warp                           0.86\n",
            "No Eligible                                                                          %                          79.43\n",
            "Instructions Per Active Issue Slot                                          inst/cycle                              1\n",
            "Issued Warp Per Scheduler                                                                                        0.21\n",
            "One or More Eligible                                                                 %                          20.57\n",
            "---------------------------------------------------------------------- --------------- ------------------------------\n",
            "\n",
            "Section: Warp State Statistics\n",
            "---------------------------------------------------------------------- --------------- ------------------------------\n",
            "Avg. Not Predicated Off Threads Per Warp                                                                        31.97\n",
            "Avg. Active Threads Per Warp                                                                                       32\n",
            "Warp Cycles Per Executed Instruction                                             cycle                          38.33\n",
            "Warp Cycles Per Issued Instruction                                                                              38.33\n",
            "Warp Cycles Per Issue Active                                                                                    38.33\n",
            "---------------------------------------------------------------------- --------------- ------------------------------\n",
            "\n",
            "Section: Instruction Statistics\n",
            "---------------------------------------------------------------------- --------------- ------------------------------\n",
            "Avg. Executed Instructions Per Scheduler                                          inst                   1,070,694.40\n",
            "Executed Instructions                                                             inst                    171,311,104\n",
            "Avg. Issued Instructions Per Scheduler                                            inst                   1,070,727.65\n",
            "Issued Instructions                                                               inst                    171,316,424\n",
            "---------------------------------------------------------------------- --------------- ------------------------------\n",
            "\n",
            "Section: Launch Statistics\n",
            "---------------------------------------------------------------------- --------------- ------------------------------\n",
            "Block Size                                                                                                        256\n",
            "Grid Size                                                                                                       4,096\n",
            "Registers Per Thread                                                   register/thread                             48\n",
            "Shared Memory Configuration Size                                                 Kbyte                             48\n",
            "Dynamic Shared Memory Per Block                                             byte/block                              0\n",
            "Static Shared Memory Per Block                                              byte/block                              0\n",
            "Threads                                                                         thread                      1,048,576\n",
            "Waves Per SM                                                                                                    25.60\n",
            "---------------------------------------------------------------------- --------------- ------------------------------\n",
            "\n",
            "Section: Occupancy\n",
            "---------------------------------------------------------------------- --------------- ------------------------------\n",
            "Block Limit SM                                                                   block                             16\n",
            "Block Limit Registers                                                         register                              5\n",
            "Block Limit Local Mem                                                             byte                            nan\n",
            "Block Limit Warps                                                                 warp                              4\n",
            "Achieved Active Warps Per SM                                                      warp                          31.54\n",
            "Achieved Occupancy                                                                   %                          98.56\n",
            "Theoretical Active Warps per SM                                             warp/cycle                             32\n",
            "Theoretical Occupancy                                                                %                            100\n",
            "---------------------------------------------------------------------- --------------- ------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7iI_ObhKlih",
        "outputId": "c5578a86-ff6d-48ce-ecae-1a4e150d007f"
      },
      "source": [
        "%%cu\n",
        "//matrix multiplication w/ shared memory new\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <assert.h>\n",
        "\n",
        "#define BLOCK_SIZE 32 // 32x32\n",
        "#define TILE_WIDTH 64\n",
        "\n",
        "__global__ void gpu_matrix_mult(int *a,int *b, int *c, int m, int n, int k){ \n",
        "    \n",
        "    __shared__ int A[TILE_WIDTH][TILE_WIDTH];\n",
        "    __shared__ int B[TILE_WIDTH][TILE_WIDTH];\n",
        "\n",
        "    int bx = blockIdx.x;\n",
        "    int by = blockIdx.y;\n",
        "    int tx = threadIdx.x;\n",
        "    int ty = threadIdx.y;\n",
        "    int row = by * blockDim.y + ty;\n",
        "    int col = bx * blockDim.x + tx;\n",
        "\n",
        "    int Csub0 = 0;\n",
        "    int Csub1 = 0;\n",
        "    int Csub2 = 0;\n",
        "    int Csub3 = 0;\n",
        "\n",
        "    for (int t=0; t<n/TILE_WIDTH; t++)\n",
        "    {\n",
        "      A[tx*2][ty*2] = a[2*row*n + t*TILE_WIDTH + 2*tx];\n",
        "      A[tx*2+1][ty*2] = a[2*row*n + t*TILE_WIDTH + 2*tx+1];\n",
        "      A[tx*2][ty*2+1] = a[2*row*n + t*TILE_WIDTH + 2*tx+n];\n",
        "      A[tx*2+1][ty*2+1] = a[2*row*n + t*TILE_WIDTH + 2*tx+n+1];\n",
        "  \n",
        "      B[tx*2][ty*2] = b[(t*TILE_WIDTH + 2*ty)*k + 2*col];\n",
        "      B[tx*2+1][ty*2] = b[(t*TILE_WIDTH + 2*ty)*k + 2*col+1];\n",
        "      B[tx*2][ty*2+1] = b[(t*TILE_WIDTH + 2*ty)*k + 2*col+n];\n",
        "      B[tx*2+1][ty*2+1] = b[(t*TILE_WIDTH + 2*ty)*k + 2*col+n+1];\n",
        "     \n",
        "    /*  B[tx*2][ty*2] = b[2*row*n + t*TILE_WIDTH + 2*tx];\n",
        "      B[tx*2+1][ty*2] = b[2*row*n + t*TILE_WIDTH + 2*tx+1];\n",
        "      B[tx*2][ty*2+1] = b[2*row*n + t*TILE_WIDTH + 2*tx+n];\n",
        "      B[tx*2+1][ty*2+1] = b[2*row*n + t*TILE_WIDTH + 2*tx+n+1];\n",
        "     */\n",
        "      __syncthreads();\n",
        "      \n",
        "      for (int i=0; i<TILE_WIDTH; i++)\n",
        "      {\n",
        "          Csub0 += A[i][2*ty] * B[2*tx][i];\n",
        "       //Csub0 += A[i][2*ty] * B[i][2*ty];\n",
        "      } \n",
        "      for (int i=0; i<TILE_WIDTH; i++)\n",
        "      {\n",
        "          Csub1 += A[i][2*ty] * B[2*tx+1][i];\n",
        "       //Csub1 += A[i][2*ty] * B[i][2*ty];\n",
        "      }\n",
        "      for (int i=0; i<TILE_WIDTH; i++)\n",
        "      {\n",
        "          Csub2 += A[i][2*ty+1] * B[2*tx][i];\n",
        "       //Csub2 += A[i][2*ty+1] * B[i][2*ty+1];\n",
        "      }\n",
        "      for (int i=0; i<TILE_WIDTH; i++)\n",
        "      {\n",
        "          \n",
        "          Csub3 += A[i][2*ty+1] * B[2*tx+1][i];\n",
        "       //Csub3 += A[i][2*ty+1] * B[i][2*ty+1];\n",
        "      }\n",
        "      __syncthreads();\n",
        "    }\n",
        "    c[by*TILE_WIDTH*n+2*ty*n+bx*TILE_WIDTH+2*tx] = Csub0;\n",
        "    c[by*TILE_WIDTH*n+2*ty*n+bx*TILE_WIDTH+2*tx+1] = Csub1;\n",
        "    c[by*TILE_WIDTH*n+2*ty*n+bx*TILE_WIDTH+2*tx+n] = Csub2;\n",
        "    c[by*TILE_WIDTH*n+2*ty*n+bx*TILE_WIDTH+2*tx+n+1] = Csub3; \n",
        "} \n",
        "\n",
        "int main(int argc, char const *argv[])\n",
        "{\n",
        "    int m=512*16, n=m, k=m;\n",
        " \n",
        "    srand(3333);\n",
        "\n",
        "    // allocate memory in host RAM, h_cc is used to store CPU result\n",
        "    int *h_a, *h_b, *h_c, *h_cc;\n",
        "    h_a = (int *)malloc((sizeof(int)*m*n)) ;\n",
        "    h_b = (int *)malloc((sizeof(int)*m*n)) ;\n",
        "    h_c = (int *)malloc((sizeof(int)*m*n)) ;\n",
        "    h_cc = (int *)malloc((sizeof(int)*m*n)) ;\n",
        "    float gpu_elapsed_time_ms, cpu_elapsed_time_ms;\n",
        "\n",
        "    // some events to count the execution time\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&stop);\n",
        "cudaEventCreate(&start);\n",
        "    // start to count execution time of GPU version\n",
        "    // Allocate memory space on the device \n",
        "    int *d_a, *d_b, *d_c;\n",
        "    cudaMalloc(&d_a, sizeof(int)*m*n);\n",
        "    cudaMalloc(&d_b, sizeof(int)*n*k);\n",
        "    cudaMalloc(&d_c, sizeof(int)*m*k);\n",
        "\n",
        "    // copy matrix A and B from host to device memory\n",
        "    cudaMemcpy(d_a, h_a, sizeof(int)*m*n, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, h_b, sizeof(int)*n*k, cudaMemcpyHostToDevice);\n",
        "\n",
        "    unsigned int grid_rows = m/BLOCK_SIZE/2;\n",
        "    unsigned int grid_cols = k/BLOCK_SIZE/2;\n",
        "    dim3 dimGrid(grid_cols, grid_rows);\n",
        "    dim3 dimBlock(BLOCK_SIZE, BLOCK_SIZE);\n",
        "    \n",
        "    // Launch kernel \n",
        "   \n",
        "    cudaEventRecord(start, 0);\n",
        "    gpu_matrix_mult<<<dimGrid, dimBlock>>>(d_a, d_b, d_c, m, n, k);    \n",
        "\n",
        "    cudaEventRecord(stop,0);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    // compute time elapse on GPU computing\n",
        "    cudaEventElapsedTime(&gpu_elapsed_time_ms, start, stop);\n",
        "    printf(\"Time elapsed on matrix multiplication of %dx%d . %dx%d on GPU: %f ms.\\n\\n\", m, n, n, k, gpu_elapsed_time_ms);\n",
        "cudaMemcpy(h_c, d_c, sizeof(int)*m*k, cudaMemcpyDeviceToHost);\n",
        "    // start the CPU version \n",
        "    cudaEventRecord(start, 0);\n",
        "    //cpu_matrix_mult(h_a, h_b, h_cc, m, n, k);\n",
        "\n",
        "    cudaEventRecord(stop, 0);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&cpu_elapsed_time_ms, start, stop);\n",
        "    printf(\"Time elapsed on matrix multiplication of %dx%d . %dx%d on CPU: %f ms.\\n\\n\", m, n, n, k, cpu_elapsed_time_ms);\n",
        "\n",
        "\n",
        "    // free memory\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_c);\n",
        "    free(h_a);\n",
        "    free(h_b);\n",
        "    free(h_c);\n",
        "    free(h_cc);\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time elapsed on matrix multiplication of 8192x8192 . 8192x8192 on GPU: 5439.753906 ms.\n",
            "\n",
            "Time elapsed on matrix multiplication of 8192x8192 . 8192x8192 on CPU: 0.002720 ms.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qZkubvWoH4r",
        "outputId": "1a257091-1de1-4acd-878a-972ed60aeebf"
      },
      "source": [
        "%%cu\n",
        "// test fermi\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <assert.h>\n",
        "#define size 4\n",
        "#define BLOCK_SIZE 16\n",
        "#define TILE_WIDTH 64\n",
        "#define TILE_DIM 32\n",
        "#define BLOCK_ROWS 8\n",
        "__global__ void gpu_matrix_mult3(int *a,int *b, int *c, int m, int n, int k){ \n",
        "    \n",
        "    __shared__ int A[TILE_WIDTH][TILE_WIDTH];\n",
        "    __shared__ int B[TILE_WIDTH][TILE_WIDTH];\n",
        "\n",
        "    int bx = blockIdx.x;\n",
        "    int by = blockIdx.y;\n",
        "    int tx = threadIdx.x;\n",
        "    int ty = threadIdx.y;\n",
        "    int row = by * blockDim.y + ty;\n",
        "    int col = bx * blockDim.x + tx;\n",
        "\n",
        "    int sum[size*size]={0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};\n",
        "    for (int t=0; t<n/TILE_WIDTH; t++)\n",
        "    {\n",
        "      for (int j=0; j<size; j++){\n",
        "          for (int i=0; i<size; i++){\n",
        "              A[ty+j*BLOCK_SIZE][tx+i*BLOCK_SIZE] = a[by*TILE_WIDTH*n + t*TILE_WIDTH+ty*n+BLOCK_SIZE*n*j+BLOCK_SIZE*i+tx];\n",
        "              B[ty+j*BLOCK_SIZE][tx+i*BLOCK_SIZE] = b[t*TILE_WIDTH*n + bx*TILE_WIDTH+ty*n+BLOCK_SIZE*n*j+BLOCK_SIZE*i+tx];\n",
        "          }\n",
        "      }\n",
        "      __syncthreads();\n",
        "      for (int i=0; i<TILE_WIDTH; i++){\n",
        "          int b_t[size];\n",
        "          int a_t[size];\n",
        "          for (int k=0; k<size; k++){\n",
        "              b_t[k]=B[i][size*tx+k];\n",
        "              a_t[k]=A[size*ty+k][i];\n",
        "          }\n",
        "          for (int q=0; q<size; q++){\n",
        "              for (int w=0; w<size; w++){\n",
        "                  sum[size*q+w]+=b_t[w]*a_t[q];\n",
        "                  printf(\"%i \", sum[size*q+w]);\n",
        "              }\n",
        "          }\n",
        "      }\n",
        "      __syncthreads();\n",
        "    }\n",
        "    for (int j=0; j<size; j++){\n",
        "        for (int i=0; i<size; i++){\n",
        "            c[by*TILE_WIDTH*n+size*ty*n+bx*TILE_WIDTH+size*tx+i+n*j]=sum[size*j+i];\n",
        "            //c[by*TILE_WIDTH*n+bx*TILE_WIDTH+j*size*n+ty*n+tx]=sum[size*j+i];\n",
        "        }\n",
        "    }\n",
        "}\n",
        "__global__ void gpu_matrix_mult(int *a,int *b, int *c, int m, int n, int k){ \n",
        "    \n",
        "    __shared__ int A[TILE_WIDTH][TILE_WIDTH];\n",
        "    __shared__ int B[TILE_WIDTH][TILE_WIDTH];\n",
        "\n",
        "    int bx = blockIdx.x;\n",
        "    int by = blockIdx.y;\n",
        "    int tx = threadIdx.x;\n",
        "    int ty = threadIdx.y;\n",
        "    int row = by * blockDim.y + ty;\n",
        "    int col = bx * blockDim.x + tx;\n",
        "\n",
        "    int Csub0 = 0;\n",
        "    int Csub1 = 0;\n",
        "    int Csub2 = 0;\n",
        "    int Csub3 = 0;\n",
        "    //int sum[4]={0,0,0,0};\n",
        "    for (int t=0; t<n/TILE_WIDTH; t++)\n",
        "    {\n",
        "      A[ty][tx] = a[by*TILE_WIDTH*n + t*TILE_WIDTH + ty*n+tx];\n",
        "      A[ty][tx+BLOCK_SIZE] = a[by*TILE_WIDTH*n + t*TILE_WIDTH + ty*n+tx+BLOCK_SIZE];\n",
        "      A[ty+BLOCK_SIZE][tx] = a[by*TILE_WIDTH*n + t*TILE_WIDTH + ty*n+tx+BLOCK_SIZE*n];\n",
        "      A[ty+BLOCK_SIZE][tx+BLOCK_SIZE] = a[by*TILE_WIDTH*n + t*TILE_WIDTH + ty*n+tx+BLOCK_SIZE*n+BLOCK_SIZE];\n",
        "  \n",
        "      /*B[ty*2][tx*2] = b[(t*TILE_WIDTH + 2*ty)*k + 2*col];\n",
        "      B[ty*2][tx*2+1] = b[(t*TILE_WIDTH + 2*ty)*k + 2*col+1];\n",
        "      B[ty*2+1][tx*2] = b[(t*TILE_WIDTH + 2*ty)*k + 2*col+n];\n",
        "      B[ty*2+1][tx*2+1] = b[(t*TILE_WIDTH + 2*ty)*k + 2*col+n+1];*/\n",
        "     \n",
        "      B[ty][tx] = b[t*TILE_WIDTH*n + bx*TILE_WIDTH + ty*n + tx];\n",
        "      B[ty][tx+BLOCK_SIZE] = b[t*TILE_WIDTH*n + bx*TILE_WIDTH + ty*n + tx + BLOCK_SIZE];\n",
        "      B[ty+BLOCK_SIZE][tx] = b[t*TILE_WIDTH*n + bx*TILE_WIDTH + ty*n + tx + BLOCK_SIZE*n];\n",
        "      B[ty+BLOCK_SIZE][tx+BLOCK_SIZE] = b[t*TILE_WIDTH*n + bx*TILE_WIDTH + ty*n + tx + BLOCK_SIZE*n + BLOCK_SIZE];\n",
        "\n",
        "      __syncthreads();\n",
        "      \n",
        "      /*for (int i=0; i<TILE_WIDTH; i++){\n",
        "          int b1 = B[i][2*tx];\n",
        "          int b2 = B[i][2*tx+1];\n",
        "          int a1 = A[2*ty][i];\n",
        "          int a2 = A[2*ty+1][i];\n",
        "          sum[0]+=b1*a1;\n",
        "          sum[1]+=b2*a1;\n",
        "          sum[2]+=a2*b1;\n",
        "          sum[3]+=a2*b2;\n",
        "      }*/\n",
        "\n",
        "      for (int i=0; i<TILE_WIDTH; i++)\n",
        "      {\n",
        "          Csub0 += A[ty][i] * B[i][tx];       \n",
        "      }\n",
        "      for (int i=0; i<TILE_WIDTH; i++)\n",
        "      {\n",
        "          Csub1 += A[ty][i] * B[i][tx+BLOCK_SIZE];\n",
        "      }\n",
        "      for (int i=0; i<TILE_WIDTH; i++)\n",
        "      {\n",
        "          Csub2 += A[ty+BLOCK_SIZE][i] * B[i][tx];\n",
        "      }\n",
        "      for (int i=0; i<TILE_WIDTH; i++)\n",
        "      {\n",
        "          Csub3 += A[ty+BLOCK_SIZE][i] * B[i][tx+BLOCK_SIZE];\n",
        "      }\n",
        "      __syncthreads();\n",
        "    }\n",
        "    c[by*TILE_WIDTH*n+ty*n+bx*TILE_WIDTH+tx] = Csub0;\n",
        "    c[by*TILE_WIDTH*n+ty*n+bx*TILE_WIDTH+tx+BLOCK_SIZE] = Csub1;\n",
        "    c[by*TILE_WIDTH*n+ty*n+bx*TILE_WIDTH+tx+BLOCK_SIZE*n] = Csub2;\n",
        "    c[by*TILE_WIDTH*n+ty*n+bx*TILE_WIDTH+tx+BLOCK_SIZE*n+BLOCK_SIZE] = Csub3;\n",
        "\n",
        "    /*c[by*TILE_WIDTH*n+2*ty*n+bx*TILE_WIDTH+2*tx] = sum[0];\n",
        "    c[by*TILE_WIDTH*n+2*ty*n+bx*TILE_WIDTH+2*tx+1] = sum[1];\n",
        "    c[by*TILE_WIDTH*n+2*ty*n+bx*TILE_WIDTH+2*tx+n] = sum[2];\n",
        "    c[by*TILE_WIDTH*n+2*ty*n+bx*TILE_WIDTH+2*tx+n+1] = sum[3];*/\n",
        "} \n",
        "\n",
        "__global__ void gpu_matrix_mult2(int *a,int *b, int *c, int m, int n, int k){ \n",
        "    \n",
        "    __shared__ int A[TILE_WIDTH][TILE_WIDTH];\n",
        "    __shared__ int B[TILE_WIDTH][TILE_WIDTH];\n",
        "\n",
        "    int bx = blockIdx.x;\n",
        "    int by = blockIdx.y;\n",
        "    int tx = threadIdx.x;\n",
        "    int ty = threadIdx.y;\n",
        "    int row = by * blockDim.y + ty;\n",
        "    int col = bx * blockDim.x + tx;\n",
        "    int Csub = 0;\n",
        "    for (int t=0; t<n/TILE_WIDTH; t++)\n",
        "    {\n",
        "      A[ty][tx] = b[(t*TILE_WIDTH + ty)*k + col];//a[row*n + t*TILE_WIDTH + tx];\n",
        "      B[ty][tx] = b[(t*TILE_WIDTH + ty)*k + col];//b[row*n + t*TILE_WIDTH + tx];\n",
        "      \n",
        "      __syncthreads();\n",
        "      for (int i=0; i<TILE_WIDTH; i++)\n",
        "      {\n",
        "          Csub += A[ty][i] * B[ty][i]; \n",
        "      }\n",
        "      __syncthreads();\n",
        "    }\n",
        "    c[row*k + col] = Csub;\n",
        "}\n",
        "\n",
        "void cpu_matrix_mult(int *h_a, int *h_b, int *h_result, int m, int n, int k) {\n",
        "    for (int i = 0; i < m; ++i) \n",
        "    {\n",
        "        for (int j = 0; j < k; ++j) \n",
        "        {\n",
        "            int tmp = 0.0;\n",
        "            for (int h = 0; h < n; ++h) \n",
        "            {\n",
        "                tmp += h_a[i * n + h] * h_b[h * k + j];\n",
        "            }\n",
        "            h_result[i * k + j] = tmp;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "int main(int argc, char const *argv[])\n",
        "{\n",
        "    int m=64;\n",
        "    int n=m;\n",
        "    int k=m;\n",
        " \n",
        "    srand(3333);\n",
        "\n",
        "    // allocate memory in host RAM, h_cc is used to store CPU result\n",
        "    int *h_a, *h_b, *h_c, *h_cc;\n",
        "    h_a = (int *)malloc((sizeof(int)*m*n)) ;\n",
        "    h_b = (int *)malloc((sizeof(int)*m*n)) ;\n",
        "    h_c = (int *)malloc((sizeof(int)*m*n)) ;\n",
        "    h_cc = (int *)malloc((sizeof(int)*m*n)) ;\n",
        "\n",
        "    // random initialize matrix A\n",
        "    for (int i = 0; i < m; ++i) {\n",
        "        for (int j = 0; j < n; ++j) {\n",
        "            h_a[i * n + j] = random()%10;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // random initialize matrix B\n",
        "    for (int i = 0; i < n; ++i) {\n",
        "        for (int j = 0; j < k; ++j) {\n",
        "            h_b[i * k + j] = random()%10;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    float gpu_elapsed_time_ms, cpu_elapsed_time_ms;\n",
        "\n",
        "    // some events to count the execution time\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    // start to count execution time of GPU version\n",
        "    // Allocate memory space on the device \n",
        "    int *d_a, *d_b, *d_c, *d_bt;\n",
        "    cudaMalloc(&d_a, sizeof(int)*m*n);\n",
        "    cudaMalloc(&d_b, sizeof(int)*n*k);\n",
        "    cudaMalloc(&d_c, sizeof(int)*m*k);\n",
        "    cudaMalloc(&d_bt, sizeof(int)*m*k);\n",
        "\n",
        "    // copy matrix A and B from host to device memory\n",
        "    cudaMemcpy(d_a, h_a, sizeof(int)*m*n, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, h_b, sizeof(int)*n*k, cudaMemcpyHostToDevice);\n",
        "\n",
        "    unsigned int grid_rows = m/BLOCK_SIZE/4;\n",
        "    unsigned int grid_cols = k/BLOCK_SIZE/4;\n",
        "    dim3 dimGrid(grid_cols, grid_rows);\n",
        "\n",
        "    dim3 dimGrid2(grid_cols, grid_rows);\n",
        "    dim3 dimBlock2(BLOCK_SIZE, BLOCK_SIZE); \n",
        " \n",
        "    cudaEventRecord(start, 0);\n",
        "    gpu_matrix_mult3<<<dimGrid, dimBlock2>>>(d_a, d_b, d_c, m, n, k);\n",
        "    cudaDeviceSynchronize();\n",
        "    cudaEventRecord(stop, 0);\n",
        " \n",
        "\n",
        "    cudaEventElapsedTime(&gpu_elapsed_time_ms, start, stop);\n",
        "    // Transefer results from device to host\n",
        "    cudaMemcpy(h_c, d_c, sizeof(int)*m*k, cudaMemcpyDeviceToHost);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // compute time elapse on GPU computing\n",
        "    printf(\"Time elapsed on matrix multiplication of %dx%d . %dx%d on GPU: %f ms.\\n\\n\", m, n, n, k, gpu_elapsed_time_ms);\n",
        "\n",
        "    // start the CPU version\n",
        "    cudaEventRecord(start, 0);\n",
        "\n",
        "    cpu_matrix_mult(h_a, h_b, h_cc, m, n, k);\n",
        "\n",
        "    cudaEventRecord(stop, 0);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&cpu_elapsed_time_ms, start, stop);\n",
        "    printf(\"Time elapsed on matrix multiplication of %dx%d . %dx%d on CPU: %f ms.\\n\\n\", m, n, n, k, cpu_elapsed_time_ms);\n",
        "\n",
        "    // validate results computed by GPU\n",
        "    int all_ok = 1;\n",
        "    for (int i = 0; i < m; ++i)\n",
        "    {\n",
        "        for (int j = 0; j < k; ++j)\n",
        "        {\n",
        "            //printf(\"[%d][%d]:%d == [%d][%d]:%d, \", i, j, h_cc[i*k + j], i, j, h_c[i*k + j]);\n",
        "         //printf(\"%d \",h_cc[i*k + j]);\n",
        "            if(h_cc[i*k + j] != h_c[i*k + j])\n",
        "            {\n",
        "                all_ok = 0;\n",
        "            }\n",
        "        }\n",
        "        //printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    // roughly compute speedup\n",
        "    if(all_ok)\n",
        "    {\n",
        "        printf(\"all results are correct!!!, speedup = %f\\n\", cpu_elapsed_time_ms / gpu_elapsed_time_ms);\n",
        "    }\n",
        "    else\n",
        "    {\n",
        "        printf(\"incorrect results\\n\");\n",
        "    }\n",
        "    // free memory\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_c);\n",
        "    free(h_a);\n",
        "    free(h_b);\n",
        "    free(h_c);\n",
        "    free(h_cc);\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 20 36 0 12 16 20 16 32 24 16 12 20 8 24 4 0 45 81 0 27 36 45 36 72 54 36 27 45 18 54 9 0 10 18 0 6 8 10 8 16 12 8 6 10 4 12 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 20 36 0 12 16 20 16 32 24 16 12 20 8 24 4 0 5 9 0 3 4 5 4 8 6 4 3 5 2 6 1 0 15 27 0 9 12 15 12 24 18 12 9 15 6 18 3 0 30 54 0 18 24 30 24 48 36 24 18 30 12 36 6 0 5 9 0 3 4 5 4 8 6 4 3 5 2 6 1 0 40 72 0 24 32 40 32 64 48 32 24 40 16 48 8 0 25 45 0 15 20 25 20 40 30 20 15 25 10 30 5 0 35 63 0 21 28 35 28 56 42 28 21 35 14 42 7 0 35 63 0 21 28 35 28 56 42 28 21 35 14 42 7 0 10 18 0 6 8 10 8 16 12 8 6 10 4 12 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 15 27 0 9 12 15 12 24 18 12 9 15 6 18 3 4 20 12 24 0 28 28 28 36 4 20 20 20 36 4 28 9 45 27 54 0 63 63 63 81 9 45 45 45 81 9 63 4 20 12 24 0 28 28 28 36 4 20 20 20 36 4 28 1 5 3 6 0 7 7 7 9 1 5 5 5 9 1 7 3 15 9 18 0 21 21 21 27 3 15 15 15 27 3 21 6 30 18 36 0 42 42 42 54 6 30 30 30 54 6 42 2 10 6 12 0 14 14 14 18 2 10 10 10 18 2 14 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 5 3 6 0 7 7 7 9 1 5 5 5 9 1 7 8 40 24 48 0 56 56 56 72 8 40 40 40 72 8 56 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 15 9 18 0 21 21 21 27 3 15 15 15 27 3 21 5 25 15 30 0 35 35 35 45 5 25 25 25 45 5 35 7 35 21 42 0 49 49 49 63 7 35 35 35 63 7 49 7 35 21 42 0 49 49 49 63 7 35 35 35 63 7 49 2 10 6 12 0 14 14 14 18 2 10 10 10 18 2 14 8 4 16 8 16 24 8 28 0 0 8 32 12 28 0 36 18 9 36 18 36 54 18 63 0 0 18 72 27 63 0 81 8 4 16 8 16 24 8 28 0 0 8 32 12 28 0 36 2 1 4 2 4 6 2 7 0 0 2 8 3 7 0 9 6 3 12 6 12 18 6 21 0 0 6 24 9 21 0 27 12 6 24 12 24 36 12 42 0 0 12 48 18 42 0 54 4 2 8 4 8 12 4 14 0 0 4 16 6 14 0 18 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 4 2 4 6 2 7 0 0 2 8 3 7 0 9 16 8 32 16 32 48 16 56 0 0 16 64 24 56 0 72 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 6 3 12 6 12 18 6 21 0 0 6 24 9 21 0 27 10 5 20 10 20 30 10 35 0 0 10 40 15 35 0 45 14 7 28 14 28 42 14 49 0 0 14 56 21 49 0 63 14 7 28 14 28 42 14 49 0 0 14 56 21 49 0 63 4 2 8 4 8 12 4 14 0 0 4 16 6 14 0 18 12 16 20 4 20 24 0 24 24 20 20 24 4 32 8 36 27 36 45 9 45 54 0 54 54 45 45 54 9 72 18 81 21 28 35 7 35 42 0 42 42 35 35 42 7 56 14 63 6 8 10 2 10 12 0 12 12 10 10 12 2 16 4 18 3 4 5 1 5 6 0 6 6 5 5 6 1 8 2 9 24 32 40 8 40 48 0 48 48 40 40 48 8 64 16 72 12 16 20 4 20 24 0 24 24 20 20 24 4 32 8 36 3 4 5 1 5 6 0 6 6 5 5 6 1 8 2 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 9 12 15 3 15 18 0 18 18 15 15 18 3 24 6 27 6 8 10 2 10 12 0 12 12 10 10 12 2 16 4 18 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 9 12 15 3 15 18 0 18 18 15 15 18 3 24 6 27 18 24 30 6 30 36 0 36 36 30 30 36 6 48 12 54 15 20 25 5 25 30 0 30 30 25 25 30 5 40 10 45 21 28 35 7 35 42 0 42 42 35 35 42 7 56 14 63 0 40 72 0 24 32 40 32 64 48 32 24 40 16 48 8 0 45 81 0 27 36 45 36 72 54 36 27 45 18 54 9 0 40 72 0 24 32 40 32 64 48 32 24 40 16 48 8 0 40 72 0 24 32 40 32 64 48 32 24 40 16 48 8 0 30 54 0 18 24 30 24 48 36 24 18 30 12 36 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 15 27 0 9 12 15 12 24 18 12 9 15 6 18 3 0 25 45 0 15 20 25 20 40 30 20 15 25 10 30 5 0 25 45 0 15 20 25 20 40 30 20 15 25 10 30 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 20 36 0 12 16 20 16 32 24 16 12 20 8 24 4 0 25 45 0 15 20 25 20 40 30 20 15 25 10 30 5 0 15 27 0 9 12 15 12 24 18 12 9 15 6 18 3 0 40 72 0 24 32 40 32 64 48 32 24 40 16 48 8 0 5 9 0 3 4 5 4 8 6 4 3 5 2 6 1 0 35 63 0 21 28 35 28 56 42 28 21 35 14 42 7 8 40 24 48 0 56 56 56 72 8 40 40 40 72 8 56 9 45 27 54 0 63 63 63 81 9 45 45 45 81 9 63 8 40 24 48 0 56 56 56 72 8 40 40 40 72 8 56 8 40 24 48 0 56 56 56 72 8 40 40 40 72 8 56 5 25 15 30 0 35 35 35 45 5 25 25 25 45 5 35 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 6 30 18 36 0 42 42 42 54 6 30 30 30 54 6 42 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 15 9 18 0 21 21 21 27 3 15 15 15 27 3 21 5 25 15 30 0 35 35 35 45 5 25 25 25 45 5 35 4 20 12 24 0 28 28 28 36 4 20 20 20 36 4 28 5 25 15 30 0 35 35 35 45 5 25 25 25 45 5 35 16 8 32 16 32 48 16 56 0 0 16 64 24 56 0 72 18 9 36 18 36 54 18 63 0 0 18 72 27 63 0 81 3 15 9 18 0 21 21 21 27 3 15 15 15 27 3 21 8 40 24 48 0 56 56 56 72 8 40 40 40 72 8 56 1 5 3 6 0 7 7 7 9 1 5 5 5 9 1 7 7 35 21 42 0 49 49 49 63 7 35 35 35 63 7 49 16 8 32 16 32 48 16 56 0 0 16 64 24 56 0 72 16 8 32 16 32 48 16 56 0 0 16 64 24 56 0 72 24 32 40 8 40 48 0 48 48 40 40 48 8 64 16 72 27 36 45 9 45 54 0 54 54 45 45 54 9 72 18 81 6 3 12 6 12 18 6 21 0 0 6 24 9 21 0 27 16 8 32 16 32 48 16 56 0 0 16 64 24 56 0 72 2 1 4 2 4 6 2 7 0 0 2 8 3 7 0 9 14 7 28 14 28 42 14 49 0 0 14 56 21 49 0 63 12 6 24 12 24 36 12 42 0 0 12 48 18 42 0 54 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 10 5 20 10 20 30 10 35 0 0 10 40 15 35 0 45 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8 4 16 8 16 24 8 28 0 0 8 32 12 28 0 36 10 5 20 10 20 30 10 35 0 0 10 40 15 35 0 45 6 3 12 6 12 18 6 21 0 0 6 24 9 21 0 27 10 5 20 10 20 30 10 35 0 0 10 40 15 35 0 45 24 32 40 8 40 48 0 48 48 40 40 48 8 64 16 72 24 32 40 8 40 48 0 48 48 40 40 48 8 64 16 72 0 40 72 0 24 32 40 32 64 48 32 24 40 16 48 8 0 10 18 0 6 8 10 8 16 12 8 6 10 4 12 2 9 12 15 3 15 18 0 18 18 15 15 18 3 24 6 27 24 32 40 8 40 48 0 48 48 40 40 48 8 64 16 72 0 25 45 0 15 20 25 20 40 30 20 15 25 10 30 5 0 30 54 0 18 24 30 24 48 36 24 18 30 12 36 6 12 16 20 4 20 24 0 24 24 20 20 24 4 32 8 36 15 20 25 5 25 30 0 30 30 25 25 30 5 40 10 45 15 20 25 5 25 30 0 30 30 25 25 30 5 40 10 45 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 18 24 30 6 30 36 0 36 36 30 30 36 6 48 12 54 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 4 5 1 5 6 0 6 6 5 5 6 1 8 2 9 21 28 35 7 35 42 0 42 42 35 35 42 7 56 14 63 9 12 15 3 15 18 0 18 18 15 15 18 3 24 6 27 15 20 25 5 25 30 0 30 30 25 25 30 5 40 10 45 0 20 36 0 12 16 20 16 32 24 16 12 20 8 24 4 0 25 45 0 15 20 25 20 40 30 20 15 25 10 30 5 8 40 24 48 0 56 56 56 72 8 40 40 40 72 8 56 2 10 6 12 0 14 14 14 18 2 10 10 10 18 2 14 5 25 15 30 0 35 35 35 45 5 25 25 25 45 5 35 6 30 18 36 0 42 42 42 54 6 30 30 30 54 6 42 0 35 63 0 21 28 35 28 56 42 28 21 35 14 42 7 0 15 27 0 9 12 15 12 24 18 12 9 15 6 18 3 0 35 63 0 21 28 35 28 56 42 28 21 35 14 42 7 0 30 54 0 18 24 30 24 48 36 24 18 30 12 36 6 0 45 81 0 27 36 45 36 72 54 36 27 45 18 54 9 0 25 45 0 15 20 25 20 40 30 20 15 25 10 30 5 0 20 36 0 12 16 20 16 32 24 16 12 20 8 24 4 0 35 63 0 21 28 35 28 56 42 28 21 35 14 42 7 0 10 18 0 6 8 10 8 16 12 8 6 10 4 12 2 0 40 72 0 24 32 40 32 64 48 32 24 40 16 48 8 16 8 32 16 32 48 16 56 0 0 16 64 24 56 0 72 4 2 8 4 8 12 4 14 0 0 4 16 6 14 0 18 4 20 12 24 0 28 28 28 36 4 20 20 20 36 4 28 5 25 15 30 0 35 35 35 45 5 25 25 25 45 5 35 10 5 20 10 20 30 10 35 0 0 10 40 15 35 0 45 12 6 24 12 24 36 12 42 0 0 12 48 18 42 0 54 9 45 27 54 0 63 63 63 81 9 45 45 45 81 9 63 5 25 15 30 0 35 35 35 45 5 25 25 25 45 5 35 7 35 21 42 0 49 49 49 63 7 35 35 35 63 7 49 3 15 9 18 0 21 21 21 27 3 15 15 15 27 3 21 4 20 12 24 0 28 28 28 36 4 20 20 20 36 4 28 7 35 21 42 0 49 49 49 63 7 35 35 35 63 7 49 7 35 21 42 0 49 49 49 63 7 35 35 35 63 7 49 6 30 18 36 0 42 42 42 54 6 30 30 30 54 6 42 2 10 6 12 0 14 14 14 18 2 10 10 10 18 2 14 8 40 24 48 0 56 56 56 72 8 40 40 40 72 8 56 15 20 25 5 25 30 0 30 30 25 25 30 5 40 10 45 18 24 30 6 30 36 0 36 36 30 30 36 6 48 12 54 24 32 40 8 40 48 0 48 48 40 40 48 8 64 16 72 6 8 10 2 10 12 0 12 12 10 10 12 2 16 4 18 14 7 28 14 28 42 14 49 0 0 14 56 21 49 0 63 6 3 12 6 12 18 6 21 0 0 6 24 9 21 0 27 8 4 16 8 16 24 8 28 0 0 8 32 12 28 0 36 10 5 20 10 20 30 10 35 0 0 10 40 15 35 0 45 18 9 36 18 36 54 18 63 0 0 18 72 27 63 0 81 10 5 20 10 20 30 10 35 0 0 10 40 15 35 0 45 4 2 8 4 8 12 4 14 0 0 4 16 6 14 0 18 16 8 32 16 32 48 16 56 0 0 16 64 24 56 0 72 14 7 28 14 28 42 14 49 0 0 14 56 21 49 0 63 12 6 24 12 24 36 12 42 0 0 12 48 18 42 0 54 8 4 16 8 16 24 8 28 0 0 8 32 12 28 0 36 14 7 28 14 28 42 14 49 0 0 14 56 21 49 0 63 27 36 45 9 45 54 0 54 54 45 45 54 9 72 18 81 15 20 25 5 25 30 0 30 30 25 25 30 5 40 10 45 6 8 10 2 10 12 0 12 12 10 10 12 2 16 4 18 24 32 40 8 40 48 0 48 48 40 40 48 8 64 16 72 12 16 20 4 20 24 0 24 24 20 20 24 4 32 8 36 21 28 35 7 35 42 0 42 42 35 35 42 7 56 14 63 0 5 9 0 3 4 5 4 8 6 4 3 5 2 6 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 21 28 35 7 35 42 0 42 42 35 35 42 7 56 14 63 18 24 30 6 30 36 0 36 36 30 30 36 6 48 12 54 0 20 36 0 12 16 20 16 32 24 16 12 20 8 24 4 0 5 9 0 3 4 5 4 8 6 4 3 5 2 6 1 21 28 35 7 35 42 0 42 42 35 35 42 7 56 14 63 9 12 15 3 15 18 0 18 18 15 15 18 3 24 6 27 12 16 20 4 20 24 0 24 24 20 20 24 4 32 8 36 15 20 25 5 25 30 0 30 30 25 25 30 5 40 10 45 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 20 36 0 12 16 20 16 32 24 16 12 20 8 24 4 0 40 72 0 24 32 40 32 64 48 32 24 40 16 48 8 0 10 18 0 6 8 10 8 16 12 8 6 10 4 12 2 0 5 9 0 3 4 5 4 8 6 4 3 5 2 6 1 0 25 45 0 15 20 25 20 40 30 20 15 25 10 30 5 1 5 3 6 0 7 7 7 9 1 5 5 5 9 1 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5 9 0 3 4 5 4 8 6 4 3 5 2 6 1 0 30 54 0 18 24 30 24 48 36 24 18 30 12 36 6 0 25 45 0 15 20 25 20 40 30 20 15 25 10 30 5 0 15 27 0 9 12 15 12 24 18 12 9 15 6 18 3 4 20 12 24 0 28 28 28 36 4 20 20 20 36 4 28 1 5 3 6 0 7 7 7 9 1 5 5 5 9 1 7 0 30 54 0 18 24 30 24 48 36 24 18 30 12 36 6 0 45 81 0 27 36 45 36 72 54 36 27 45 18 54 9 432 403 451 372 407 436 576 550 572 576 500 645 677 545 612 473 587 655 426 833 590 726 498 474 498 464 501 585 645 535 508 500 503 599 367 727 563 636 533 503 536 382 508 583 497 552 574 466 464 697 597 524 462 624 473 526 659 427 560 637 523 588 685 417 416 631 643 453 644 628 803 578 643 530 585 611 610 565 570 604 636 563 641 498 493 569 750 570 625 595 572 483 618 627 661 537 639 578 627 465 514 558 485 481 550 477 634 642 640 607 605 486 450 693 646 534 452 642 427 508 533 480 512 559 452 507 491 326 365 645 592 461 461 516 504 505 436 330 404 442 509 479 516 480 501 490 332 557 395 561 526 541 580 468 493 646 658 484 529 607 515 540 305 701 637 584 492 542 463 483 378 524 620 582 595 489 533 525 425 509 422 602 755 708 578 683 548 707 855 707 759 554 695 687 621 674 678 778 468 528 588 442 638 619 529 570 704 432 441 675 609 516 528 563 520 510 730 544 673 742 623 594 711 458 419 698 649 534 616 639 331 399 310 310 367 360 303 401 347 262 226 413 366 376 381 456 463 472 433 447 515 507 473 392 492 417 308 549 443 473 474 517 521 428 432 408 432 557 649 502 469 434 573 511 377 681 539 567 597 466 554 533 513 628 769 572 539 449 588 586 417 776 626 659 565 404 573 436 517 479 639 567 505 476 509 503 332 681 556 628 476 506 483 467 387 486 554 469 563 470 435 465 332 585 423 598 566 524 388 523 391 573 593 640 528 424 487 511 541 548 529 644 490 499 455 416 360 505 603 503 528 336 512 473 429 449 434 524 691 727 609 627 593 617 745 630 662 614 531 643 631 651 621 783 739 703 512 632 568 722 706 782 765 567 652 708 676 684 669 841 450 450 370 386 309 386 449 453 472 432 363 472 364 434 406 523 577 497 496 534 473 590 685 626 537 526 529 511 474 449 505 611 504 494 407 469 315 488 562 508 491 420 415 372 390 479 408 599 562 525 371 481 419 516 592 617 550 416 604 545 453 506 504 672 618 444 532 388 494 510 492 514 488 518 543 499 548 436 456 475 850 644 746 537 708 636 665 714 642 684 737 653 758 631 579 650 401 341 241 376 287 379 468 400 426 325 349 421 315 311 372 462 520 451 392 450 431 470 582 506 474 431 449 468 370 498 460 540 343 447 553 459 460 575 485 629 502 300 452 664 535 455 430 532 474 568 581 498 537 614 532 585 606 413 442 718 571 515 466 578 423 477 560 423 537 580 508 513 592 405 373 650 601 479 407 422 484 421 417 369 565 558 474 536 559 408 378 612 506 481 422 539 658 520 620 457 534 521 550 611 522 544 608 552 654 493 522 517 726 571 548 435 571 537 634 540 578 638 658 509 688 518 426 511 505 385 424 379 449 419 461 418 359 437 466 376 461 345 339 385 611 497 540 432 453 406 517 451 533 446 494 440 466 444 417 494 631 551 607 432 524 544 566 556 465 554 611 498 583 504 512 539 614 428 464 409 472 421 520 495 474 497 537 419 505 378 439 458 639 527 669 452 585 484 570 645 515 576 572 508 629 436 500 425 747 488 594 473 622 466 546 615 532 589 558 522 650 509 405 438 453 458 511 426 419 515 665 458 494 468 424 482 299 609 477 608 708 556 623 598 576 677 798 646 659 620 633 679 456 791 778 775 581 564 649 436 515 527 541 620 516 513 610 493 612 460 541 532 547 465 549 416 490 412 513 487 472 467 522 485 589 460 411 428 547 552 600 445 496 466 565 573 522 470 467 469 499 524 495 608 617 566 552 529 530 544 687 618 600 502 585 535 498 557 506 648 519 521 464 560 441 575 631 609 577 457 635 483 570 489 386 581 482 525 396 532 444 499 560 421 545 485 426 529 439 466 430 624 623 417 550 494 462 535 674 500 589 490 523 517 397 654 600 643 580 427 638 525 558 597 738 571 573 550 643 603 414 782 691 685 402 379 403 354 383 394 513 400 487 358 403 385 270 528 358 455 464 403 442 452 416 578 599 495 491 524 470 445 350 614 480 610 515 525 504 493 484 573 629 514 554 458 485 572 361 685 541 679 510 462 512 398 432 527 580 427 497 502 487 448 343 657 481 600 579 468 464 495 508 602 678 450 574 439 518 597 401 698 605 670 541 402 478 473 444 573 660 487 565 539 513 604 354 690 703 651 413 453 414 444 507 508 486 511 502 344 311 623 525 436 431 539 612 618 668 537 746 739 683 689 849 590 520 784 703 644 664 686 536 518 508 552 468 589 637 473 556 417 563 591 418 777 590 710 478 458 466 452 485 557 625 499 480 472 491 567 363 699 563 628 624 396 450 389 470 463 453 343 427 406 500 404 426 364 368 455 745 511 528 495 623 516 554 586 531 563 629 472 644 514 503 507 773 553 628 490 585 566 565 525 540 599 611 533 626 458 483 564 726 550 613 563 572 447 582 595 637 533 619 554 615 433 506 554 468 525 564 433 611 595 505 555 692 426 432 675 588 513 507 536 520 502 666 520 601 678 559 554 679 442 395 698 593 526 560 567 329 402 428 390 429 418 355 398 454 262 364 592 426 390 292 405 476 379 456 401 505 599 545 579 558 389 405 572 516 458 420 506 485 473 486 453 562 578 576 567 573 470 426 693 590 526 396 570 427 502 485 462 458 511 404 477 467 314 347 645 550 455 419 462 533 503 536 382 508 583 497 552 574 466 464 697 597 524 462 624 473 521 619 412 515 597 483 563 665 407 401 631 608 448 609 583 460 510 439 467 346 496 592 558 587 461 509 489 409 481 422 574 747 700 572 679 540 700 848 701 757 547 689 678 617 667 678 771 469 545 539 427 530 608 533 556 527 453 432 665 651 554 438 633 456 417 516 389 423 573 491 557 542 466 438 630 632 466 437 535 494 497 420 324 396 428 499 461 502 466 495 474 330 543 395 557 496 517 532 450 469 604 628 430 487 565 497 492 299 659 637 572 629 573 581 473 463 544 640 613 662 542 631 597 435 700 557 715 708 526 606 469 505 637 742 620 618 599 627 561 455 791 594 752 578 615 459 561 481 559 655 569 636 459 528 537 542 571 524 663 624 580 526 600 595 675 661 686 565 466 637 592 514 647 473 675 414 381 412 424 339 402 522 410 433 382 478 315 415 402 304 509 522 499 422 511 467 478 568 436 522 424 440 507 348 446 445 596 587 612 470 520 477 518 655 491 625 542 542 551 485 523 472 674 497 552 454 487 451 467 535 598 471 468 467 498 417 505 372 596 619 601 579 554 552 569 696 544 555 523 515 542 517 596 510 645 594 598 569 578 439 590 698 588 597 422 488 512 501 561 477 566 572 464 479 343 462 435 524 514 502 508 518 433 626 414 423 398 695 495 500 420 485 504 519 462 486 548 506 421 560 399 434 478 611 541 483 532 533 519 635 560 602 560 561 634 546 593 534 690 605 527 471 513 444 543 615 578 523 442 428 517 516 405 469 620 420 364 408 406 462 461 452 549 440 327 346 549 439 454 383 471 440 550 606 469 571 574 532 561 664 458 433 672 645 476 592 588 525 519 573 484 638 586 496 566 653 415 436 671 552 611 575 587 545 599 603 471 544 560 496 665 561 391 450 706 629 594 513 609 602 439 410 375 464 425 524 484 411 519 521 371 516 413 401 418 598 435 475 356 458 449 531 446 463 497 464 441 581 461 464 398 762 586 525 554 649 513 695 554 630 627 654 540 760 549 507 512 724 446 563 437 501 523 530 603 503 574 621 525 616 502 475 524 521 370 406 398 385 345 472 392 479 421 539 379 492 362 387 385 619 492 559 347 503 516 506 501 451 538 508 397 585 400 401 361 638 507 565 455 540 481 572 580 578 559 590 453 629 477 459 509 629 461 461 462 502 399 542 560 453 509 535 385 509 530 491 478 530 373 445 430 443 484 620 444 464 391 535 524 399 625 550 576 524 501 528 431 421 496 583 463 561 478 544 496 384 616 446 629 571 527 494 429 488 417 456 518 443 477 525 400 534 448 406 511 578 489 496 471 489 403 527 494 488 471 558 422 575 414 452 453 466 447 381 462 392 460 538 465 438 423 377 429 386 344 438 593 575 670 549 564 411 548 645 638 596 500 500 572 564 649 539 624 601 569 476 570 490 592 702 591 603 443 610 587 497 527 503 682 589 616 555 641 521 581 771 688 565 480 582 545 540 499 510 675 453 391 481 411 417 454 549 411 430 417 444 487 292 590 543 551 481 422 464 428 377 497 597 438 493 431 523 535 393 576 513 567 558 538 652 515 577 699 810 599 563 607 641 582 402 816 655 658 577 465 536 480 476 532 647 459 609 571 512 580 373 685 633 714 471 381 497 396 438 551 558 393 493 500 521 423 326 608 536 541 504 401 420 380 375 455 543 396 548 382 553 555 396 608 509 586 521 522 492 523 527 626 721 498 571 484 629 584 440 763 540 669 471 461 446 470 413 491 588 475 510 471 492 544 343 629 553 559 400 407 496 408 483 518 461 488 525 395 361 567 540 471 454 492 467 439 474 458 553 516 445 464 523 354 354 599 494 478 505 494 474 417 458 471 459 508 566 462 419 376 445 451 304 588 497 545 518 455 494 423 441 506 579 515 509 464 502 466 343 656 511 571 757 579 529 486 537 574 610 494 654 505 662 545 529 479 472 577 556 439 417 351 487 429 488 497 409 465 573 383 513 476 455 461 476 254 348 323 422 330 416 408 320 403 500 332 433 313 326 331 709 590 589 504 582 493 644 624 585 559 587 487 715 527 537 494 397 440 450 448 483 504 441 415 493 347 289 586 466 391 426 428 431 353 460 416 498 547 454 500 565 372 350 563 460 413 513 501 510 528 699 551 568 666 558 640 690 472 511 782 652 519 613 632 466 538 542 504 585 564 533 555 646 422 347 646 673 516 532 558 440 391 557 385 458 484 454 506 532 387 339 518 478 455 438 520 387 414 524 373 470 455 425 489 462 279 348 584 514 466 390 543 491 458 567 453 520 614 558 569 554 451 437 661 679 546 475 607 416 450 493 451 540 556 494 544 606 392 364 622 572 464 448 517 572 496 436 412 459 530 577 592 474 403 446 479 448 479 514 601 517 538 431 472 500 541 596 516 477 443 462 506 401 472 431 624 448 410 473 322 517 556 509 447 564 486 383 535 519 428 441 437 392 462 542 414 501 523 437 537 552 390 434 648 574 502 417 487 623 530 617 449 483 555 694 733 596 586 580 552 370 732 498 652 476 460 391 397 463 419 564 471 425 347 521 563 345 599 501 510 429 356 322 293 357 380 420 343 438 344 407 421 253 475 471 431 534 485 483 509 480 671 693 480 469 504 553 601 404 737 667 699 666 671 628 611 571 689 720 742 577 491 560 577 590 600 535 725 587 628 422 559 388 528 665 598 645 502 457 543 476 550 491 666 469 505 443 451 421 469 542 527 467 426 464 429 428 469 361 502 540 542 406 399 481 527 507 526 494 373 328 513 367 434 501 614 486 523 355 525 458 496 493 507 423 375 331 468 387 532 436 595 480 526 513 422 474 476 566 514 487 413 516 478 430 570 437 582 618 491 543 501 507 592 653 657 525 523 509 562 484 553 559 701 529 555 484 477 389 442 574 514 539 466 398 475 450 492 442 573 564 420 359 395 353 417 430 329 380 335 399 339 435 361 273 334 602 505 503 413 476 547 528 505 496 445 620 448 554 436 434 456 452 476 549 522 677 645 577 700 574 376 372 631 505 596 519 632 363 365 451 382 468 524 484 537 515 414 338 559 552 493 362 491 523 496 441 486 477 538 596 438 486 457 471 482 485 465 443 522 530 547 477 517 416 490 592 566 512 431 479 452 561 463 428 599 306 366 444 331 382 393 335 395 455 309 296 494 409 394 346 377 463 482 561 457 511 639 611 619 578 481 477 678 685 467 544 593 616 495 486 422 510 516 530 466 468 458 622 543 548 418 469 490 578 547 459 424 538 441 572 526 557 502 554 445 599 457 441 453 605 544 590 452 474 505 559 594 483 475 537 445 547 547 453 425 564 503 553 438 489 421 567 625 559 523 525 460 610 377 462 397 597 376 496 375 462 427 461 482 461 479 505 464 535 388 340 369 778 558 571 543 579 578 653 543 554 549 589 475 635 516 384 464 711 514 494 533 586 578 689 558 528 536 679 568 659 568 477 509 663 508 577 462 460 542 605 573 527 524 578 501 618 489 442 433 602 559 454 577 510 608 664 622 561 524 638 610 448 552 457 709 545 482 444 382 395 447 488 535 486 484 409 510 445 499 472 574 435 363 406 227 306 434 389 411 382 418 463 424 287 457 351 431 524 410 534 412 469 496 616 443 459 434 546 533 333 659 556 580 880 574 597 518 549 620 711 617 600 646 663 535 679 620 519 566 646 451 533 440 478 510 483 485 512 485 609 567 623 426 457 510 430 413 357 383 274 384 425 467 407 342 311 321 395 411 366 469 638 655 613 507 521 563 697 642 610 512 470 571 571 514 609 650 549 407 551 392 453 464 620 553 470 441 404 450 257 584 528 554 430 435 481 492 384 560 613 594 449 398 447 492 288 700 563 537 544 416 467 467 451 545 568 464 540 483 500 569 379 634 551 555 493 387 461 503 438 628 670 420 493 414 473 492 354 691 585 618 441 355 438 361 454 545 611 370 443 479 434 469 285 553 518 549 555 485 540 455 453 664 580 517 594 594 613 593 383 703 627 622 589 491 586 422 528 676 730 580 539 535 559 600 345 699 593 606 529 433 540 485 421 514 587 567 643 529 531 594 376 728 554 647 587 513 545 413 526 481 522 544 504 516 587 439 517 459 421 515 557 480 553 466 462 437 507 553 460 466 565 448 533 465 486 468 302 281 473 281 399 359 418 419 387 248 319 336 418 334 407 417 388 445 532 412 513 540 550 553 474 352 331 564 545 447 400 574 610 559 569 530 529 599 697 629 691 657 674 744 472 786 648 783 648 450 578 387 536 483 710 526 554 509 524 482 375 595 474 625 725 583 527 519 574 553 726 652 612 601 668 540 676 533 589 509 710 644 644 476 558 560 597 601 668 551 617 512 698 536 441 518 393 472 466 438 561 524 486 567 568 387 363 627 403 460 417 508 343 477 542 441 542 576 478 535 526 409 382 600 460 461 509 563 458 427 570 376 589 582 577 559 628 456 446 580 623 474 443 590 445 526 517 422 490 505 489 477 492 403 405 598 528 478 464 619 428 369 478 339 417 498 474 460 523 372 335 506 536 390 463 510 483 484 704 464 609 621 600 546 648 423 490 593 588 468 627 615 566 482 505 478 479 520 618 571 562 411 577 540 397 688 477 571 563 458 486 425 410 545 553 513 556 442 487 487 354 598 482 553 453 457 633 474 561 604 632 632 612 430 452 668 547 430 505 591 362 459 608 473 604 658 478 540 624 348 409 630 644 504 534 569 412 485 396 357 404 389 506 410 337 274 404 436 334 366 351 439 481 497 483 517 492 505 581 585 512 494 561 544 526 586 479 589 490 460 619 601 693 741 659 671 731 447 439 690 755 602 627 645 469 429 492 410 555 504 502 583 610 430 392 622 573 518 446 499 628 554 618 547 546 602 783 661 594 492 625 689 427 807 621 682 596 492 597 554 529 677 707 560 545 577 627 625 443 841 672 731 504 562 389 546 415 512 606 472 582 451 543 458 501 483 404 577 586 503 471 462 423 562 612 552 524 437 551 484 493 494 439 576 616 625 499 525 538 515 659 490 511 489 492 565 498 583 502 651 571 500 499 497 468 502 700 527 505 443 546 473 441 560 492 576 471 445 551 378 571 610 483 511 561 418 390 611 527 537 430 519 429 427 547 326 563 531 478 525 599 422 419 580 540 500 427 510 484 493 453 468 383 501 559 526 488 381 429 481 364 517 419 506 642 611 575 584 572 612 736 593 517 429 587 575 519 588 534 659 606 678 581 543 541 620 700 633 589 497 534 526 515 541 517 675 622 579 471 522 478 546 587 595 590 472 530 584 531 566 393 678 584 439 434 406 430 399 486 411 473 462 516 375 487 465 375 468 699 513 541 492 595 548 587 566 546 477 644 525 614 553 480 513 730 674 539 546 523 591 698 668 672 552 526 681 484 592 544 778 532 577 398 545 426 583 655 592 565 448 459 479 514 447 508 663 475 524 574 568 652 677 569 632 611 455 422 723 576 580 566 683 522 519 671 452 591 689 639 639 588 517 448 643 738 528 593 659 581 405 426 400 414 453 478 451 406 440 518 341 434 447 398 461 681 490 535 485 578 448 562 596 552 560 609 469 542 418 491 582 605 421 456 440 470 444 561 506 546 503 553 426 532 428 392 413 660 542 532 463 476 581 625 503 585 514 683 536 609 455 486 547 576 472 481 497 548 592 580 579 471 496 582 560 462 567 443 641 533 624 453 494 468 504 572 501 536 442 455 520 488 537 447 613 645 475 532 489 516 529 577 583 578 522 669 526 632 442 513 474 728 534 489 471 557 607 553 515 498 516 522 530 655 527 391 452 784 652 665 566 653 607 691 682 615 605 683 577 717 617 651 559 793 654 717 477 642 657 695 673 665 677 779 616 784 629 634 636 409 399 475 447 392 494 568 495 502 473 511 487 324 630 495 573 500 518 516 448 478 597 637 540 451 550 529 588 314 697 649 575 488 353 470 320 355 449 373 435 411 337 417 470 424 327 361 359 577 422 461 407 435 413 548 559 473 527 542 393 519 371 462 442 698 612 567 535 581 626 715 695 574 571 580 613 543 587 526 726 694 630 565 613 632 709 751 725 547 549 593 676 601 601 601 739 366 500 413 396 384 503 465 412 454 459 438 480 271 567 429 525 589 564 518 515 473 603 667 569 539 465 536 553 373 719 567 696 427 441 465 434 422 587 604 493 556 529 420 434 275 638 470 536 608 554 618 433 493 619 644 659 582 543 599 545 389 728 508 649 708 559 468 528 577 496 688 572 584 495 607 462 637 487 513 461 661 560 538 526 582 496 677 674 564 568 656 557 658 549 556 560 390 342 508 396 491 592 483 522 545 342 307 538 508 410 444 460 406 489 591 461 562 646 607 627 638 472 445 614 656 487 579 633 624 544 611 471 486 565 656 609 616 508 568 559 360 703 568 593 467 475 548 401 378 531 613 541 526 485 514 558 317 648 488 540 645 580 613 569 573 633 792 581 623 571 611 659 440 786 655 697 723 554 669 621 548 602 774 706 689 583 680 696 490 828 772 779 454 387 432 352 303 389 489 420 460 375 379 471 288 540 396 528 481 445 475 490 428 564 587 411 490 409 487 511 336 631 564 615 632 525 529 425 515 507 510 505 473 528 635 500 603 470 373 547 597 495 615 442 472 495 458 492 471 457 506 456 458 453 481 498 362 357 438 403 500 523 517 456 484 386 324 503 556 425 422 485 526 548 474 467 566 599 514 528 517 460 366 657 509 534 525 533 374 433 536 468 553 532 500 531 545 378 384 614 538 463 459 557 432 477 595 506 643 583 581 593 570 429 427 623 566 613 494 607 546 448 472 423 452 592 674 547 504 469 588 551 382 716 539 577 612 478 578 542 525 649 784 599 560 470 597 610 420 797 626 665 461 466 411 471 430 510 462 495 503 407 378 481 391 455 375 559 629 567 538 542 401 545 649 685 607 489 556 633 585 567 581 627 434 513 618 457 672 585 495 561 640 473 404 619 553 597 598 647 335 463 600 436 547 517 478 507 568 314 389 538 539 391 604 589 544 555 616 534 682 709 634 729 737 527 530 810 702 586 564 722 536 571 676 575 806 806 656 714 839 563 472 817 695 684 643 658 331 406 366 331 430 416 359 436 375 276 247 413 415 383 430 519 463 474 449 453 533 523 489 402 500 421 314 549 457 475 488 535 565 404 573 436 517 479 639 567 505 476 509 503 332 681 556 628 486 514 499 473 395 500 564 487 577 484 441 481 334 599 423 602 490 490 400 406 349 421 484 483 482 467 393 517 384 469 406 558 577 497 496 534 473 590 685 626 537 526 529 511 474 449 505 611 552 542 443 493 363 530 604 544 503 462 451 426 414 521 408 641 634 597 425 517 491 579 655 671 568 479 658 626 489 569 504 735 343 452 593 474 505 615 525 654 522 310 467 664 570 460 465 577 474 571 605 507 564 638 556 600 618 419 451 718 592 518 487 605 642 464 544 420 494 546 528 546 512 522 563 523 560 468 464 479 856 649 749 545 708 645 674 722 648 685 742 659 761 639 581 651 638 596 442 559 463 636 656 694 546 487 541 592 577 611 529 707 562 571 509 452 432 568 666 557 546 399 566 554 465 512 434 587 707 743 621 635 609 631 759 642 666 628 543 661 639 665 621 797 747 711 518 636 576 729 713 788 767 574 658 717 680 691 669 848 457 397 283 404 343 428 517 442 440 374 391 484 343 360 372 511 536 467 404 458 447 484 596 518 478 445 461 486 378 512 460 554 423 477 560 423 537 580 508 513 592 405 373 650 601 479 407 422 484 423 433 375 583 574 490 546 567 412 384 612 520 483 436 557 679 591 631 496 524 616 638 620 513 562 651 546 607 568 528 547 650 458 482 457 472 475 574 543 510 503 567 455 523 426 451 464 639 527 669 452 585 484 570 645 515 576 572 508 629 436 500 425 777 513 609 513 622 511 591 655 562 594 583 552 665 549 415 443 587 592 630 465 536 501 600 603 532 505 497 514 519 559 495 643 641 590 570 541 554 565 708 636 606 523 603 562 510 578 506 669 473 474 543 438 435 543 685 494 522 496 436 514 303 637 477 616 713 560 631 601 580 684 803 655 666 627 636 687 457 798 778 777 676 535 629 481 534 548 577 635 540 547 623 570 663 517 528 520 774 611 572 499 571 609 706 604 626 646 698 557 712 582 442 519 541 415 442 427 449 473 515 466 395 443 496 412 479 393 351 391 629 512 549 456 453 433 544 475 551 449 509 458 475 468 423 497 629 604 673 500 515 599 613 684 564 521 650 541 636 524 557 540 571 485 561 448 490 448 549 519 496 471 542 509 601 492 419 432 519 521 464 560 441 575 631 609 577 457 635 483 570 489 386 581 498 541 408 540 460 513 574 433 549 499 438 547 447 480 430 638 555 557 568 517 516 629 669 586 610 514 509 636 369 741 541 695 540 486 560 416 456 569 610 481 539 544 505 496 349 699 481 612 579 468 464 495 508 602 678 450 574 439 518 597 401 698 605 670 566 422 518 488 464 608 685 532 600 574 528 644 359 725 703 661 636 406 456 405 470 481 471 359 439 408 510 416 432 380 372 457 781 541 546 543 623 570 608 634 567 569 659 508 662 562 515 513 413 457 446 456 543 540 518 531 518 352 323 623 553 440 459 575 612 619 676 540 755 747 691 694 853 592 523 784 710 645 671 695 638 429 574 503 474 556 689 527 610 511 532 541 400 675 600 649 620 459 702 549 590 653 778 643 629 606 667 667 422 838 691 701 1241 1248 1096 1233 1132 1253 1467 1492 1171 1100 1179 1262 1396 1418 1291 1281 1025 1059 917 969 1014 1018 1230 1270 1016 928 1063 1086 1129 1090 979 1008 923 765 918 783 968 922 901 887 1027 870 727 1021 978 833 800 992 1157 1158 1108 1096 1174 1151 1035 1202 1129 1172 1022 1406 1221 1198 1088 1267 897 968 827 987 1066 980 958 1071 1084 864 729 1175 1065 1053 871 1087 1140 1089 1081 1042 1278 1225 1250 1238 1390 1220 968 1462 1203 1328 1237 1050 1007 1035 1060 1073 1129 1085 1280 1146 1093 1009 1409 1264 1181 986 1133 910 983 1021 939 1260 1194 1109 1171 1290 991 886 1434 1213 1177 975 986 973 800 1043 829 821 1045 1050 976 999 896 935 1001 746 1157 1090 929 918 828 960 919 917 1072 1083 931 868 829 933 900 717 1163 1115 926 1153 1111 1175 1145 1165 1129 1173 1164 1071 1050 1172 1215 1113 1025 989 965 1191 1111 1121 1037 1004 1086 1138 1194 1006 1025 1147 1315 1103 954 903 1051 1127 981 1120 962 915 1123 964 1044 872 905 1056 1061 878 954 837 978 1075 919 1096 906 965 861 954 1048 904 886 1011 991 909 843 874 890 993 1106 911 1032 1005 991 1233 1137 1021 879 1126 1008 1119 1066 1062 1083 985 1062 901 1142 952 1104 1307 1091 901 966 1049 1039 1051 1021 1036 1068 1148 1096 970 1052 1144 1173 1182 1264 946 965 1046 1094 1105 1108 1071 1121 1141 1209 908 1062 956 1079 1281 1175 1009 995 1086 1098 1125 1128 1006 1105 1204 1108 1393 1184 1158 1288 1173 1327 1091 1057 1359 1376 1246 1119 1036 1027 1163 1069 1229 1039 1190 1201 1000 1105 985 1023 1102 1209 1082 1104 852 877 829 916 857 930 894 985 949 1135 944 843 863 888 883 1002 891 881 1164 1148 1002 1083 1002 1061 1307 1417 1057 1011 1112 1170 1343 1244 1295 1099 1221 1094 1154 1148 1341 1333 1215 1351 1376 1158 1037 1517 1467 1368 1158 1273 966 862 821 910 1056 948 879 1110 1100 874 828 1161 1171 1136 842 979 1023 854 1153 980 1086 1092 1188 1198 974 1025 1000 933 721 1251 1252 1040 909 803 1000 1000 962 1137 1172 1172 979 960 969 977 707 1256 1232 949 1010 1211 1004 1062 1023 1078 1264 1144 1038 976 1072 941 1175 1104 1122 1053 1100 961 863 898 924 991 1062 1023 917 888 990 997 1026 991 965 983 973 800 981 891 1000 1062 1139 1059 991 859 1027 936 795 1210 1031 893 1060 866 1042 977 952 1238 1046 1069 1086 898 989 978 752 1070 1119 936 906 748 918 744 830 859 786 844 716 771 1013 959 884 709 634 655 1246 1157 1291 1072 1101 1223 1176 1270 1036 1044 1217 1270 1249 1122 965 934 959 858 1016 783 829 942 847 1032 825 769 950 885 761 833 694 696 1190 1078 1350 1114 1001 1146 1085 1326 1101 947 1173 1262 1114 943 979 1023 1119 1009 1286 1174 1259 1400 1418 1343 1301 1191 1378 1171 943 1458 1402 1133 950 913 1136 951 941 1232 1158 1110 1025 1017 1110 1082 792 1230 1149 943 1034 898 1091 873 874 970 990 1001 805 939 1025 1145 938 854 893 831 1340 1135 1311 1003 1116 1210 1070 1239 1099 1104 1223 1338 1235 1069 916 1064 1264 1235 957 1216 1183 1206 1331 1318 1205 1092 1173 1291 1134 1251 1217 1365 931 1038 771 1051 915 1034 1216 1051 999 983 924 946 1075 972 1027 1096 923 970 985 923 1190 1086 1007 1118 1165 1017 921 1275 953 1013 1019 1053 881 920 990 942 1089 1009 948 1015 1007 1009 868 1224 1046 1084 1053 1069 1208 1102 1207 1024 1028 1146 1078 1119 875 970 1178 1203 1125 1044 998 925 1171 1055 1285 961 1033 1098 1058 1054 985 1006 1181 1129 1161 1035 1050 996 968 864 910 722 1066 1002 891 979 995 933 848 1205 1011 1119 822 947 951 887 895 797 1082 1029 921 1062 1149 889 869 1184 1026 1143 876 1000 866 749 763 795 888 1009 934 828 925 750 924 843 619 920 988 756 1001 924 1122 1118 1066 1401 1331 1093 1029 1139 1198 1152 835 1378 1386 1102 857 698 913 833 757 1018 985 776 904 853 883 961 729 986 1005 836 1022 886 1193 1080 1051 1387 1312 1095 1140 993 1179 1081 889 1371 1417 1089 1070 1115 1208 1084 1377 1143 1176 1234 1274 1214 1035 1443 1276 1424 1230 1286 777 984 1086 936 1101 1013 953 1126 1131 876 862 1247 1174 955 1133 1190 918 819 1083 947 1003 1186 1223 996 969 952 943 931 647 1136 1075 982 1209 978 1160 1058 1128 1294 1291 1177 1141 1100 1140 1159 849 1314 1377 1179 1001 946 1035 1013 1066 1184 1295 1044 1147 1010 1144 1057 757 1271 1321 979 1085 872 1021 1050 1006 1095 1184 1149 1065 1029 1142 956 830 1313 1317 1040 942 1132 853 1120 906 979 1227 1182 1078 972 1063 986 1160 1109 991 1013 1051 999 882 1016 923 1084 1174 1100 891 940 1074 952 1026 1057 988 1021 1043 827 1111 842 814 1066 829 973 901 729 947 1089 838 759 822 844 1021 845 950 854 789 873 934 990 852 890 962 942 928 734 848 861 913 883 860 950 957 1046 1101 1051 852 869 1025 1004 963 1057 947 1065 990 1157 819 989 1025 968 1124 1051 913 1004 958 1066 1069 1051 1005 1012 755 768 872 740 858 853 816 915 893 891 714 1140 884 998 771 797 1038 1008 1096 1038 1082 1202 1206 1232 1087 1127 971 1425 1369 1141 1136 1166 908 744 912 768 956 907 880 872 1003 843 718 994 957 818 782 989 1122 1109 1094 1061 1146 1116 986 1167 1073 1109 1001 1343 1172 1163 1046 1260 1241 1203 1051 1193 1087 1233 1427 1467 1131 1095 1134 1237 1391 1383 1276 1256 1025 1050 908 961 1005 1014 1222 1265 1008 927 1054 1081 1128 1083 976 1003 872 933 817 962 1046 955 923 1046 1044 819 714 1130 1030 1028 841 1082 1095 1026 1063 997 1242 1180 1187 1193 1318 1139 941 1381 1140 1283 1183 1179 1040 993 1031 1050 1065 1119 1071 1270 1130 1075 1003 1391 1250 1171 974 1131 910 983 1021 939 1260 1194 1109 1171 1290 991 886 1434 1213 1177 975 986 1111 945 1108 930 899 1103 948 1040 836 881 1052 1025 866 942 829 958 1071 910 1093 898 961 856 950 1047 895 880 1010 982 906 840 872 885 946 782 1031 805 806 1030 1026 952 999 875 932 977 728 1151 1075 926 900 816 952 903 907 1062 1067 915 868 815 931 884 705 1159 1105 924 1133 1066 1160 1105 1145 1104 1153 1159 1026 1020 1167 1170 1098 1010 979 940 1191 1111 1121 1037 1004 1086 1138 1194 1006 1025 1147 1315 1103 954 903 1051 843 813 719 824 756 853 934 958 783 757 829 829 883 893 854 831 1264 1151 1046 1059 1226 1166 1324 1321 1073 1110 1079 1211 1198 1190 1271 1146 1021 983 729 853 955 837 1029 930 688 849 845 908 965 946 845 945 1132 1136 1010 1095 1097 1109 1403 1181 1065 1074 1203 1019 1281 1275 1178 1064 1193 1089 1395 1086 1106 1183 1048 1283 1071 1061 1245 1310 1139 1048 1014 1011 1369 1246 1395 1143 1210 1416 1228 1333 1197 1202 1383 1442 1301 1205 948 1118 920 932 849 973 825 1034 1139 1075 983 1004 972 979 929 958 937 1021 1207 1212 969 1126 1025 1168 1415 1283 1136 1014 1201 1206 1108 1150 1197 1131 1203 1161 952 1129 1160 1106 1195 1116 1067 1064 1050 1027 1144 1185 1157 1149 1073 1057 867 1086 1036 1087 1215 1215 1255 968 1023 1172 1184 1158 1197 1213 809 896 992 934 946 1180 1086 1004 951 1057 992 1004 741 1167 1134 856 948 874 960 891 928 1123 1126 970 921 863 944 947 719 1177 1084 979 863 885 851 837 946 883 808 1017 851 858 718 1073 993 1027 860 1067 909 882 887 815 998 928 883 889 954 969 773 1124 908 1020 872 1001 1056 895 1085 1027 1072 1289 1212 1192 1020 1027 1214 1019 822 1322 1258 1026 1188 939 1266 1075 1093 1377 1325 1219 1097 966 1159 1126 818 1322 1309 1054 1223 1108 1180 1135 1112 1227 1182 1271 1121 1026 1243 1241 1146 1022 1046 1122 1121 1032 1156 893 867 986 919 950 998 853 907 996 1025 861 753 848 1188 1030 1204 1064 1065 1207 1088 1151 1072 1070 1233 1254 1056 971 871 902 1049 973 1120 928 888 1103 1022 995 969 927 1057 1112 988 821 809 958 1173 894 1233 1131 1040 1343 1380 1089 1101 1098 1171 1149 912 1304 1328 1165 1171 962 1419 1201 1239 1436 1478 1330 1196 1297 1364 1312 976 1545 1533 1200 1097 1063 1215 990 1033 1164 1001 1186 1023 1003 1229 1249 1121 1000 926 942 1093 930 1023 895 818 1018 895 900 827 861 900 995 946 802 724 807 957 854 1052 945 871 1054 875 1033 827 900 1063 1081 943 816 781 829 1146 1048 1226 1072 869 1060 1018 977 945 920 952 1192 1013 917 832 967 1004 903 944 933 1090 1075 1022 1068 1041 976 815 1243 1165 1040 925 978 952 975 813 819 973 1019 865 1022 958 900 727 1164 973 949 897 959 887 925 634 921 877 899 1076 991 875 847 907 949 990 889 953 960 857 903 686 946 822 901 1065 1022 867 819 922 934 871 983 946 957 783 969 1073 1035 1105 1079 1021 1233 1077 882 963 1356 1183 1122 1005 1152 1042 1025 1055 1089 1110 1157 995 1213 1147 1031 891 1328 1091 1161 974 1197 1102 962 1252 1121 1104 1231 1308 1302 1121 1045 1215 1199 933 1412 1325 1096 982 820 988 851 968 1114 1087 967 953 953 1000 978 722 1267 1185 991 1007 955 1027 1038 1090 1408 1360 1186 1173 1208 1094 1056 836 1375 1307 1022 1094 924 1203 938 1005 1189 1088 1270 1076 1030 1083 952 724 1239 1181 1045 1065 1110 1147 1012 1281 1218 1074 1195 1351 1018 950 1406 1183 1175 1045 1219 1200 1104 1298 1162 1331 1352 1231 1280 1298 1177 1022 1570 1338 1322 1306 1275 1014 834 1069 1051 1067 1241 1291 1045 993 1015 1160 1169 904 1240 1268 1027 899 874 951 796 872 1014 1047 969 1022 888 996 911 686 1089 1002 932 864 830 972 929 934 1059 1112 973 906 919 940 912 664 1145 1034 949 988 865 1003 1026 1079 1272 1151 1136 1071 1111 1051 930 725 1268 1209 1022 922 999 740 984 921 963 1068 1119 937 1025 909 1064 1023 1128 1034 1021 913 831 768 929 905 959 1068 1137 881 885 907 864 902 858 900 946 1039 1001 1153 864 940 1095 989 1132 954 890 1145 1110 959 900 847 908 1006 918 1015 877 796 942 944 948 842 815 943 1100 1018 817 764 877 1062 1112 1094 1037 1081 1014 1249 1228 1050 1097 1006 1114 1191 1162 1116 1087 1106 1153 964 1077 1074 1114 1300 1202 1031 1093 1240 1119 1089 1086 1043 1049 1024 954 1058 1057 1260 1176 1098 1183 1175 1036 992 1418 1125 1294 1100 1166 904 867 995 804 963 1083 959 1103 961 932 850 1092 1186 959 936 1025 1053 1066 1120 1092 1231 1059 1167 1204 1155 1066 992 1491 1195 1268 1066 1236 888 900 999 1016 1164 1048 915 1093 1024 942 855 1140 1110 1120 945 1107 1127 1155 949 1100 1090 1133 1286 1334 1088 1056 1118 1116 1263 1203 1164 1200 1289 1243 1090 1234 1339 1373 1435 1557 1091 1157 1348 1317 1298 1476 1229 1255 986 951 1062 929 1152 1080 1094 1091 1142 988 941 1368 1143 1241 1078 1049 870 830 872 860 985 947 815 971 869 829 804 1091 982 920 939 970 829 936 964 859 1062 934 885 894 966 812 869 1291 912 1030 882 959 1024 861 907 942 1088 1221 1135 1228 1139 949 986 1212 1073 1157 947 969 1115 1034 1300 1013 1046 1192 1023 1157 991 982 1185 1163 1034 1053 930 1015 1277 1116 1278 1172 1079 1229 1183 1185 1023 1093 1234 1352 1112 989 1039 999 986 860 1073 998 897 1201 1143 1053 1051 905 1081 1091 844 1306 1095 1052 893 794 953 851 1010 1084 1142 1058 923 946 940 1022 712 1226 1153 984 1046 834 989 859 865 971 878 855 754 797 1017 1065 790 779 778 866 1261 1051 1104 1021 1073 1154 1084 1123 961 1003 1185 1215 1127 998 915 962 1156 1251 1017 1129 1106 1082 1404 1389 1028 1084 1074 1188 1315 1271 1183 1196 953 1025 812 1000 1033 1045 1129 1182 893 873 995 1007 990 960 946 1022 1143 1157 986 1114 976 1180 1299 1230 1018 1105 1091 1018 1109 1251 1153 1191 983 1095 771 1058 943 1036 1178 1114 991 1013 1175 1103 1020 1065 983 1111 1060 840 1066 964 807 989 994 983 922 854 1056 1062 968 842 795 828 1029 999 1185 865 1025 1105 888 1019 936 974 1016 991 1012 853 750 771 1131 1113 935 994 1112 1123 1227 1396 910 965 1000 1074 1179 1200 1230 1110 903 960 809 863 940 986 1019 968 856 856 973 877 841 961 822 948 899 912 886 938 871 1002 1103 1021 862 941 1026 867 1009 1032 980 984 989 1087 879 1077 1052 1010 1236 1125 1028 1005 993 1087 962 1060 1078 1043 998 897 1154 1053 1006 1213 1180 1136 1087 1019 1077 1147 816 1329 1129 1145 1151 1033 1253 1112 1238 1417 1324 1197 1163 1233 1194 1054 882 1466 1323 1044 944 930 978 913 1044 1006 981 1028 957 979 794 1213 1165 1106 899 1243 946 821 900 786 920 945 936 1009 951 951 911 1176 1092 1133 876 996 925 859 967 807 884 1036 1082 971 929 961 1007 930 718 1053 1039 915 1035 942 1110 948 1091 1280 1176 1107 1083 1093 1053 1018 723 1269 1275 931 1093 1049 1192 950 1050 1050 1044 1106 880 982 1239 1206 1044 989 883 963 988 1030 1202 926 939 1003 876 1022 838 851 1044 1129 873 928 831 896 1128 1085 1384 1033 1193 1161 1036 1279 1013 1048 1210 1252 1103 996 941 982 1249 1050 1246 1027 1050 1008 1064 1212 984 955 1129 1191 1167 1023 856 834 976 801 1159 998 994 1214 1112 1027 1062 1084 1125 888 744 1260 1196 952 960 809 896 901 903 1081 1037 970 1034 875 1080 1009 822 1132 1118 973 1082 947 1016 957 783 1081 845 848 806 833 871 1068 881 808 670 863 1168 1114 1173 940 868 1142 1021 1043 1008 865 1185 1249 1058 861 868 921 1072 932 1054 958 907 1141 1001 1126 952 977 1092 1108 1074 915 806 968 1104 991 1086 995 997 1003 998 1107 917 1004 1048 1122 993 985 860 911 947 873 1043 927 1173 1122 1115 1114 1109 1066 935 1303 1183 1159 935 1170 1094 1119 1114 1078 1214 1178 1121 1245 1191 1085 1025 1540 1285 1272 1088 1117 1131 1017 876 961 996 1012 1216 1022 931 1128 1073 1149 1124 1134 1041 1132 966 1045 845 992 843 1030 1128 1050 896 870 907 976 973 925 1011 1032 952 829 854 935 980 870 874 1050 923 852 776 1106 922 1011 824 1017 940 990 1055 1001 1131 1047 1072 1190 1167 1053 945 1292 1217 1212 1127 1160 1027 790 1222 1027 1127 1228 1290 1122 996 1017 1132 982 803 1262 1237 1030 904 891 1049 999 920 1159 1092 1016 1066 924 955 977 728 1049 1049 933 1047 875 1143 1107 1045 1283 1273 1093 1118 1015 1170 1156 912 1308 1343 1098 999 798 1061 1034 1089 1231 1274 1057 1140 1046 1078 1077 778 1256 1375 1056 1007 845 1065 968 1053 1091 976 1074 1094 995 825 1103 1068 1046 945 1069 807 860 984 775 1023 897 924 990 956 804 817 1292 1075 1064 863 1021 970 824 999 795 925 1103 922 1057 892 1007 1008 921 671 1044 1068 795 1131 833 1190 1052 1121 1250 1275 1051 1005 1008 1082 1111 770 1182 1263 1008 927 905 1079 1023 1041 1245 1216 1092 1024 994 1160 1063 760 1358 1136 1059 1029 926 1134 1047 987 1177 1083 1074 1044 973 1056 1045 717 1178 1229 990 1078 1190 934 1047 986 997 1294 1180 1013 1065 1068 1065 1202 1147 1064 1187 1108 1233 992 1227 1188 1155 1338 1457 1048 1129 1145 1259 1198 1271 1129 1194 1154 1119 1139 961 872 1010 940 1095 920 858 1084 1169 1077 923 805 979 1136 1156 1184 1139 1027 1150 1135 1133 1012 994 1146 1248 1114 934 854 959 961 875 813 966 851 1012 1048 972 803 918 895 839 922 879 971 1024 1102 1238 1008 1123 930 1120 1311 1229 1020 1077 1053 1137 1153 1268 1109 1099 954 1021 1043 955 1119 1004 993 1077 1154 1022 796 1346 1201 1097 870 1023 995 879 842 838 1072 1053 911 1112 1083 945 819 1210 1025 1033 903 1053 1117 994 1072 929 1088 1095 993 1146 1114 1134 1009 1450 1264 1208 1018 1223 1018 1008 1114 820 1096 1081 1015 1076 1233 1053 915 1303 1229 1090 1088 1090 1056 1040 771 1084 1058 1047 1194 1111 867 1015 893 1100 1075 1155 1037 1121 840 1050 878 931 979 871 1102 1066 938 869 988 1034 1011 1115 980 971 846 784 978 863 999 927 942 1039 943 853 822 1041 984 1009 952 859 1056 991 975 964 1130 1119 1106 1132 1087 1043 778 1303 1125 1208 924 1041 889 850 1070 945 1049 1037 1060 1077 926 993 866 1236 1124 1177 1001 1097 891 875 944 987 1056 1082 941 1126 1122 1004 831 1224 1119 1085 989 1094 1037 895 1002 897 900 999 904 966 790 906 1006 975 888 872 740 762 1271 1083 1337 1064 1057 1326 1153 1254 1126 1010 1194 1308 1217 1114 1001 971 994 859 1057 1009 1079 1188 1188 1050 970 902 999 970 750 1143 1177 936 1080 959 1186 1080 1082 1356 1286 1284 1103 1109 1145 1091 834 1325 1245 996 1216 1163 1168 1067 1032 1121 1075 1063 1153 941 1233 1273 967 973 909 1037 1051 906 1008 894 939 1050 929 1064 857 891 1105 1065 916 965 885 934 1021 944 908 1117 979 1113 1206 1192 1038 953 1146 1029 1191 1035 968 1047 948 1017 770 1046 975 974 1032 1010 921 971 949 977 997 1012 933 990 1148 1170 1009 1147 1141 1080 1312 1225 1010 1077 1108 1087 1276 1284 1184 1128 1057 1136 947 1196 1026 1066 1211 1142 1054 814 1032 1055 1054 1090 1032 1036 1078 867 1076 868 872 994 890 1040 859 867 1083 1114 1003 848 743 788 1278 996 1133 1035 936 1125 1103 997 893 862 1011 1057 997 872 799 842 800 1083 783 977 973 953 1156 1029 846 865 964 1017 949 989 991 852 940 1079 967 1153 1133 1137 1232 1105 985 1022 1222 1110 1070 1157 1138 1082 1063 1026 972 985 954 1127 1239 1204 919 1026 1007 1082 1048 1146 1149 1089 1030 1099 866 1011 966 940 1148 1167 938 1002 1003 1052 1031 997 968 966 814 802 854 807 896 1001 1058 875 917 893 951 944 684 1087 1128 857 1094 938 1239 1109 1072 1357 1296 1169 1211 1148 1247 1234 946 1361 1365 1143 1112 900 939 794 1099 1118 1052 1018 1134 1153 900 1233 1056 1117 969 949 1021 1029 1093 979 1197 1173 1027 1236 1166 1005 1017 1413 1240 1210 990 1172 1158 940 1216 1025 1101 1255 1264 1320 1133 1101 1084 1144 836 1251 1177 1065 902 836 992 938 975 1054 1122 1006 846 921 1056 1054 803 1197 1145 858 1229 1075 1344 1094 1096 1149 993 1026 1039 963 1112 1192 1059 974 913 1042 1126 931 1070 1021 885 986 993 1040 929 904 1043 1150 955 730 871 973 823 724 921 896 1008 1151 1273 874 919 1004 1001 942 736 1118 1186 943 895 819 983 840 954 1255 1143 918 1039 1075 1103 1010 774 1208 1203 893 1219 1135 1191 1103 1149 1064 1229 1118 1059 1075 1242 1225 1219 1049 959 806 1283 993 1182 1093 921 1189 1066 1165 949 970 1159 1305 1042 926 884 1005 991 1010 1087 991 920 968 861 854 856 939 977 1130 926 975 758 912 1247 1033 1321 1155 1147 1336 1057 1215 1073 997 1232 1290 1170 1098 922 1039 1177 990 1162 1104 1086 1215 1030 1057 1032 964 1197 1255 1056 1040 884 1045 1068 923 1033 939 835 1067 963 967 869 941 1090 1169 986 839 766 757 857 889 856 889 932 939 906 914 930 790 783 1169 996 900 897 890 1100 945 1054 1050 1222 1235 1153 1203 1193 1073 953 1395 1189 1254 1209 1163 1022 1050 847 1052 1053 1101 1181 1029 924 947 1100 1049 1037 1038 1043 974 1065 1166 967 1104 1055 1120 1287 1283 1012 1116 1158 1184 1252 1198 1068 1142 997 974 963 996 1250 1146 1105 1267 1216 946 853 1280 1034 1268 1049 1125 912 850 915 898 994 964 1012 1044 1023 1038 732 1249 1049 1052 887 935 1108 923 1139 953 967 1162 1122 1196 1187 1060 1184 1107 847 1254 1176 1128 1114 873 1140 957 1000 1189 1228 1163 1036 1084 1094 928 761 1282 1189 1067 1030 869 989 790 949 904 1030 913 1030 1013 829 1215 1066 1057 924 977 1021 932 1067 896 1064 1032 1040 1044 1069 867 919 1167 1053 1002 985 996 994 966 1168 1054 1216 1462 1448 1142 1100 1121 1217 1088 843 1395 1293 1051 1083 925 1193 1025 1071 1264 1299 1076 1204 1137 1142 1108 816 1258 1317 1112 873 833 912 949 990 1120 1138 1064 926 1013 1031 997 712 1125 1139 898 960 971 1176 1110 1112 1279 1282 1171 1044 1206 1231 1173 790 1425 1441 1109 1103 903 1168 945 1004 1245 1207 1182 1010 1077 1108 1086 767 1257 1277 1007 924 811 1001 999 876 1119 1098 1000 1022 1001 1039 984 725 1211 1113 975 1529 1210 1366 1115 1065 1279 1293 1289 1157 1091 1298 1314 1251 1157 1089 1107 1087 941 1152 916 918 1012 923 965 925 849 1042 1183 1008 853 859 996 951 945 808 838 977 895 1025 1122 844 896 909 877 964 1015 824 869 1216 1242 964 1030 1261 1174 1310 1323 1027 1057 988 1239 1176 1209 1274 1212 1041 1204 912 1183 1084 1071 1235 1269 980 991 1150 1064 1050 1110 1031 1136 997 1012 883 914 939 928 1109 1127 936 954 891 992 1075 1099 1100 980 946 926 1001 906 1196 1087 980 1130 1142 915 947 1305 1185 1230 1041 1114 1001 993 1021 965 1067 950 887 1100 971 868 874 1225 1080 1160 929 1027 1014 1109 1131 993 1131 1124 1059 1206 1226 1017 971 1486 1241 1105 1107 1215 1054 977 1053 1068 1212 1072 1057 1106 1180 1013 904 1243 1227 1148 1026 1157 964 982 847 1027 899 999 1068 1059 901 771 945 949 917 1095 1050 972 1030 984 936 965 1061 1061 1247 1133 888 816 985 1014 1040 1080 1004 1034 837 831 935 871 1013 1068 1016 1128 1064 884 832 1231 1072 1031 977 905 993 1001 1172 1044 1149 1210 1206 1192 1149 1207 1054 1370 1306 1226 1215 1146 917 889 1135 996 1106 1072 1099 1166 1078 1069 949 1374 1150 1041 1041 1080 821 868 1002 935 1032 988 980 938 1037 914 778 1269 1041 1059 976 987 1202 1041 1225 1103 1192 1341 1424 1275 1289 1266 1289 1311 1021 1467 1482 1229 1139 778 1120 849 1039 1043 1188 1114 1057 1005 1021 936 787 1070 1123 975 1115 1079 1261 1114 1055 1164 1005 1124 1027 990 1124 1317 1096 1009 912 1010 1037 1046 1124 913 1031 1046 993 1062 1010 966 1063 1140 1029 916 820 848 1087 1038 1123 950 929 1009 1004 998 982 875 1038 1100 957 837 876 948 1024 1090 1149 1045 949 1017 992 1061 972 944 1098 1197 983 867 859 915 993 1112 965 1050 1011 1015 1275 1173 1075 909 1132 1032 1143 1084 1068 1107 985 1066 937 1154 956 1120 1335 1115 937 986 1053 1055 1067 1033 1040 1084 1148 1105 1051 1079 1153 1209 1245 1318 1027 1010 1055 1130 1141 1135 1080 1157 1141 1216 971 1083 963 1107 1330 1217 1072 1030 1093 1126 1153 1149 1013 1133 1224 1153 1408 1224 1178 1313 1193 1332 1136 1087 1364 1421 1261 1134 1046 1052 1167 1078 1232 1047 1194 1206 1004 1106 994 1029 1103 1218 1085 1107 854 882 829 970 911 978 948 1009 997 1165 992 849 917 918 889 1044 909 911 1164 1193 1047 1123 1047 1081 1347 1442 1097 1016 1157 1195 1348 1279 1310 1124 1010 1265 1058 1110 1077 1102 1312 1174 1086 982 1126 971 1181 1146 1140 1083 1100 1015 917 946 978 1015 1110 1053 965 894 1044 1027 1032 1033 983 1013 1221 1094 1154 1148 1341 1333 1215 1351 1376 1158 1037 1517 1467 1368 1158 1273 976 876 825 920 1064 958 893 1120 1116 892 834 1179 1185 1146 854 981 1104 908 1189 1052 1131 1137 1260 1270 974 1088 1009 1005 775 1269 1297 1049 945 827 1016 1032 982 1157 1204 1204 979 988 973 1009 731 1264 1252 953 1054 854 1017 963 1045 1107 1211 1131 991 922 1036 1008 849 1228 1076 902 1141 920 1078 1049 997 1283 1118 1141 1086 961 998 1050 806 1088 1164 945 926 773 958 789 865 869 826 854 726 801 1013 989 884 734 679 665 1278 1197 1355 1144 1157 1239 1240 1286 1052 1092 1217 1318 1249 1162 1037 950 971 885 1025 807 841 957 859 1035 852 787 953 912 770 842 700 711 1218 1141 1371 1170 1029 1181 1113 1333 1164 989 1180 1325 1135 964 993 1058 1164 1039 1306 1214 1284 1425 1458 1383 1301 1226 1383 1211 973 1468 1427 1138 959 919 1140 959 946 1237 1166 1118 1025 1024 1111 1090 798 1232 1154 944 1054 943 1106 913 894 995 1010 1006 850 969 1030 1190 953 869 903 856 1376 1216 1338 1075 1152 1255 1106 1248 1180 1158 1232 1419 1262 1096 934 1109 1216 1120 1213 1040 1036 1156 1086 1121 893 982 1180 1221 1131 1050 1002 935 1171 1055 1285 961 1033 1098 1058 1054 985 1006 1181 1129 1161 1035 1050 996 1264 1235 957 1216 1183 1206 1331 1318 1205 1092 1173 1291 1134 1251 1217 1365 931 1056 789 1067 933 1042 1232 1061 1015 985 942 956 1077 986 1033 1106 1013 927 928 767 1102 1047 954 1024 1067 1014 875 1286 1074 1164 876 956 996 950 913 842 1118 1074 984 1107 1221 970 896 1265 1089 1188 930 1009 968 1033 1003 968 1226 1131 1070 1163 1237 1098 948 1356 1016 1058 1073 1062 901 948 998 962 1105 1029 976 1035 1039 1045 880 1260 1074 1104 1077 1073 896 769 773 800 928 1054 974 858 935 790 949 853 649 960 1018 761 1049 956 1138 1126 1130 1473 1395 1141 1045 1203 1238 1168 883 1442 1434 1110 1095 1150 1218 1109 1397 1168 1211 1259 1314 1259 1050 1488 1311 1449 1260 1291 782 991 1088 941 1105 1018 960 1131 1139 885 865 1256 1181 960 1139 1191 884 716 925 857 772 1033 1009 800 904 874 886 985 747 992 1020 839 1085 928 1221 1136 1086 1422 1368 1151 1140 1042 1186 1137 931 1385 1452 1096 963 849 1103 987 1028 1211 1263 1036 969 987 948 971 677 1146 1100 987 1290 1032 1196 1130 1173 1339 1363 1249 1141 1163 1149 1231 903 1332 1422 1188 1019 958 1043 1029 1076 1194 1311 1060 1147 1024 1146 1073 769 1275 1331 981 1085 872 1021 1050 1006 1095 1184 1149 1065 1029 1142 956 830 1313 1317 1040 1055 854 1120 866 826 1081 841 976 928 747 950 1116 847 768 828 859 1029 863 956 870 797 883 942 992 870 902 964 960 934 740 852 871 913 964 941 1022 1038 1082 1173 1096 924 878 1106 1049 972 1120 974 1110 990 1238 900 1061 1106 1004 1196 1096 985 1013 1039 1111 1078 1114 1032 1057 942 1213 934 1192 987 1015 1299 1227 1150 981 1144 1031 1169 1172 1018 1058 1051 1035 918 1048 959 1100 1206 1120 923 944 1110 972 1030 1085 1000 1041 765 808 882 765 898 858 841 955 933 891 734 1170 919 1003 776 842 1054 1072 1112 1078 1146 1210 1246 1296 1151 1127 1003 1473 1425 1149 1144 1238 1159 1194 1160 1228 1181 1364 1309 1343 1155 1215 1384 1072 1236 1515 1369 1279 1263 1389 1147 1462 1404 1355 1488 1418 1313 1230 1331 1351 1192 1424 1469 1288 1269 1119 1060 1308 1222 1313 1250 1261 1032 1209 1266 1124 1266 1228 1364 1229 1304 1496 1282 1404 1225 1373 1484 1515 1276 1272 1387 1351 1351 1622 1433 1313 1340 1082 1291 1302 1057 1224 1237 1322 1241 1156 1252 1350 1197 1107 1070 1063 1209 1229 1410 1101 1283 1274 1150 1253 1173 1279 1222 1282 1179 1100 984 982 1332 1319 1587 1272 1417 1363 1263 1452 1271 1369 1424 1522 1314 1256 1239 1169 1420 1191 1365 1173 1198 1144 1209 1386 1173 1169 1306 1360 1288 1200 1011 994 1163 1186 1131 1129 1250 1247 1190 1288 1152 1237 1046 1489 1413 1419 1029 1460 1228 1141 1051 1065 1168 1206 1224 1317 1192 1244 1176 1468 1336 1490 1020 1216 1326 1015 1397 1346 1278 1468 1372 1295 1197 1328 1442 1158 1019 1387 1466 1180 1304 1024 1168 1142 1132 1314 1150 1179 1179 1079 1267 1235 1033 1289 1380 1084 1349 1255 1423 1314 1257 1340 1184 1380 1281 1225 1328 1568 1226 1233 1105 1225 1401 1275 1384 1198 1349 1348 1252 1383 1350 1339 1291 1491 1272 1238 1156 1085 1391 1111 1406 1372 1247 1478 1447 1288 1209 1240 1415 1338 1131 1464 1541 1284 1261 921 1262 1202 1259 1334 1405 1185 1213 1241 1272 1235 921 1363 1527 1201 1328 1205 1122 1206 1247 1232 1343 1284 1139 1300 1369 1340 1347 1431 1337 1328 1179 1318 1091 1303 1132 1362 1351 1337 1171 1085 1252 1241 1195 1229 1298 1294 1465 1436 1348 1327 1317 1352 1374 1300 1390 1308 1474 1523 1208 1271 1204 1290 1412 1247 1294 1223 1296 1382 1328 1423 1238 1373 1413 1476 1203 1394 1322 1289 1311 1095 1267 1217 1074 1360 1238 1439 1213 1294 1215 1344 1260 1164 1069 1171 1383 1285 1303 1342 1250 1281 1268 1396 1280 1351 1267 1462 1217 1249 1180 1172 1228 1092 1102 1373 1176 1322 1345 1418 1267 1141 1324 1135 1384 1362 1309 1196 1188 1180 1037 1241 1256 1199 1158 1281 1125 1167 1209 1182 1225 1337 1281 1196 1224 1042 1231 1061 1143 1352 1082 1203 1048 1187 1248 1148 935 1153 1231 1028 1466 1094 1491 1360 1321 1452 1455 1220 1167 1216 1361 1412 986 1341 1484 1212 1279 1203 1222 1229 1215 1341 1214 1345 1253 1281 1098 1326 1355 1373 1112 1252 1014 1164 1127 1000 1199 1154 1159 1260 1157 1039 1040 1581 1315 1414 1003 1203 1428 1076 1420 1273 1382 1305 1412 1460 1097 1335 1265 1224 1030 1401 1513 1234 1390 1103 1364 1383 1313 1493 1411 1462 1185 1232 1355 1323 1069 1450 1563 1229 1465 1185 1477 1288 1303 1511 1430 1470 1276 1285 1340 1400 1048 1380 1429 1270 1358 1202 1394 1288 1312 1444 1337 1234 1088 1213 1403 1407 1207 1415 1487 1149 1329 1269 1214 1182 1303 1300 1235 1417 1267 1314 1231 1681 1530 1552 1134 1426 1203 1214 1247 969 1270 1162 1183 1282 1304 1141 1045 1450 1408 1286 1170 1231 1088 1038 1146 1057 1145 1126 1196 1298 1078 1018 1045 1235 1154 1294 1093 1060 1360 1325 1153 1237 1369 1304 1428 1445 1213 1201 1025 1532 1358 1502 1098 1284 1446 1305 1316 1178 1056 1292 1181 1384 1232 1192 1262 1430 1277 1210 1081 1232 1429 1410 1417 1423 1288 1404 1417 1437 1326 1359 1402 1552 1379 1283 1223 1211 1177 1124 1322 1291 1221 1478 1407 1250 1206 1093 1389 1319 966 1471 1357 1218 1387 1218 1379 1346 1254 1437 1351 1306 1172 1224 1343 1321 1051 1326 1490 1209 1494 1355 1565 1345 1347 1448 1281 1257 1407 1362 1328 1576 1260 1279 1225 1333 1491 1280 1425 1453 1188 1333 1375 1428 1393 1390 1346 1566 1283 1175 1316 1323 1272 1293 1027 1362 1354 1297 1404 1367 1116 1270 1169 1274 1348 1508 1344 1331 1066 1213 1076 1119 1247 1161 1249 1308 1150 1105 1286 1234 1229 1445 1264 1222 1086 1185 1151 1177 1172 1320 1278 1299 1060 1250 1122 1462 1301 1485 1134 1320 1190 1201 1155 1288 1317 1347 1226 1428 1329 1360 1175 1494 1456 1435 1191 1322 1284 1239 1083 1248 1484 1329 1421 1643 1386 1074 1133 1553 1282 1622 1176 1370 1299 1260 1165 1245 1321 1304 1435 1522 1292 1361 1153 1643 1382 1579 1104 1281 1316 1408 1206 1383 1385 1376 1458 1491 1222 1319 1417 1302 1448 1644 1488 1334 1228 1285 1052 1419 1200 1273 1347 1313 1230 1018 1229 1179 1212 1346 1284 1145 1329 1092 1360 1303 1305 1405 1428 1212 1117 1130 1301 1255 990 1298 1386 1182 1480 1204 1509 1432 1377 1626 1524 1523 1245 1400 1499 1327 1125 1501 1486 1286 1016 1264 1021 1247 1175 1137 1311 1278 1020 1042 1155 1171 1113 1239 1277 1010 1210 1255 1174 1469 1372 1443 1426 1378 1260 1255 1454 1279 1228 1515 1520 1304 1550 1236 1496 1264 1202 1379 1346 1424 1315 1358 1455 1406 1139 1474 1429 1320 1619 1265 1543 1439 1359 1549 1575 1485 1239 1447 1485 1321 1153 1497 1532 1393 1225 1282 1225 1156 1433 1282 1280 1414 1357 1290 1160 1533 1237 1312 1219 1219 1239 1285 1217 1254 1325 1290 1264 1403 1259 1290 1185 1489 1381 1501 1268 1298 1378 1081 1317 1069 1130 1264 1171 1318 1171 1263 1265 1469 1193 1182 1062 1055 1547 1288 1412 1337 1255 1396 1453 1307 1214 1287 1288 1463 1218 1227 1174 1125 1437 1300 1324 1289 1294 1259 1430 1386 1310 1323 1413 1462 1340 1260 1178 1011 1640 1284 1383 1402 1185 1467 1302 1489 1357 1361 1408 1646 1279 1217 1193 1301 1313 1204 1311 1218 1159 1199 1050 1086 1167 1213 1192 1353 1150 1249 1043 1121 1604 1226 1527 1395 1377 1609 1265 1499 1394 1328 1410 1556 1391 1403 1215 1281 1258 1304 1201 1231 1527 1317 1339 1480 1346 1204 1239 1613 1498 1607 1243 1365 1397 1524 1287 1382 1377 1341 1308 1517 1221 1260 1279 1635 1501 1680 1171 1386 1226 1416 1119 1462 1329 1407 1403 1517 1116 1211 1418 1301 1307 1468 1353 1357 1361 1316 1216 1296 1317 1329 1334 1524 1212 1274 1305 1305 1387 1564 1581 1291 1251 1188 1215 1196 1173 1358 1344 1458 1099 1195 1225 1222 1254 1405 1435 1324 1275 1382 1155 1301 1293 1177 1341 1463 1235 1215 1339 1267 1230 1368 1324 1169 1408 1257 1084 1050 1334 1280 1352 1331 1248 1349 1152 1447 1294 1407 1124 1191 1299 1411 1255 1254 1414 1405 1323 1580 1308 1216 1290 1665 1524 1622 1127 1420 1231 1383 1093 1464 1222 1221 1490 1443 1409 1233 1382 1156 1360 1483 1325 1202 1375 1280 1211 1373 1206 1435 1441 1481 1224 1251 1417 1182 1293 1525 1384 1319 1226 1006 1302 1179 1252 1434 1411 1068 1138 1211 1265 1240 1041 1325 1456 1135 1328 1157 1341 1159 1249 1578 1303 1154 1255 1306 1412 1309 1101 1418 1517 1091 1496 1236 1424 1420 1373 1430 1277 1390 1344 1378 1427 1607 1260 1331 1190 1266 1381 1043 1228 1060 1047 1375 1158 1142 1140 1237 1163 1421 1171 1103 1034 948 1256 1143 1392 1230 1448 1662 1622 1268 1240 1306 1431 1295 1089 1511 1467 1226 1542 1152 1527 1325 1374 1469 1593 1335 1321 1485 1472 1386 1162 1441 1585 1380 1429 1211 1340 1215 1093 1401 1234 1314 1202 1285 1286 1388 1103 1324 1248 1223 1432 1200 1374 1229 1285 1177 1280 1397 1274 1329 1265 1383 1135 1206 1208 1217 1431 1300 1393 1239 1266 1287 1288 1310 1328 1270 1298 1439 1203 1141 1218 1199 1342 1297 1325 1268 1154 1296 1245 1330 1265 1310 1263 1438 1212 1172 1174 1168 1205 1049 1178 1176 1272 1391 1319 1236 1069 1291 1290 1226 1055 1265 1374 1176 1307 1210 1470 1409 1368 1542 1496 1359 1223 1428 1552 1466 1111 1566 1684 1389 1309 1219 1167 1061 1194 1199 1352 1261 1244 1270 1115 1512 1270 1476 1096 1235 1320 1307 1277 1212 1329 1359 1384 1430 1307 1162 1270 1507 1366 1471 1193 1275 1250 1309 1073 1394 1304 1396 1379 1312 1173 1181 1356 1237 1235 1367 1418 1159 1290 1478 1212 1431 1357 1462 1486 1597 1229 1441 1493 1422 1533 1603 1448 1362 1267 1421 1210 1401 1345 1387 1511 1503 1433 1183 1525 1280 1284 1514 1477 1370 1304 1417 1326 1577 1410 1509 1593 1533 1313 1325 1492 1375 1391 1540 1561 1398 1529 1151 1501 1231 1291 1524 1378 1432 1234 1241 1406 1370 1067 1415 1590 1183 1213 1077 1306 1246 1049 1408 1221 1116 1211 1134 1280 1269 998 1376 1323 1191 1204 1233 1334 1282 1243 1491 1357 1212 1136 1319 1330 1282 1119 1343 1339 1163 1392 1232 1324 1244 1218 1441 1334 1210 1166 1097 1285 1350 1081 1370 1439 1211 1870 1491 1605 1420 1335 1585 1615 1619 1513 1521 1554 1700 1450 1506 1444 1410 1314 1136 1348 1126 1075 1183 1141 1209 1193 1180 1261 1425 1181 1133 1134 1210 1484 1128 1367 1222 1365 1443 1385 1349 1194 1173 1334 1293 1202 1420 1402 1138 1425 1170 1366 1305 1191 1543 1326 1259 1262 1125 1264 1326 1077 1220 1376 1195 1219 1384 1279 1151 1303 1283 1263 1439 1340 1247 1189 1686 1429 1350 1245 1398 1370 1334 1283 1342 1522 1285 1337 1439 1353 1313 1183 1507 1530 1521 1184 1388 1151 1063 1228 1113 1257 1298 1241 1416 1269 1144 1120 1451 1350 1362 1107 1133 1320 1281 1392 1300 1379 1433 1486 1516 1316 1419 1341 1573 1532 1559 1372 1382 1286 1208 1101 1344 1164 1361 1262 1403 1180 1048 1286 1161 1112 1471 1423 1281 1364 1210 1222 1236 1377 1426 1418 1495 1168 1114 1356 1253 1265 1542 1433 1351 1162 949 1160 1025 1064 1064 1007 1090 1000 1098 1162 1235 1040 909 854 819 1572 1475 1561 1454 1378 1485 1521 1624 1409 1411 1458 1635 1448 1469 1313 1239 1145 1199 1307 1234 1283 1407 1319 1471 1297 1323 1211 1656 1381 1469 1194 1304 1129 1102 1171 1201 1248 1202 1270 1242 1244 1118 1055 1444 1225 1349 1137 1204 1290 1286 1147 1219 1340 1361 1372 1423 1221 1255 1136 1545 1440 1477 1118 1281 1354 1337 1089 1171 1294 1342 1255 1444 1211 1202 1120 1498 1291 1382 1144 1263 1371 1301 1256 1317 1377 1372 1369 1549 1213 1188 1281 1262 1310 1372 1383 1278 1422 1576 1215 1508 1309 1430 1599 1536 1414 1359 1505 1351 1419 1563 1400 1334 1331 1238 1177 1054 1366 1350 1221 1386 1333 1328 1211 1583 1394 1595 1042 1212 1285 1239 1049 1091 1350 1289 1289 1442 1374 1167 1192 1505 1281 1526 1062 1282 1517 1381 1656 1531 1401 1559 1448 1648 1469 1414 1645 1728 1460 1435 1363 1279 1420 1320 1464 1370 1440 1426 1254 1412 1287 1371 1353 1508 1324 1411 1181 1122 1242 945 1043 1023 1159 1276 1158 1053 1075 954 1136 1108 828 1085 1262 879 1425 1199 1421 1435 1477 1778 1672 1375 1210 1518 1509 1482 1226 1577 1696 1363 1126 1177 1146 1279 1210 1207 1185 1444 1254 1082 1197 1158 1157 1319 1232 1056 1457 1422 1301 1488 1281 1346 1570 1744 1363 1263 1418 1377 1582 1616 1655 1331 1590 1398 1560 1384 1492 1687 1627 1453 1545 1458 1595 1672 1407 1633 1783 1465 1467 978 1414 1066 1257 1220 1362 1268 1197 1226 1236 1134 1020 1214 1308 1164 1296 1211 1326 1230 1138 1226 1308 1307 1141 1308 1301 1506 1141 1156 1222 1105 1656 1450 1526 1343 1392 1484 1359 1544 1462 1514 1458 1689 1457 1387 1176 1385 1311 1450 1343 1395 1316 1416 1476 1524 1346 1275 1472 1127 1444 1574 1483 1401 1366 1192 1157 1218 1130 1291 1297 1352 1220 1049 1247 1227 1151 1293 1271 1255 1181 1334 1089 1361 1235 1244 1247 1509 1194 1285 1270 1294 1206 1531 1472 1244 1319 1036 1078 1311 1227 1321 1315 1506 1246 1169 1253 1083 1141 1335 1361 1272 952 1102 989 955 1029 1072 1003 1169 1129 1118 889 1409 1157 1264 903 996 1362 1464 1330 1305 1360 1448 1551 1627 1354 1450 1286 1801 1722 1495 1346 1470 1355 1220 1346 1187 1251 1287 1216 1384 1231 1264 1255 1321 1082 1242 1114 1029 1557 1395 1696 1518 1297 1451 1404 1659 1525 1427 1437 1670 1393 1347 1353 1328 1570 1419 1411 1438 1630 1642 1577 1755 1624 1466 1424 1855 1754 1788 1396 1584 1165 1186 985 1117 1268 1183 1117 1371 1237 1084 1034 1445 1378 1491 952 1224 1248 1194 1222 1310 1326 1401 1362 1450 1211 1164 1470 1314 1270 1512 1354 1378 1207 1480 1149 1396 1336 1288 1362 1385 1165 1205 1298 1321 1298 1404 1369 1262 1545 1318 1592 1501 1627 1727 1692 1592 1488 1485 1706 1492 1310 1613 1686 1403 1322 1160 1395 1310 1181 1415 1395 1368 1141 1268 1471 1314 1046 1384 1396 1183 1396 1487 1489 1364 1618 1420 1468 1570 1502 1540 1353 1756 1651 1801 1455 1548 1049 1327 1265 1238 1329 1249 1220 1418 1234 1084 1147 1452 1468 1322 1281 1416 1541 1287 1604 1316 1322 1573 1327 1513 1452 1421 1415 1538 1315 1437 1313 1320 1650 1372 1532 1436 1365 1526 1436 1466 1404 1523 1444 1709 1354 1320 1366 1267 1429 1243 1334 1322 1326 1317 1319 1425 1344 1303 1337 1392 1254 1208 1158 1152 1576 1373 1394 1285 1297 1464 1468 1497 1409 1509 1389 1756 1340 1360 1330 1364 1371 1079 1302 1259 1098 1401 1223 1088 1131 1144 1286 1350 1127 1202 1406 1123 1521 1264 1582 1526 1370 1720 1596 1409 1368 1272 1529 1489 1253 1560 1768 1351 1012 985 1008 1053 966 1109 1118 1252 1065 962 1094 1016 1112 1197 1118 1071 1525 1451 1437 1439 1601 1413 1607 1662 1400 1387 1410 1466 1515 1542 1659 1344 1428 1352 1388 1276 1302 1306 1300 1319 1092 1250 1397 1445 1321 1300 1219 1143 1601 1314 1560 1248 1337 1458 1386 1447 1391 1464 1417 1524 1431 1463 1446 1332 1278 1147 1341 1233 1309 1504 1453 1209 1155 1179 1218 1238 995 1275 1361 1190 1634 1264 1470 1423 1386 1527 1561 1437 1298 1372 1442 1535 1130 1465 1684 1390 1637 1445 1305 1565 1478 1481 1510 1699 1494 1315 1502 1498 1342 1666 1672 1633 1149 1266 991 1352 1193 1297 1352 1335 1215 1235 1252 1139 1290 1289 1375 1287 1387 1060 1352 1211 1321 1489 1421 1352 1158 1261 1412 1284 1082 1444 1489 1211 1677 1331 1716 1421 1397 1727 1534 1429 1348 1223 1532 1489 1221 1573 1604 1341 1555 1450 1406 1521 1421 1452 1621 1830 1451 1347 1466 1460 1643 1796 1721 1454 1266 1295 1105 1256 1291 1331 1410 1535 1273 1243 1357 1249 1359 1520 1343 1228 1314 1140 1246 1277 1278 1386 1429 1252 1257 1228 1378 1280 979 1383 1568 1152 1567 1197 1457 1458 1357 1469 1441 1402 1326 1320 1550 1351 1227 1529 1641 1375 1382 1081 1331 1127 1085 1393 1111 1287 1276 1097 1183 1394 1091 1075 1148 1150 1306 1013 1192 1126 963 1147 1149 1237 1157 1193 1120 1169 1172 1032 1163 1058 1495 1233 1592 1439 1373 1594 1487 1389 1312 1357 1440 1541 1218 1552 1446 1480 1624 1310 1651 1432 1525 1704 1546 1440 1353 1506 1500 1377 1218 1669 1618 1291 1178 1261 1055 1243 1297 1248 1239 1394 1281 1154 1076 1467 1373 1414 1069 1346 1452 1388 1259 1299 1530 1409 1555 1576 1519 1371 1232 1703 1437 1640 1395 1424 1333 1128 1187 1171 1282 1272 1285 1346 1304 1160 1144 1351 1351 1344 1019 1297 1490 1542 1361 1439 1424 1514 1365 1556 1346 1434 1354 1730 1530 1652 1283 1581 1491 1356 1386 1415 1370 1466 1463 1575 1409 1353 1487 1548 1333 1315 1316 1383 1547 1379 1441 1256 1209 1363 1308 1328 1429 1372 1202 1389 1335 1295 1168 1225 1303 1202 1205 1268 1263 1304 1323 1574 1297 1206 1230 1619 1478 1486 1100 1297 1309 1429 1251 1264 1538 1498 1513 1621 1513 1271 1242 1751 1490 1649 1194 1301 1211 1153 1180 1248 1158 1255 1298 1397 1244 1207 1297 1195 1149 1352 1344 1238 1478 1478 1244 1544 1362 1519 1699 1579 1457 1281 1542 1410 1347 1601 1588 1398 1041 1238 1281 1222 1320 1264 1210 1472 1255 1142 1169 1589 1413 1370 1135 1343 1420 1421 1310 1444 1455 1465 1410 1658 1394 1328 1272 1650 1395 1642 1210 1532 1332 1066 1344 1124 1070 1278 1289 1153 1140 1178 1275 1291 1049 1334 1347 1213 1193 1090 1203 1255 1117 1310 1317 1101 1010 1009 1247 1154 958 1287 1308 1198 1452 1221 1521 1408 1391 1513 1500 1502 1302 1270 1504 1500 1219 1550 1599 1309 1450 1202 1403 1235 1301 1508 1374 1191 1205 1244 1365 1410 1122 1465 1544 1314 1524 1352 1592 1386 1365 1424 1294 1589 1392 1436 1463 1555 1395 1351 1302 1300 1553 1361 1522 1282 1317 1575 1357 1484 1385 1418 1460 1630 1426 1392 1156 1243 1348 1341 1313 1278 1472 1437 1484 1516 1376 1436 1286 1644 1546 1599 1158 1491 1412 1471 1326 1370 1490 1462 1432 1610 1432 1352 1302 1842 1554 1698 1255 1375 1396 1265 1069 1240 1315 1274 1309 1334 1043 1197 1260 1204 1284 1500 1302 1314 1471 1390 1392 1507 1468 1479 1657 1594 1451 1381 1605 1272 1555 1788 1645 1418 1423 1529 1316 1482 1348 1349 1564 1613 1399 1364 1460 1374 1498 1562 1548 1462 1411 1526 1284 1625 1480 1531 1590 1832 1379 1435 1538 1507 1448 1693 1513 1492 1379 1337 1497 1325 1314 1405 1300 1504 1352 1399 1462 1593 1362 1333 1227 1198 1463 1160 1269 1169 1026 1307 1113 1201 1233 1281 1078 1366 1148 1122 1025 1065 1252 1241 1099 1173 1278 1085 1153 1387 1021 1139 1073 1318 1330 1371 1049 1342 1205 1208 1067 1122 1189 1169 1156 1153 1088 1188 1060 1312 1187 1325 1029 1252 1312 1308 1319 1347 1330 1205 1436 1481 1325 1291 1279 1292 1427 1410 1417 1234 1502 1454 1302 1485 1373 1522 1530 1655 1397 1404 1638 1379 1264 1574 1550 1378 1334 1270 1262 1302 1480 1393 1399 1529 1360 1264 1286 1690 1406 1628 1294 1393 1349 1293 1253 1174 1316 1406 1414 1607 1239 1243 1276 1499 1539 1454 1170 1400 1555 1401 1529 1486 1465 1500 1444 1534 1472 1518 1545 1725 1343 1380 1261 1216 1364 1246 1365 1282 1146 1354 1301 1312 1289 1305 1300 1414 1233 1141 1157 1207 1508 1111 1503 1419 1251 1542 1615 1281 1238 1338 1459 1428 1188 1432 1591 1413 1376 1122 1606 1390 1371 1593 1606 1448 1304 1397 1540 1461 1161 1639 1649 1343 1417 1330 1119 1390 1400 1380 1396 1314 1251 1304 1303 1206 1379 1532 1411 1351 1448 1377 1211 1545 1363 1490 1498 1630 1578 1312 1388 1422 1484 1617 1672 1527 1330 1074 1173 1107 1079 1251 1112 1220 1078 1195 1147 1238 1057 1125 1031 970 1549 1339 1550 1393 1254 1578 1371 1541 1501 1356 1409 1635 1406 1348 1287 1204 1443 1445 1277 1420 1384 1340 1595 1669 1264 1318 1338 1374 1553 1642 1556 1404 1309 1340 1189 1468 1415 1451 1397 1598 1207 1165 1396 1327 1322 1434 1456 1332 1407 1149 1275 1165 1189 1226 1202 1165 1115 1224 1320 1416 1047 1187 1126 1191 1603 1356 1360 1368 1350 1446 1348 1373 1337 1426 1394 1590 1367 1316 1249 1219 1193 1214 870 1296 1200 1222 1321 1294 1204 1120 1225 1200 1233 1276 1376 1158 1063 1111 951 1229 1056 1139 1229 1310 1121 1015 1135 1128 1062 1302 1294 1151 1564 1291 1412 1512 1517 1815 1624 1576 1398 1549 1507 1434 1244 1579 1702 1305 1425 1221 1459 1245 1281 1502 1319 1473 1275 1212 1390 1246 1060 1356 1458 1296 1383 1397 1336 1284 1552 1443 1364 1529 1499 1226 1249 1656 1415 1530 1167 1474 1333 1298 1373 1311 1435 1524 1392 1445 1394 1324 1177 1703 1443 1551 1395 1415 1452 1105 1410 1439 1364 1538 1523 1333 1177 1266 1460 1496 1162 1408 1580 1231 1375 1149 1345 1141 1163 1269 1317 1230 1218 1150 1306 1254 1021 1277 1267 1170 1307 1197 1321 1351 1231 1391 1213 1387 1214 1376 1317 1521 1211 1151 1160 1133 1466 1349 1430 1363 1146 1388 1349 1283 1312 1323 1153 1527 1258 1250 1152 1313 1075 1046 1095 1012 1129 1284 1236 1000 1101 1044 1187 1186 1016 1207 1324 1080 1473 1201 1500 1378 1335 1559 1575 1397 1333 1397 1522 1496 1258 1508 1602 1343 1373 1405 1151 1480 1382 1446 1510 1585 1323 1322 1428 1325 1556 1552 1495 1410 1420 1408 1264 1424 1473 1540 1535 1760 1230 1295 1529 1416 1390 1678 1420 1409 1380 1166 1348 1132 1230 1391 1275 1210 1159 1283 1294 1281 1111 1234 1374 1193 1455 1282 1424 1292 1360 1591 1425 1358 1271 1308 1345 1363 1078 1423 1570 1157 1296 1247 1349 1152 1161 1302 1218 1384 1247 1238 1348 1371 1149 1123 1096 1120 1312 1160 1240 1111 1060 1221 1191 1173 1182 1182 1083 1399 1249 1090 1038 1127 1420 1512 1363 1426 1459 1425 1538 1647 1450 1392 1338 1865 1575 1796 1316 1480 1186 1196 1212 1293 1376 1359 1199 1426 1231 1218 1205 1425 1395 1518 1124 1380 1366 1227 1375 1240 1232 1288 1248 1413 1161 1295 1436 1437 1224 1218 1108 1173 1225 1272 1455 1214 1190 1230 1169 1298 1119 1181 1280 1447 1061 1227 1118 1124 1264 1369 1212 1218 1335 1387 1399 1428 1359 1239 1195 1700 1440 1657 1258 1285 1155 1222 1071 1130 1226 1238 1109 1285 1078 1115 1042 1385 1225 1312 1126 1218 1328 1191 1305 1326 1223 1377 1374 1266 1108 1130 1330 1279 1007 1333 1390 1175 1346 1175 1316 1345 1308 1549 1401 1325 1235 1373 1327 1282 1044 1441 1492 1259 1109 1130 1043 1096 1122 1182 1145 1183 1113 1073 1082 1381 1188 1208 1057 1112 1356 1273 1271 1321 1475 1488 1384 1451 1370 1417 1238 1648 1507 1574 1392 1384 1327 1179 1158 1224 1277 1200 1247 1485 1204 1125 1117 1508 1219 1500 1014 1330 1262 1295 1277 1314 1391 1370 1400 1546 1439 1351 1273 1613 1523 1626 1341 1418 1301 1088 1319 1202 1115 1430 1340 1217 1199 1071 1278 1343 1093 1420 1342 1206 1270 1044 1284 1128 1223 1350 1339 1261 1069 1183 1148 1316 990 1404 1420 1171 1184 1307 1151 1226 1346 1254 1246 1304 1211 1124 1258 1592 1267 1478 1123 1221 1402 1229 1098 1271 1413 1464 1538 1629 1351 1255 1358 1535 1359 1531 1154 1255 1324 1014 1443 1310 1337 1409 1530 1292 1145 1165 1447 1272 1002 1361 1469 1235 1256 1181 1330 1294 1212 1484 1263 1224 1266 1125 1207 1275 1010 1193 1328 1119 1209 1152 1085 1102 1191 1058 1160 1410 1048 1047 1149 1042 1130 1256 1126 1038 1465 1508 1243 1333 1567 1387 1484 1632 1360 1280 1333 1415 1351 1577 1642 1399 1368 1355 1235 1307 1432 1481 1446 1722 1179 1258 1341 1299 1466 1634 1598 1418 1198 1259 1136 1290 1232 1314 1270 1357 1222 1147 1356 1064 1043 1380 1219 1236 1358 1086 1303 1308 1065 1234 1239 1322 1251 1170 1252 1364 1203 1123 1080 1067 1263 1241 1446 1119 1307 1304 1156 1253 1203 1321 1222 1324 1197 1148 1014 994 1487 1479 1366 1500 1366 1579 1603 1632 1351 1489 1518 1276 1454 1816 1584 1548 1253 1301 1089 1356 1223 1270 1333 1434 1211 1229 1466 1302 1284 1438 1352 1342 1234 1308 1202 1202 1314 1169 1224 1341 1250 1187 1046 1514 1463 1331 1031 1238 1271 1214 1041 1094 1255 1338 1200 1434 1312 1210 1110 1524 1323 1400 1103 1290 1333 1119 1132 1380 1222 1353 1298 1333 1088 1233 1290 1132 1266 1300 1404 1301 1376 1496 1363 1485 1225 1418 1538 1596 1339 1299 1414 1360 1351 1703 1478 1394 1342 1031 1411 1364 1286 1484 1374 1305 1211 1328 1454 1176 1035 1391 1476 1190 1352 1072 1210 1196 1156 1362 1156 1209 1221 1079 1303 1289 1081 1301 1410 1114 1170 1187 1137 1134 1252 1255 1197 1295 1159 1237 1050 1493 1414 1427 1034 1464 1235 1142 1057 1070 1170 1214 1231 1324 1199 1244 1180 1472 1337 1498 1025 1220 1359 1325 1605 1281 1429 1378 1266 1452 1286 1390 1424 1543 1323 1280 1254 1175 1474 1203 1401 1191 1222 1174 1215 1386 1203 1211 1306 1402 1306 1248 1041 1006 1207 1194 1214 1282 1181 1394 1345 1397 1197 1233 1402 1078 1236 1569 1399 1333 1263 1389 1147 1462 1404 1355 1488 1418 1313 1230 1331 1351 1192 1424 1469 1288 1252 1092 1129 1400 1176 1337 1363 1445 1288 1150 1333 1138 1384 1389 1324 1223 1220 1180 1073 1277 1256 1219 1182 1317 1153 1179 1221 1186 1225 1373 1301 1232 1336 1205 1131 1215 1247 1237 1349 1293 1146 1303 1372 1341 1347 1440 1342 1337 1187 1318 1100 1312 1132 1367 1357 1346 1178 1088 1255 1242 1195 1238 1303 1303 1347 1103 1291 1229 1090 1380 1242 1439 1233 1322 1215 1372 1272 1196 1089 1179 1383 1285 1303 1342 1250 1281 1268 1396 1280 1351 1267 1462 1217 1249 1180 1172 1415 1135 1427 1399 1259 1502 1450 1303 1230 1240 1433 1365 1155 1470 1556 1299 1309 969 1304 1256 1283 1382 1411 1215 1255 1241 1308 1289 969 1375 1557 1231 1293 1205 1234 1239 1219 1357 1228 1359 1267 1281 1106 1334 1357 1389 1122 1260 1056 1170 1163 1030 1211 1202 1201 1302 1199 1039 1064 1605 1321 1462 1033 1227 1512 1359 1577 1351 1355 1458 1283 1257 1417 1376 1328 1590 1266 1295 1235 1337 1563 1296 1473 1477 1220 1373 1383 1428 1433 1446 1346 1622 1307 1239 1356 1339 1527 1323 1370 1205 1092 1337 1190 1384 1277 1255 1262 1493 1304 1282 1126 1250 1483 1422 1453 1441 1312 1434 1423 1437 1356 1401 1402 1594 1397 1331 1253 1223 1350 1272 1232 1197 1309 1324 1256 1438 1288 1314 1243 1693 1533 1576 1149 1438 1245 1220 1283 999 1282 1210 1225 1324 1346 1141 1069 1474 1414 1334 1200 1255 1209 1156 1350 1327 1237 1510 1411 1270 1234 1093 1413 1355 998 1479 1377 1238 1387 1218 1379 1346 1254 1437 1351 1306 1172 1224 1343 1321 1051 1326 1490 1209 1288 1293 1045 1380 1354 1307 1416 1385 1130 1276 1175 1276 1348 1526 1354 1349 1114 1213 1130 1173 1247 1191 1285 1362 1192 1123 1304 1240 1229 1499 1294 1276 1566 1252 1510 1282 1210 1395 1348 1434 1329 1358 1467 1424 1155 1478 1439 1330 1683 1329 1599 1511 1391 1613 1583 1525 1295 1447 1533 1393 1217 1513 1572 1433 1401 1164 1423 1384 1341 1477 1437 1257 1180 1130 1355 1336 1062 1316 1431 1227 1528 1252 1551 1486 1401 1674 1530 1553 1287 1400 1535 1381 1173 1513 1516 1316 1340 1408 1233 1410 1385 1391 1476 1518 1243 1328 1426 1305 1448 1671 1503 1361 1276 1285 1106 1473 1200 1303 1383 1367 1272 1036 1247 1185 1212 1400 1314 1199 1114 1189 1175 1197 1180 1352 1306 1327 1088 1250 1138 1478 1305 1517 1154 1336 1190 1201 1155 1288 1317 1347 1226 1428 1329 1360 1175 1494 1456 1435 1191 1322 1272 1306 1213 1241 1531 1333 1353 1494 1360 1204 1247 1621 1500 1623 1253 1373 1453 1532 1335 1422 1393 1405 1364 1573 1277 1260 1311 1667 1509 1744 1211 1418 1464 1306 1342 1298 1306 1274 1433 1386 1325 1344 1413 1483 1349 1284 1193 1017 1640 1284 1383 1402 1185 1467 1302 1489 1357 1361 1408 1646 1279 1217 1193 1301 1471 1266 1138 1095 1352 1352 1415 1394 1311 1349 1188 1483 1303 1479 1169 1227 1341 1417 1291 1284 1426 1453 1365 1622 1350 1216 1314 1689 1530 1670 1157 1444 1283 1188 1251 1232 1173 1378 1368 1494 1127 1207 1237 1226 1254 1441 1455 1360 1275 1382 1155 1301 1293 1177 1341 1463 1235 1215 1339 1267 1230 1368 1324 1169 1280 1167 1413 1257 1460 1686 1625 1283 1261 1306 1449 1322 1113 1517 1482 1241 1542 1152 1527 1325 1374 1469 1593 1335 1321 1485 1472 1386 1162 1441 1585 1380 1322 1309 1154 1475 1304 1441 1433 1393 1236 1208 1383 1246 1235 1448 1463 1240 1338 1478 1266 1485 1357 1492 1522 1651 1271 1459 1511 1428 1533 1657 1478 1416 1283 1421 1228 1419 1345 1397 1523 1521 1447 1189 1531 1282 1284 1532 1487 1388 1368 1417 1398 1649 1410 1549 1641 1605 1369 1349 1516 1383 1391 1612 1601 1470 1240 1387 1297 1166 1309 1307 1284 1460 1361 1247 1201 1698 1432 1374 1260 1410 1370 1334 1283 1342 1522 1285 1337 1439 1353 1313 1183 1507 1530 1521 1184 1388 1189 955 1178 1034 1076 1079 1010 1090 1015 1119 1162 1256 1049 933 869 825 1626 1487 1597 1472 1402 1515 1527 1624 1439 1453 1458 1677 1466 1517 1343 1251 1395 1301 1283 1344 1377 1387 1387 1576 1234 1197 1290 1265 1310 1399 1398 1305 1422 1576 1215 1508 1309 1430 1599 1536 1414 1359 1505 1351 1419 1563 1400 1334 1266 969 1064 1050 1171 1300 1161 1068 1096 954 1154 1135 852 1091 1277 894 1473 1247 1463 1489 1501 1826 1678 1405 1252 1518 1545 1536 1274 1589 1726 1393 973 1105 1007 970 1035 1096 1024 1190 1150 1118 901 1421 1160 1288 918 1008 1404 1470 1366 1335 1372 1496 1593 1669 1396 1450 1310 1825 1728 1543 1376 1494 1036 985 1035 1080 966 1124 1136 1279 1086 971 1103 1019 1112 1224 1133 1098 1573 1451 1491 1493 1601 1443 1643 1716 1442 1405 1428 1472 1515 1596 1689 1398 Time elapsed on matrix multiplication of 64x64 . 64x64 on GPU: 264.732635 ms.\n",
            "\n",
            "Time elapsed on matrix multiplication of 64x64 . 64x64 on CPU: 0.760288 ms.\n",
            "\n",
            "all results are correct!!!, speedup = 0.002872\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a9wb6lpkRLT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDCAU-lbmzKB",
        "outputId": "06d230eb-1a44-4798-9526-d6780ce2e633"
      },
      "source": [
        "%%cu\n",
        "//matrix addition\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <assert.h>\n",
        "\n",
        "#define BLOCK_SIZE_X 32\n",
        "#define BLOCK_SIZE_Y 32\n",
        "#define TILE_DIM 32\n",
        "#define BLOCK_ROWS 8\n",
        "__global__ void gpu_matrix_mult(int *a,int *b, int *c, int m, int n, int k)\n",
        "{\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    c[row * k + col] = a[row * k + col]+b[row * k + col];\n",
        "} \n",
        "__global__ void transposeNaive(int *odata, const int *idata)\n",
        "{\n",
        "  int x = blockIdx.x * TILE_DIM + threadIdx.x;\n",
        "  int y = blockIdx.y * TILE_DIM + threadIdx.y;\n",
        "  int width = gridDim.x * TILE_DIM;\n",
        "\n",
        "  for (int j = 0; j < TILE_DIM; j+= BLOCK_ROWS)\n",
        "    odata[x*width + (y+j)] = idata[(y+j)*width + x];\n",
        "}\n",
        "\n",
        "int main(int argc, char const *argv[])\n",
        "{\n",
        "    int m=8*1024, n=8*1024, k=8*1024;\n",
        " printf(\"a\");\n",
        "    srand(3333);\n",
        "\n",
        "   // int *h_a, *h_b, *h_c, *h_cc;\n",
        "   // h_a = (int *)malloc((sizeof(int)*m*n)) ;\n",
        "   // h_b = (int *)malloc((sizeof(int)*m*n)) ;\n",
        "   // h_c = (int *)malloc((sizeof(int)*m*n)) ;\n",
        "    int h_a[m][n];\n",
        " int h_b[m][n];\n",
        " int h_c[m][n];\n",
        " //int h_cc[m][n];\n",
        "\n",
        "    float gpu_elapsed_time_ms;\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    \n",
        "    int *d_a, *d_b, *d_c;\n",
        "    cudaMalloc(&d_a, sizeof(int)*m*n);\n",
        "    cudaMalloc(&d_b, sizeof(int)*n*k);\n",
        "    cudaMalloc(&d_c, sizeof(int)*m*k);\n",
        "\n",
        "    cudaMemcpy(d_a, h_a, sizeof(int)*m*n, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, h_b, sizeof(int)*n*k, cudaMemcpyHostToDevice);\n",
        " \n",
        "    unsigned int grid_cols = k/BLOCK_SIZE_X;\n",
        "    unsigned int grid_rows = m/BLOCK_SIZE_Y;\n",
        "    dim3 dimGrid(grid_cols, grid_rows);\n",
        "    dim3 dimBlock(32, 32);\n",
        " \n",
        "    cudaEventRecord(start, 0); \n",
        "    transposeNaive<<<dimGrid, dimBlock>>>(d_c, d_b);\n",
        "    cudaEventRecord(stop, 0);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&gpu_elapsed_time_ms, start, stop);\n",
        "    printf(\"Time elapsed on matrix multiplication of %dx%d . %dx%d on GPU: %f ms.\\n\\n\", m, n, n, k, gpu_elapsed_time_ms);\n",
        "\n",
        "    cudaMemcpy(h_c, d_c, sizeof(int)*m*k, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_c);\n",
        "    free(h_a);\n",
        "    free(h_b);\n",
        "    free(h_c);\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTYsP6xxuNgE",
        "outputId": "4d410704-37d0-4e21-9f3c-f0bbe9d8e39b"
      },
      "source": [
        "%%cu \n",
        "//sum of arrays values\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <assert.h>\n",
        "#define size 64\n",
        "#define BLOCK_SIZE 64\n",
        "\n",
        "__global__ void gpu_matrix_mult(int *a,int *b, int n)\n",
        "{\n",
        "    int col = blockIdx.x * blockDim.x;\n",
        "    int sum=0;\n",
        "    for (int i=0; i<size; i++){ \n",
        "        sum+=a[(col+threadIdx.x)*size+i]; //sum+=a[col*size+i*threadIdx.x]; \n",
        "    }\n",
        "    atomicAdd(b, sum);\n",
        "} \n",
        "\n",
        "int main(int argc, char const *argv[])\n",
        "{\n",
        "    int n=128*1024*1024;\n",
        " \n",
        "    int *h_a, *answer;\n",
        "    h_a = (int *)malloc((sizeof(int)*n)) ;\n",
        "    answer = (int *)malloc((sizeof(int)*1)) ;\n",
        "\n",
        "\n",
        "    float gpu_elapsed_time_ms;\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    \n",
        "    int *d_a, *answercuda;\n",
        "    cudaMalloc(&d_a, sizeof(int)*n);\n",
        "    cudaMalloc(&answercuda, sizeof(int)*1);\n",
        "\n",
        "    cudaMemcpy(d_a, h_a, sizeof(int)*n, cudaMemcpyHostToDevice);\n",
        " \n",
        "\n",
        "    unsigned int grid_cols = (n) / size / BLOCK_SIZE;\n",
        "    dim3 dimGrid(grid_cols);\n",
        "    dim3 dimBlock(BLOCK_SIZE);\n",
        " \n",
        "    cudaEventRecord(start, 0); \n",
        "    gpu_matrix_mult<<<dimGrid, dimBlock>>>(d_a, answercuda, n);\n",
        "    cudaEventRecord(stop, 0);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&gpu_elapsed_time_ms, start, stop);\n",
        "    printf(\"%f ms\\n\", gpu_elapsed_time_ms);\n",
        "\n",
        "    cudaMemcpy(answer, answercuda, sizeof(int)*1, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(answercuda);\n",
        "    free(h_a);\n",
        "    free(answer);\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11.649152 ms\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYOt7tccW7pR",
        "outputId": "cae85c4c-1ef2-4fdd-9149-21f551227de1"
      },
      "source": [
        "%%cu\n",
        "//matrix multiplication w/ shared memory govno\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <assert.h>\n",
        "\n",
        "#define BLOCK_SIZE 32 // 32x32\n",
        "#define TILE_WIDTH 32\n",
        "\n",
        "__global__ void gpu_matrix_mult(int *a,int *b, int *c, int m, int n, int k){ \n",
        "    \n",
        "    __shared__ int A[110][110];\n",
        "    __shared__ int B[111][111];\n",
        "\n",
        "    int bx = blockIdx.x;\n",
        "    int by = blockIdx.y;\n",
        "    int tx = threadIdx.x;\n",
        "    int ty = threadIdx.y;\n",
        "    int row = by * blockDim.y + ty;\n",
        "    int col = bx * blockDim.x + tx;\n",
        "\n",
        "    int Csub = 0;\n",
        "\n",
        "    for (int t=0; t<n/TILE_WIDTH; t++)\n",
        "    {\n",
        "      A[ty][tx] = a[row*n + t*TILE_WIDTH + tx];\n",
        "      B[ty][tx] = b[(t*TILE_WIDTH + ty)*k + col];\n",
        "      \n",
        "      __syncthreads();\n",
        "      for (int i=0; i<TILE_WIDTH; i++)\n",
        "      {\n",
        "          Csub += A[i][0];\n",
        "      }\n",
        "      __syncthreads();\n",
        "    }\n",
        "    c[row*k + col] = Csub;\n",
        "}\n",
        "\n",
        "\n",
        "void cpu_matrix_mult(int *h_a, int *h_b, int *h_result, int m, int n, int k) {\n",
        "    for (int i = 0; i < m; ++i) \n",
        "    {\n",
        "        for (int j = 0; j < k; ++j) \n",
        "        {\n",
        "            int tmp = 0.0;\n",
        "            for (int h = 0; h < n; ++h) \n",
        "            {\n",
        "                tmp += h_a[i * n + h] * h_b[h * k + j];\n",
        "            }\n",
        "            h_result[i * k + j] = tmp;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "int main(int argc, char const *argv[])\n",
        "{\n",
        "\n",
        "    int m=4*2048, n=m, k=m;\n",
        " \n",
        "    srand(3333);\n",
        "\n",
        "    // allocate memory in host RAM, h_cc is used to store CPU result\n",
        "    int *h_a, *h_b, *h_c, *h_cc;\n",
        "    h_a = (int *)malloc((sizeof(int)*m*n)) ;\n",
        "    h_b = (int *)malloc((sizeof(int)*m*n)) ;\n",
        "    h_c = (int *)malloc((sizeof(int)*m*n)) ;\n",
        "    h_cc = (int *)malloc((sizeof(int)*m*n)) ;\n",
        "    float gpu_elapsed_time_ms, cpu_elapsed_time_ms;\n",
        "\n",
        "    // some events to count the execution time\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&stop);\n",
        "cudaEventCreate(&start);\n",
        "    // start to count execution time of GPU version\n",
        "    // Allocate memory space on the device \n",
        "    int *d_a, *d_b, *d_c;\n",
        "    cudaMalloc(&d_a, sizeof(int)*m*n);\n",
        "    cudaMalloc(&d_b, sizeof(int)*n*k);\n",
        "    cudaMalloc(&d_c, sizeof(int)*m*k);\n",
        "\n",
        "    // copy matrix A and B from host to device memory\n",
        "    cudaMemcpy(d_a, h_a, sizeof(int)*m*n, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, h_b, sizeof(int)*n*k, cudaMemcpyHostToDevice);\n",
        "\n",
        "    unsigned int grid_rows = m/BLOCK_SIZE;\n",
        "    unsigned int grid_cols = k/BLOCK_SIZE;\n",
        "    dim3 dimGrid(grid_cols, grid_rows);\n",
        "    dim3 dimBlock(BLOCK_SIZE, BLOCK_SIZE);\n",
        "    \n",
        "    // Launch kernel \n",
        "   \n",
        "    cudaEventRecord(start, 0);\n",
        "    gpu_matrix_mult<<<dimGrid, dimBlock>>>(d_a, d_b, d_c, m, n, k);    \n",
        "\n",
        "    cudaEventRecord(stop,0);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    // compute time elapse on GPU computing\n",
        "    cudaEventElapsedTime(&gpu_elapsed_time_ms, start, stop);\n",
        "    printf(\"Time elapsed on matrix multiplication of %dx%d . %dx%d on GPU: %f ms.\\n\\n\", m, n, n, k, gpu_elapsed_time_ms);\n",
        "    cudaMemcpy(h_c, d_c, sizeof(int)*m*k, cudaMemcpyDeviceToHost);\n",
        "    // start the CPU version \n",
        "    cudaEventRecord(start, 0);\n",
        "    //cpu_matrix_mult(h_a, h_b, h_cc, m, n, k);\n",
        "\n",
        "    cudaEventRecord(stop, 0);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&cpu_elapsed_time_ms, start, stop);\n",
        "    printf(\"Time elapsed on matrix multiplication of %dx%d . %dx%d on CPU: %f ms.\\n\\n\", m, n, n, k, cpu_elapsed_time_ms);\n",
        "\n",
        "\n",
        "    // free memory\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_c);\n",
        "    free(h_a);\n",
        "    free(h_b);\n",
        "    free(h_c);\n",
        "    free(h_cc);\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time elapsed on matrix multiplication of 8192x8192 . 8192x8192 on GPU: 810.394287 ms.\n",
            "\n",
            "Time elapsed on matrix multiplication of 8192x8192 . 8192x8192 on CPU: 0.002688 ms.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miR-tD2bkSSO",
        "outputId": "9f1c9be2-e574-4185-dce3-ba42d6f26e8d"
      },
      "source": [
        "%%cu\n",
        "// test\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <assert.h>\n",
        "\n",
        "#define size 3\n",
        "#define BLOCK_SIZE 26\n",
        "#define TILE_WIDTH 78\n",
        "__global__ void gpu_matrix_mult(int *a,int *b, int *c, int m, int n, int k){ \n",
        "    \n",
        "    __shared__ int A[TILE_WIDTH][TILE_WIDTH];\n",
        "    __shared__ int B[TILE_WIDTH][TILE_WIDTH];\n",
        "\n",
        "    int bx = blockIdx.x;\n",
        "    int by = blockIdx.y;\n",
        "    int tx = threadIdx.x;\n",
        "    int ty = threadIdx.y;\n",
        "    int row = by * blockDim.y + ty;\n",
        "    int col = bx * blockDim.x + tx;\n",
        "\n",
        "    int sum[size*size];\n",
        "    for (int t=0; t<n/TILE_WIDTH; t++)\n",
        "    {\n",
        "      for (int j=0; j<size; j++){\n",
        "          for (int i=0; i<size; i++){\n",
        "              A[ty+j*BLOCK_SIZE][tx+i*BLOCK_SIZE] = a[by*TILE_WIDTH*n + t*TILE_WIDTH+ty*n+BLOCK_SIZE*n*j+BLOCK_SIZE*i+tx];\n",
        "              B[ty+j*BLOCK_SIZE][tx+i*BLOCK_SIZE] = b[t*TILE_WIDTH*n + bx*TILE_WIDTH+ty*n+BLOCK_SIZE*n*j+BLOCK_SIZE*i+tx];\n",
        "          }\n",
        "      }\n",
        "      __syncthreads();\n",
        "     \n",
        "      for (int i=0; i<TILE_WIDTH; i++){\n",
        "          int b_t[size];\n",
        "          int a_t[size];\n",
        "          for (int k=0; k<size; k++){\n",
        "              b_t[k]=B[i][size*tx+k];\n",
        "              a_t[k]=A[size*ty+k][i];\n",
        "          }\n",
        "          for (int q=0; q<size; q++){\n",
        "              for (int w=0; w<size; w++){\n",
        "                  sum[size*q+w]+=b_t[w]*a_t[q];\n",
        "              }\n",
        "          }\n",
        "      }\n",
        "      __syncthreads();\n",
        "    }\n",
        "    for (int j=0; j<size; j++){\n",
        "        for (int i=0; i<size; i++){\n",
        "            c[by*TILE_WIDTH*n+size*ty*n+bx*TILE_WIDTH+size*tx+i+n*j]=sum[size*j+i];\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "void cpu_matrix_mult(int *h_a, int *h_b, int *h_result, int m, int n, int k) {\n",
        "    for (int i = 0; i < m; ++i) \n",
        "    {\n",
        "        for (int j = 0; j < k; ++j) \n",
        "        {\n",
        "            int tmp = 0.0;\n",
        "            for (int h = 0; h < n; ++h) \n",
        "            {\n",
        "                tmp += h_a[i * n + h] * h_b[h * k + j];\n",
        "            }\n",
        "            h_result[i * k + j] = tmp;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "int main(int argc, char const *argv[])\n",
        "{\n",
        "    int m=8*1024;\n",
        "    int n=m;\n",
        "    int k=m;\n",
        " \n",
        "    srand(3333);\n",
        "\n",
        "    // allocate memory in host RAM, h_cc is used to store CPU result\n",
        "    int *h_a, *h_b, *h_c, *h_cc;\n",
        "    h_a = (int *)malloc((sizeof(int)*m*n)) ;\n",
        "    h_b = (int *)malloc((sizeof(int)*m*n)) ;\n",
        "    h_c = (int *)malloc((sizeof(int)*m*n)) ;\n",
        "    h_cc = (int *)malloc((sizeof(int)*m*n)) ;\n",
        "\n",
        "    // random initialize matrix A\n",
        "    for (int i = 0; i < m; ++i) {\n",
        "        for (int j = 0; j < n; ++j) {\n",
        "            h_a[i * n + j] = random()%10;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // random initialize matrix B\n",
        "    for (int i = 0; i < n; ++i) {\n",
        "        for (int j = 0; j < k; ++j) {\n",
        "            h_b[i * k + j] = random()%10;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    float gpu_elapsed_time_ms, cpu_elapsed_time_ms;\n",
        "\n",
        "    // some events to count the execution time\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    // start to count execution time of GPU version\n",
        "    // Allocate memory space on the device \n",
        "    int *d_a, *d_b, *d_c, *d_bt;\n",
        "    cudaMalloc(&d_a, sizeof(int)*m*n);\n",
        "    cudaMalloc(&d_b, sizeof(int)*n*k);\n",
        "    cudaMalloc(&d_c, sizeof(int)*m*k);\n",
        "    cudaMalloc(&d_bt, sizeof(int)*m*k);\n",
        " \n",
        "    cudaEventRecord(start, 0);\n",
        " \n",
        "    cudaMemcpy(d_a, h_a, sizeof(int)*m*n, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, h_b, sizeof(int)*n*k, cudaMemcpyHostToDevice);\n",
        "\n",
        "    unsigned int grid_rows = m/ BLOCK_SIZE/size;\n",
        "    unsigned int grid_cols = k/ BLOCK_SIZE/size;\n",
        "    dim3 dimGrid(grid_cols, grid_rows);\n",
        "    //dim3 dimBlock(32, 8);\n",
        "\n",
        "    dim3 dimGrid2(grid_cols, grid_rows);\n",
        "    dim3 dimBlock2(BLOCK_SIZE, BLOCK_SIZE); \n",
        " \n",
        "    //cudaEventRecord(start, 0);\n",
        "    gpu_matrix_mult<<<dimGrid2, dimBlock2>>>(d_a, d_b, d_c, m, n, k);\n",
        "    cudaDeviceSynchronize();\n",
        "    cudaMemcpy(h_c, d_c, sizeof(int)*m*k, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    cudaEventRecord(stop, 0);\n",
        "    cudaEventSynchronize(stop);\n",
        " \n",
        "\n",
        "    cudaEventElapsedTime(&gpu_elapsed_time_ms, start, stop);\n",
        "    // Transefer results from device to host\n",
        "    //cudaMemcpy(h_c, d_c, sizeof(int)*m*k, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // compute time elapse on GPU computing\n",
        "    printf(\"Time elapsed on matrix multiplication of %dx%d . %dx%d on GPU: %f ms.\\n\\n\", m, n, n, k, gpu_elapsed_time_ms);\n",
        "\n",
        "    // start the CPU version\n",
        "    cudaEventRecord(start, 0);\n",
        "\n",
        "    //cpu_matrix_mult(h_a, h_b, h_cc, m, n, k);\n",
        "\n",
        "    cudaEventRecord(stop, 0);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&cpu_elapsed_time_ms, start, stop);\n",
        "    //printf(\"Time elapsed on matrix multiplication of %dx%d . %dx%d on CPU: %f ms.\\n\\n\", m, n, n, k, cpu_elapsed_time_ms);\n",
        "\n",
        "    // free memory\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_c);\n",
        "    free(h_a);\n",
        "    free(h_b);\n",
        "    free(h_c);\n",
        "    free(h_cc);\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time elapsed on matrix multiplication of 8192x8192 . 8192x8192 on GPU: 839.951721 ms.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCOEBLJ9AtnU",
        "outputId": "22ad7b24-6b25-4530-d04f-370c9ecb37f0"
      },
      "source": [
        "%%writefile global.cu\n",
        "// final version\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <assert.h>\n",
        "\n",
        "#define ms 64\n",
        "\n",
        "#define size 4             // value per thread (square)\n",
        "#define BLOCK_SIZE 16      // thread per block (square)\n",
        "#define TILE_WIDTH 64\n",
        "\n",
        "__global__ void gpu_matrix_mult(float *a,float *b, float *c, int m, int n, int k){ \n",
        "    \n",
        "    __shared__ float A[TILE_WIDTH][TILE_WIDTH];\n",
        "    __shared__ float B[TILE_WIDTH][TILE_WIDTH];\n",
        "\n",
        "    int bx = blockIdx.x;\n",
        "    int by = blockIdx.y;\n",
        "    int tx = threadIdx.x;\n",
        "    int ty = threadIdx.y;\n",
        "\n",
        "    printf(\"Wadwd\");\n",
        "    float sum[size*size];\n",
        "    for (int t=0; t<n/TILE_WIDTH; t++)\n",
        "    {\n",
        "      for (int j=0; j<size; j++){\n",
        "          for (int i=0; i<size; i++){\n",
        "              A[ty+j*BLOCK_SIZE][tx+i*BLOCK_SIZE] = a[by*TILE_WIDTH*n + t*TILE_WIDTH+ty*n+BLOCK_SIZE*n*j+BLOCK_SIZE*i+tx];\n",
        "              B[ty+j*BLOCK_SIZE][tx+i*BLOCK_SIZE] = b[t*TILE_WIDTH*n + bx*TILE_WIDTH+ty*n+BLOCK_SIZE*n*j+BLOCK_SIZE*i+tx];\n",
        "          }\n",
        "      }\n",
        "      __syncthreads();\n",
        "      for (int i=0; i<TILE_WIDTH; i++){\n",
        "          float b_t[size];\n",
        "          float a_t[size];\n",
        "          for (int k=0; k<size; k++){\n",
        "              b_t[k]=B[i][size*tx+k];\n",
        "              a_t[k]=A[size*ty+k][i];\n",
        "          }\n",
        "          for (int q=0; q<size; q++){\n",
        "              for (int w=0; w<size; w++){\n",
        "                  sum[size*q+w]+=b_t[w]*a_t[q];\n",
        "              }\n",
        "          }\n",
        "      }\n",
        "      __syncthreads();\n",
        "    }\n",
        "    for (int j=0; j<size; j++){\n",
        "        for (int i=0; i<size; i++){\n",
        "            c[by*TILE_WIDTH*n+bx*TILE_WIDTH+size*ty*n+size*tx+i+n*j]=sum[size*j+i];\n",
        "            //c[by*TILE_WIDTH*n+bx*TILE_WIDTH+j*size*n+ty*n+tx]=sum[size*j+i];\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "void cpu_matrix_mult(float *h_a, float *h_b, float *h_result, int m, int n, int k) {\n",
        "    for (int i = 0; i < m; ++i) \n",
        "    {\n",
        "        for (int j = 0; j < k; ++j) \n",
        "        {\n",
        "            int tmp = 0.0;\n",
        "            for (int h = 0; h < n; ++h) \n",
        "            {\n",
        "                tmp += h_a[i * n + h] * h_b[h * k + j];\n",
        "            }\n",
        "            h_result[i * k + j] = tmp;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "int main(int argc, char const *argv[])\n",
        "{\n",
        "    int m=ms;\n",
        "    int n=m;\n",
        "    int k=m;\n",
        " \n",
        "    srand(3333);\n",
        "\n",
        "    // allocate memory in host RAM, h_cc is used to store CPU result\n",
        "    float *h_a, *h_b, *h_c, *h_cc;\n",
        "    h_a = (float *)malloc((sizeof(float)*m*n)) ;\n",
        "    h_b = (float *)malloc((sizeof(float)*m*n)) ;\n",
        "    h_c = (float *)malloc((sizeof(float)*m*n)) ;\n",
        "    h_cc = (float *)malloc((sizeof(float)*m*n)) ;\n",
        "\n",
        "    // random initialize matrix A\n",
        "    for (int i = 0; i < m; ++i) {\n",
        "        for (int j = 0; j < n; ++j) {\n",
        "            h_a[i * n + j] = random()%10;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // random initialize matrix B\n",
        "    for (int i = 0; i < n; ++i) {\n",
        "        for (int j = 0; j < k; ++j) {\n",
        "            h_b[i * k + j] = random()%10;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    float gpu_elapsed_time_ms, cpu_elapsed_time_ms;\n",
        "\n",
        "    // some events to count the execution time\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    // start to count execution time of GPU version\n",
        "    // Allocate memory space on the device \n",
        "    float *d_a, *d_b, *d_c, *d_bt;\n",
        "    cudaMalloc(&d_a, sizeof(float)*m*n);\n",
        "    cudaMalloc(&d_b, sizeof(float)*n*k);\n",
        "    cudaMalloc(&d_c, sizeof(float)*m*k);\n",
        "    cudaMalloc(&d_bt, sizeof(float)*m*k);\n",
        " \n",
        "    //cudaEventRecord(start, 0);\n",
        " \n",
        "    cudaMemcpy(d_a, h_a, sizeof(float)*m*n, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, h_b, sizeof(float)*n*k, cudaMemcpyHostToDevice);\n",
        "\n",
        "    unsigned int grid_rows = m/ BLOCK_SIZE/size;\n",
        "    unsigned int grid_cols = k/ BLOCK_SIZE/size;\n",
        "    dim3 dimGrid(grid_cols, grid_rows);\n",
        "    //dim3 dimBlock(32, 8);\n",
        "\n",
        "    dim3 dimGrid2(grid_cols, grid_rows);\n",
        "    dim3 dimBlock2(BLOCK_SIZE, BLOCK_SIZE); \n",
        " \n",
        "    cudaEventRecord(start, 0);\n",
        "    gpu_matrix_mult<<<dimGrid2, dimBlock2>>>(d_a, d_b, d_c, m, n, k);\n",
        "    cudaDeviceSynchronize();\n",
        "    cudaEventRecord(stop, 0);\n",
        "    cudaMemcpy(h_c, d_c, sizeof(float)*m*k, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    cudaEventSynchronize(stop);\n",
        " \n",
        "\n",
        "    cudaEventElapsedTime(&gpu_elapsed_time_ms, start, stop);\n",
        "    // Transefer results from device to host\n",
        "    //cudaMemcpy(h_c, d_c, sizeof(float)*m*k, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // compute time elapse on GPU computing\n",
        "    printf(\"Time elapsed on matrix multiplication of %dx%d . %dx%d on GPU: %f ms.\\n\\n\", m, n, n, k, gpu_elapsed_time_ms);\n",
        "\n",
        "    // start the CPU version\n",
        "    cudaEventRecord(start, 0);\n",
        "\n",
        "    cpu_matrix_mult(h_a, h_b, h_cc, m, n, k);\n",
        "\n",
        "    cudaEventRecord(stop, 0);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&cpu_elapsed_time_ms, start, stop);\n",
        "    //printf(\"Time elapsed on matrix multiplication of %dx%d . %dx%d on CPU: %f ms.\\n\\n\", m, n, n, k, cpu_elapsed_time_ms);\n",
        " int all_ok = 1;\n",
        "    for (int i = 0; i < m; ++i)\n",
        "    {\n",
        "        for (int j = 0; j < k; ++j)\n",
        "        {\n",
        "            //printf(\"\\n --%f--, --%f-- \\n\", h_cc[i*k + j], h_c[i*k + j]);\n",
        "            //printf(\"[%d][%d]:%d == [%d][%d]:%d, \", i, j, h_cc[i*k + j], i, j, h_c[i*k + j]);\n",
        "            if(h_cc[i*k + j] != h_c[i*k + j])\n",
        "            {\n",
        "                all_ok = 0;\n",
        "             \n",
        "            }\n",
        "        }\n",
        "        //printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    // roughly compute speedup\n",
        "    if(all_ok)\n",
        "    {\n",
        "        printf(\"all results are correct!!!, speedup = %f\\n\", cpu_elapsed_time_ms / gpu_elapsed_time_ms);\n",
        "    }\n",
        "    else\n",
        "    {\n",
        "        printf(\"incorrect results\\n\");\n",
        "    }\n",
        "\n",
        "    // free memory\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_c);\n",
        "    free(h_a);\n",
        "    free(h_b);\n",
        "    free(h_c);\n",
        "    free(h_cc);\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting global.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrRq7d25Xu0t"
      },
      "source": [
        "!nvcc -o global global.cu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-j1Y-hpjXwkA",
        "outputId": "b3892a5f-0ccb-488b-a8a9-143ea0937d97"
      },
      "source": [
        "!./global"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time elapsed on matrix multiplication of 64x64 . 64x64 on GPU: 0.011648 ms.\n",
            "\n",
            "incorrect results\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKqZxUJbHQYU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1de25227-6cef-4bfe-cbe1-ad899e440169"
      },
      "source": [
        "%%cu\n",
        "// paper version\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <assert.h>\n",
        "\n",
        "#define ms 1024\n",
        "\n",
        "#define size 4             // value per thread (square)\n",
        "#define BLOCK_SIZE 32      // thread per block (square)\n",
        "#define TILE_WIDTH 64\n",
        "\n",
        "__global__ void gpu_matrix_mult(float *C,float *A, float *B, int wA, int wB){ \n",
        "    \n",
        "    int bx = blockIdx.x;\n",
        "    int by = blockIdx.y;\n",
        "    int tx = threadIdx.x;\n",
        "    int ty = threadIdx.y;\n",
        "    int aBegin = wA*BLOCK_SIZE-1;\n",
        "    int aEnd = aBegin + wA-1;\n",
        "    int aStep = BLOCK_SIZE;\n",
        "    int bBegin = BLOCK_SIZE*bx;\n",
        "    int bStep = BLOCK_SIZE*wB;\n",
        "    float Csub = 0;\n",
        "\n",
        "    for (int a = aBegin, b=bBegin; a<=aEnd; a+=aStep, b+=bStep){\n",
        "        __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];\n",
        "        __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];\n",
        "        As[ty][tx] = A[a+wA*ty+tx];\n",
        "        Bs[ty][tx] = B[b+wB*ty+tx];\n",
        "        __syncthreads();\n",
        "        for (int k=0; k<BLOCK_SIZE; k++){\n",
        "            Csub+=As[ty][k]*Bs[k][tx];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "    int c = wB*BLOCK_SIZE*by+BLOCK_SIZE*bx;\n",
        "    C[c+wB*ty+tx]=Csub;\n",
        "}\n",
        "\n",
        "\n",
        "int main(int argc, char const *argv[])\n",
        "{\n",
        "    int m=ms;\n",
        "    int n=m;\n",
        "    int k=m;\n",
        " \n",
        "    srand(3333);\n",
        "\n",
        "    // allocate memory in host RAM, h_cc is used to store CPU result\n",
        "    float *h_a, *h_b, *h_c, *h_cc;\n",
        "    h_a = (float *)malloc((sizeof(float)*m*n)) ;\n",
        "    h_b = (float *)malloc((sizeof(float)*m*n)) ;\n",
        "    h_c = (float *)malloc((sizeof(float)*m*n)) ;\n",
        "    h_cc = (float *)malloc((sizeof(float)*m*n)) ;\n",
        "\n",
        "    // random initialize matrix A\n",
        "    for (int i = 0; i < m; ++i) {\n",
        "        for (int j = 0; j < n; ++j) {\n",
        "            h_a[i * n + j] = random()%10;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // random initialize matrix B\n",
        "    for (int i = 0; i < n; ++i) {\n",
        "        for (int j = 0; j < k; ++j) {\n",
        "            h_b[i * k + j] = random()%10;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    float gpu_elapsed_time_ms, cpu_elapsed_time_ms;\n",
        "\n",
        "    // some events to count the execution time\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    // start to count execution time of GPU version\n",
        "    // Allocate memory space on the device \n",
        "    float *d_a, *d_b, *d_c, *d_bt;\n",
        "    cudaMalloc(&d_a, sizeof(float)*m*n);\n",
        "    cudaMalloc(&d_b, sizeof(float)*n*k);\n",
        "    cudaMalloc(&d_c, sizeof(float)*m*k);\n",
        "    cudaMalloc(&d_bt, sizeof(float)*m*k);\n",
        " \n",
        "    //cudaEventRecord(start, 0);\n",
        " \n",
        "    cudaMemcpy(d_a, h_a, sizeof(float)*m*n, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, h_b, sizeof(float)*n*k, cudaMemcpyHostToDevice);\n",
        "\n",
        "    unsigned int grid_rows = m/ BLOCK_SIZE;\n",
        "    unsigned int grid_cols = k/ BLOCK_SIZE;\n",
        "    dim3 dimGrid(grid_cols, grid_rows);\n",
        "    //dim3 dimBlock(32, 8);\n",
        "\n",
        "    dim3 dimGrid2(grid_cols, grid_rows);\n",
        "    dim3 dimBlock2(BLOCK_SIZE, BLOCK_SIZE); \n",
        " \n",
        "    cudaEventRecord(start, 0);\n",
        "    gpu_matrix_mult<<<dimGrid2, dimBlock2>>>(d_c, d_a, d_b, m, n);\n",
        "    cudaDeviceSynchronize();\n",
        "    cudaEventRecord(stop, 0);\n",
        "    cudaMemcpy(h_c, d_c, sizeof(float)*m*k, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    cudaEventSynchronize(stop);\n",
        " \n",
        "\n",
        "    cudaEventElapsedTime(&gpu_elapsed_time_ms, start, stop);\n",
        "    // Transefer results from device to host\n",
        "    //cudaMemcpy(h_c, d_c, sizeof(float)*m*k, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // compute time elapse on GPU computing\n",
        "    printf(\"Time elapsed on matrix multiplication of %dx%d . %dx%d on GPU: %f ms.\\n\\n\", m, n, n, k, gpu_elapsed_time_ms);\n",
        "\n",
        "    // start the CPU version\n",
        "    cudaEventRecord(start, 0);\n",
        "\n",
        "    //cpu_matrix_mult(h_a, h_b, h_cc, m, n, k);\n",
        "\n",
        "    cudaEventRecord(stop, 0);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&cpu_elapsed_time_ms, start, stop);\n",
        "    //printf(\"Time elapsed on matrix multiplication of %dx%d . %dx%d on CPU: %f ms.\\n\\n\", m, n, n, k, cpu_elapsed_time_ms);\n",
        "\n",
        "    // free memory\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_c);\n",
        "    free(h_a);\n",
        "    free(h_b);\n",
        "    free(h_c);\n",
        "    free(h_cc);\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UsageError: Cell magic `%%cu` not found.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DA19KVUXzm6l",
        "outputId": "615e3dbe-f2a1-46de-a58a-ff19dbda5620"
      },
      "source": [
        "%%cu\n",
        "// fermi\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <assert.h>\n",
        "\n",
        "#define ms 8192\n",
        "\n",
        "#define size 4             // value per thread (square)\n",
        "#define BLOCK_SIZE 16      // thread per block (square)\n",
        "#define TILE_WIDTH 64\n",
        "\n",
        "__global__ void gpu_matrix_mult(float *a,float *b, float *c, int m, int n, int k){ \n",
        "    \n",
        "    __shared__ float A[TILE_WIDTH][TILE_WIDTH/4];\n",
        "    __shared__ float B[TILE_WIDTH/4][TILE_WIDTH];\n",
        "\n",
        "    int bx = blockIdx.x;\n",
        "    int by = blockIdx.y;\n",
        "    int tx = threadIdx.x;\n",
        "    int ty = threadIdx.y;\n",
        "    int row = by * blockDim.y + ty;\n",
        "    int col = bx * blockDim.x + tx;\n",
        "\n",
        "    float sum[size*size]={0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};\n",
        "    for (int t=0; t<n/16; t++)\n",
        "    {\n",
        "      for (int j=0; j<size; j++){\n",
        "          //A[ty+j*BLOCK_SIZE][tx+i*BLOCK_SIZE] = a[by*TILE_WIDTH*n + t*TILE_WIDTH+ty*n+BLOCK_SIZE*n*j+BLOCK_SIZE*i+tx];\n",
        "          //B[ty+j*BLOCK_SIZE][tx+i*BLOCK_SIZE] = b[t*TILE_WIDTH*n + bx*TILE_WIDTH+ty*n+BLOCK_SIZE*n*j+BLOCK_SIZE*i+tx];\n",
        "          B[ty][j*16+tx] = b[bx*TILE_WIDTH+t*16*n+ty*n+tx+j*16];\n",
        "          A[j*16+ty][tx] = a[by*TILE_WIDTH*n+t*16+j*16*n+ty*n+tx];\n",
        "          //printf(\"%f\", B[ty][j*16+tx]);\n",
        "          //printf(\"%f\", A[j*16+ty][tx]);\n",
        "      }\n",
        "      __syncthreads();\n",
        "      for (int i=0; i<TILE_WIDTH/size; i++){\n",
        "          float b_t[size];\n",
        "          float a_t[size];\n",
        "          for (int k=0; k<size; k++){\n",
        "              b_t[k]=B[i][(TILE_WIDTH/size)*k+tx];\n",
        "              a_t[k]=A[(TILE_WIDTH/size)*k+ty][i];\n",
        "          }\n",
        "          for (int q=0; q<size; q++){\n",
        "              for (int w=0; w<size; w++){\n",
        "                  sum[size*q+w]+=b_t[w]*a_t[q];\n",
        "                  //printf(\"%f %f %f %f\\n\", p, sum[size*q+w], b_t[w], a_t[q]);\n",
        "              }\n",
        "          }\n",
        "      }\n",
        "      __syncthreads();\n",
        "    }\n",
        "    for (int j=0; j<size; j++){\n",
        "        for (int i=0; i<size; i++){\n",
        "            //c[by*TILE_WIDTH*n+size*ty*n+bx*TILE_WIDTH+size*tx+i+n*j]=sum[size*j+i];\n",
        "            c[by*TILE_WIDTH*n+bx*TILE_WIDTH+j*size*n+ty*n+tx]=sum[size*j+i];\n",
        "            //printf(\"%f \", sum[size*j+i]);\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "int main(int argc, char const *argv[])\n",
        "{\n",
        "    int m=ms;\n",
        "    int n=m;\n",
        "    int k=m;\n",
        " \n",
        "    srand(3333);\n",
        "\n",
        "    // allocate memory in host RAM, h_cc is used to store CPU result\n",
        "    float *h_a, *h_b, *h_c, *h_cc;\n",
        "    h_a = (float *)malloc((sizeof(float)*m*n)) ;\n",
        "    h_b = (float *)malloc((sizeof(float)*m*n)) ;\n",
        "    h_c = (float *)malloc((sizeof(float)*m*n)) ;\n",
        "    h_cc = (float *)malloc((sizeof(float)*m*n)) ;\n",
        "\n",
        "    // random initialize matrix A\n",
        "    for (int i = 0; i < m; ++i) {\n",
        "        for (int j = 0; j < n; ++j) {\n",
        "            h_a[i * n + j] = random();\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // random initialize matrix B\n",
        "    for (int i = 0; i < n; ++i) {\n",
        "        for (int j = 0; j < k; ++j) {\n",
        "            h_b[i * k + j] = random();\n",
        "        }\n",
        "    }\n",
        "\n",
        "    float gpu_elapsed_time_ms, cpu_elapsed_time_ms;\n",
        "\n",
        "    // some events to count the execution time\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    // start to count execution time of GPU version\n",
        "    // Allocate memory space on the device \n",
        "    float *d_a, *d_b, *d_c;\n",
        "    cudaMalloc(&d_a, sizeof(float)*m*n);\n",
        "    cudaMalloc(&d_b, sizeof(float)*n*k);\n",
        "    cudaMalloc(&d_c, sizeof(float)*m*k);\n",
        " \n",
        "    //cudaEventRecord(start, 0);\n",
        " \n",
        "    cudaMemcpy(d_a, h_a, sizeof(float)*m*n, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, h_b, sizeof(float)*n*k, cudaMemcpyHostToDevice);\n",
        "\n",
        "    unsigned int grid_rows = m/64;\n",
        "    unsigned int grid_cols = k/64;\n",
        "    dim3 dimGrid(grid_cols, grid_rows);\n",
        "\n",
        "    dim3 dimGrid2(grid_cols, grid_rows);\n",
        "    dim3 dimBlock2(16, 16); \n",
        " \n",
        "    cudaEventRecord(start, 0);\n",
        "    gpu_matrix_mult<<<dimGrid2, dimBlock2>>>(d_a, d_b, d_c, m, n, k);\n",
        "    cudaDeviceSynchronize();\n",
        "    cudaEventRecord(stop, 0);\n",
        "    cudaMemcpy(h_c, d_c, sizeof(float)*m*k, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    cudaEventSynchronize(stop);\n",
        " \n",
        "\n",
        "    cudaEventElapsedTime(&gpu_elapsed_time_ms, start, stop);\n",
        "    // Transefer results from device to host\n",
        "    //cudaMemcpy(h_c, d_c, sizeof(float)*m*k, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // compute time elapse on GPU computing\n",
        "    printf(\"Time elapsed on matrix multiplication of %dx%d . %dx%d on GPU: %f ms.\\n\\n\", m, n, n, k, gpu_elapsed_time_ms);\n",
        "\n",
        "    // start the CPU version\n",
        "    cudaEventRecord(start, 0);\n",
        "\n",
        "    //cpu_matrix_mult(h_a, h_b, h_cc, m, n, k);\n",
        "\n",
        "    cudaEventRecord(stop, 0);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&cpu_elapsed_time_ms, start, stop);\n",
        "    //printf(\"Time elapsed on matrix multiplication of %dx%d . %dx%d on CPU: %f ms.\\n\\n\", m, n, n, k, cpu_elapsed_time_ms);\n",
        "\n",
        "    // free memory\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_c);\n",
        "    free(h_a);\n",
        "    free(h_b);\n",
        "    free(h_c);\n",
        "    free(h_cc);\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time elapsed on matrix multiplication of 8192x8192 . 8192x8192 on GPU: 354.814789 ms.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HJbf32_I1CS",
        "outputId": "8e9c76e4-eef2-48f9-9ac9-3de20e07b09a"
      },
      "source": [
        "%%writefile fermi.cu\n",
        "// fermi\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <assert.h>\n",
        "\n",
        "#define ms 8192\n",
        "\n",
        "#define size 4             // value per thread (square)\n",
        "#define BLOCK_SIZE 16      // thread per block (square)\n",
        "#define TILE_WIDTH 64\n",
        "\n",
        "__global__ void gpu_matrix_mult(float *a,float *b, float *c, int m, int n, int k){ \n",
        "    \n",
        "    __shared__ float A[TILE_WIDTH][TILE_WIDTH/4];\n",
        "    __shared__ float B[TILE_WIDTH/4][TILE_WIDTH];\n",
        "\n",
        "    int bx = blockIdx.x;\n",
        "    int by = blockIdx.y;\n",
        "    int tx = threadIdx.x;\n",
        "    int ty = threadIdx.y;\n",
        "\n",
        "\n",
        "    float sum[size*size]={0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};\n",
        "    for (int t=0; t<n/16; t++)\n",
        "    {\n",
        "      for (int j=0; j<size; j++){\n",
        "          //A[ty+j*BLOCK_SIZE][tx+i*BLOCK_SIZE] = a[by*TILE_WIDTH*n + t*TILE_WIDTH+ty*n+BLOCK_SIZE*n*j+BLOCK_SIZE*i+tx];\n",
        "          //B[ty+j*BLOCK_SIZE][tx+i*BLOCK_SIZE] = b[t*TILE_WIDTH*n + bx*TILE_WIDTH+ty*n+BLOCK_SIZE*n*j+BLOCK_SIZE*i+tx];\n",
        "          B[ty][j*16+tx] = b[bx*TILE_WIDTH+t*16*n+ty*n+tx+j*16];\n",
        "          A[j*16+ty][tx] = a[by*TILE_WIDTH*n+t*16+j*16*n+ty*n+tx];\n",
        "          //printf(\"%f\", B[ty][j*16+tx]);\n",
        "          //printf(\"%f\", A[j*16+ty][tx]);\n",
        "      }\n",
        "      __syncthreads();\n",
        "      for (int i=0; i<TILE_WIDTH/size; i++){\n",
        "          float b_t[size];\n",
        "          float a_t[size];\n",
        "          for (int k=0; k<size; k++){\n",
        "              b_t[k]=B[i][(TILE_WIDTH/size)*k+tx];\n",
        "              a_t[k]=A[(TILE_WIDTH/size)*k+ty][i];\n",
        "          }\n",
        "          for (int q=0; q<size; q++){\n",
        "              for (int w=0; w<size; w++){\n",
        "                  sum[size*q+w]+=b_t[w]*a_t[q];\n",
        "                  //printf(\"%f %f %f %f\\n\", p, sum[size*q+w], b_t[w], a_t[q]);\n",
        "              }\n",
        "          }\n",
        "      }\n",
        "      __syncthreads();\n",
        "    }\n",
        "    for (int j=0; j<size; j++){\n",
        "        for (int i=0; i<size; i++){\n",
        "            //c[by*TILE_WIDTH*n+size*ty*n+bx*TILE_WIDTH+size*tx+i+n*j]=sum[size*j+i];\n",
        "            c[by*TILE_WIDTH*n+bx*TILE_WIDTH+j*size*n+ty*n+tx]=sum[size*j+i];\n",
        "            //printf(\"%f \", sum[size*j+i]);\n",
        "        }\n",
        "    }\n",
        "}\n",
        "void cpu_matrix_mult(float *h_a, float *h_b, float *h_result, int m, int n, int k) {\n",
        "    for (int i = 0; i < m; ++i) \n",
        "    {\n",
        "        for (int j = 0; j < k; ++j) \n",
        "        {\n",
        "            int tmp = 0.0;\n",
        "            for (int h = 0; h < n; ++h) \n",
        "            {\n",
        "                tmp += h_a[i * n + h] * h_b[h * k + j];\n",
        "            }\n",
        "            h_result[i * k + j] = tmp;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(int argc, char const *argv[])\n",
        "{\n",
        "    int m=ms;\n",
        "    int n=m;\n",
        "    int k=m;\n",
        " \n",
        "    srand(3333);\n",
        "\n",
        "    // allocate memory in host RAM, h_cc is used to store CPU result\n",
        "    float *h_a, *h_b, *h_c, *h_cc;\n",
        "    h_a = (float *)malloc((sizeof(float)*m*n)) ;\n",
        "    h_b = (float *)malloc((sizeof(float)*m*n)) ;\n",
        "    h_c = (float *)malloc((sizeof(float)*m*n)) ;\n",
        "    h_cc = (float *)malloc((sizeof(float)*m*n)) ;\n",
        "\n",
        "    // random initialize matrix A\n",
        "    for (int i = 0; i < m; ++i) {\n",
        "        for (int j = 0; j < n; ++j) {\n",
        "            h_a[i * n + j] = random();\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // random initialize matrix B\n",
        "    for (int i = 0; i < n; ++i) {\n",
        "        for (int j = 0; j < k; ++j) {\n",
        "            h_b[i * k + j] = random();\n",
        "        }\n",
        "    }\n",
        "\n",
        "    float gpu_elapsed_time_ms, cpu_elapsed_time_ms;\n",
        "\n",
        "    // some events to count the execution time\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    // start to count execution time of GPU version\n",
        "    // Allocate memory space on the device \n",
        "    float *d_a, *d_b, *d_c;\n",
        "    cudaMalloc(&d_a, sizeof(float)*m*n);\n",
        "    cudaMalloc(&d_b, sizeof(float)*n*k);\n",
        "    cudaMalloc(&d_c, sizeof(float)*m*k);\n",
        " \n",
        "    //cudaEventRecord(start, 0);\n",
        " \n",
        "    cudaMemcpy(d_a, h_a, sizeof(float)*m*n, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, h_b, sizeof(float)*n*k, cudaMemcpyHostToDevice);\n",
        "\n",
        "    unsigned int grid_rows = m/64;\n",
        "    unsigned int grid_cols = k/64;\n",
        "    dim3 dimGrid(grid_cols, grid_rows);\n",
        "\n",
        "    dim3 dimGrid2(grid_cols, grid_rows);\n",
        "    dim3 dimBlock2(16, 16); \n",
        " \n",
        "    cudaEventRecord(start, 0);\n",
        "    gpu_matrix_mult<<<dimGrid2, dimBlock2>>>(d_a, d_b, d_c, m, n, k);\n",
        "    cudaDeviceSynchronize();\n",
        "    cudaEventRecord(stop, 0);\n",
        "    cudaMemcpy(h_c, d_c, sizeof(float)*m*k, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    cudaEventSynchronize(stop);\n",
        " \n",
        "\n",
        "    cudaEventElapsedTime(&gpu_elapsed_time_ms, start, stop);\n",
        "    // Transefer results from device to host\n",
        "    //cudaMemcpy(h_c, d_c, sizeof(float)*m*k, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // compute time elapse on GPU computing\n",
        "    printf(\"Time elapsed on matrix multiplication of %dx%d . %dx%d on GPU: %f ms.\\n\\n\", m, n, n, k, gpu_elapsed_time_ms);\n",
        "\n",
        "    // start the CPU version\n",
        "    cudaEventRecord(start, 0);\n",
        "\n",
        "    //cpu_matrix_mult(h_a, h_b, h_cc, m, n, k);\n",
        "\n",
        "    cudaEventRecord(stop, 0);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&cpu_elapsed_time_ms, start, stop);\n",
        "    //printf(\"Time elapsed on matrix multiplication of %dx%d . %dx%d on CPU: %f ms.\\n\\n\", m, n, n, k, cpu_elapsed_time_ms);\n",
        "    int all_ok = 1;\n",
        "    for (int i = 0; i < m; ++i)\n",
        "    {\n",
        "        for (int j = 0; j < k; ++j)\n",
        "        {\n",
        "            //printf(\"\\n --%f--, --%f-- \\n\", h_cc[i*k + j], h_c[i*k + j]);\n",
        "            //printf(\"[%d][%d]:%d == [%d][%d]:%d, \", i, j, h_cc[i*k + j], i, j, h_c[i*k + j]);\n",
        "            if(h_cc[i*k + j] != h_c[i*k + j])\n",
        "            {\n",
        "                all_ok = 0;\n",
        "            }\n",
        "        }\n",
        "        //printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    // roughly compute speedup\n",
        "    if(all_ok)\n",
        "    {\n",
        "        printf(\"all results are correct!!!, speedup = %f\\n\", cpu_elapsed_time_ms / gpu_elapsed_time_ms);\n",
        "    }\n",
        "    else\n",
        "    {\n",
        "        printf(\"incorrect results\\n\");\n",
        "    }\n",
        "\n",
        "    // free memory\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_c);\n",
        "    free(h_a);\n",
        "    free(h_b);\n",
        "    free(h_c);\n",
        "    free(h_cc);\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting fermi.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LO-QKkjEI3vM"
      },
      "source": [
        "!nvcc -o fermi fermi.cu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLJeLrJaI570",
        "outputId": "822b14d6-c765-4a96-f457-79c641f63f2d"
      },
      "source": [
        "!./fermi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time elapsed on matrix multiplication of 8192x8192 . 8192x8192 on GPU: 365.946350 ms.\n",
            "\n",
            "incorrect results\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfKBzfrBJZ6i"
      },
      "source": [
        "!/usr/local/cuda-10.0/NsightCompute-1.0/nv-nsight-cu-cli ./fermi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xt_8IMsjD0-I",
        "outputId": "410af6e7-c400-420d-99d5-1edef9b40979"
      },
      "source": [
        "import torch\n",
        "print('__CUDA Device Name:',torch.cuda.get_device_name(0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__CUDA Device Name: Tesla K80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8I-w1-g9wn0",
        "outputId": "4aca45c7-2a5c-435b-cee3-d740e6cf227c"
      },
      "source": [
        "%%writefile 16x8_128x128_v3.cu\n",
        "// fermi\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <assert.h>\n",
        "#define check_with_cpu 0\n",
        "#define ms 8192\n",
        "#define bs_x 16\n",
        "#define bs_y 8\n",
        "#define TILE_WIDTH_Y 128\n",
        "#define TILE_WIDTH_X 128\n",
        "\n",
        "__global__ void gpu_matrix_mult(float *a,float *b, float *c, int m, int n, int k){ \n",
        "    \n",
        "    __shared__ float A[128][16];\n",
        "    __shared__ float B[16][128];\n",
        "\n",
        "    int bx = blockIdx.x;\n",
        "    int by = blockIdx.y;\n",
        "    int tx = threadIdx.x;\n",
        "    int ty = threadIdx.y;\n",
        "\n",
        "    float sum[128]={0};\n",
        "    for (int t=0; t<n/16; t++)\n",
        "    {\n",
        "      for (int i=0; i<2; i++){\n",
        "          for (int j=0; j<8; j++){\n",
        "              B[ty+8*i][tx+16*j] = b[bx*TILE_WIDTH_X+t*16*n+(ty+8*i)*n+tx+16*j];\n",
        "          }\n",
        "      }\n",
        "      for (int j=0; j<16; j++){\n",
        "          A[ty+j*8][tx] = a[by*TILE_WIDTH_Y*n+t*16+(ty+j*8)*n+tx];\n",
        "      }\n",
        "      __syncthreads();\n",
        "      for (int i=0; i<16; i++){\n",
        "          float b_t[8];\n",
        "          float a_t[16];\n",
        "          for (int k=0; k<8; k++){\n",
        "              b_t[k]=B[i][16*k+tx];\n",
        "          }\n",
        "          for (int k=0; k<16; k++){\n",
        "              a_t[k]=A[8*k+ty][i];\n",
        "          }\n",
        "          for (int q=0; q<16; q++){\n",
        "              for (int w=0; w<8; w++){\n",
        "                  sum[8*q+w]+=b_t[w]*a_t[q];\n",
        "                  //printf(\"%f %f\", b_t[w], a_t[q]);\n",
        "              }\n",
        "          }\n",
        "      }\n",
        "      __syncthreads();\n",
        "    }\n",
        "    for (int j=0; j<16; j++){\n",
        "        for (int i=0; i<8; i++){\n",
        "            c[by*TILE_WIDTH_Y*n+bx*TILE_WIDTH_X+j*bs_y*n+bs_x*i+ty*n+tx]=sum[8*j+i];\n",
        "            //printf(\"%f \", sum[8*j+i]);\n",
        "        }\n",
        "    }\n",
        "}\n",
        "void cpu_matrix_mult(float *h_a, float *h_b, float *h_result, int m, int n, int k) {\n",
        "    for (int i = 0; i < m; ++i) \n",
        "    {\n",
        "        for (int j = 0; j < k; ++j) \n",
        "        {\n",
        "            float tmp = 0.0;\n",
        "            for (int h = 0; h < n; ++h) \n",
        "            {\n",
        "                tmp += h_a[i * n + h] * h_b[h * k + j];\n",
        "            }\n",
        "            h_result[i * k + j] = tmp;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(int argc, char const *argv[])\n",
        "{\n",
        "   \n",
        "    int m=ms;\n",
        "    int n=m;\n",
        "    int k=m;\n",
        " \n",
        "    srand(3333);\n",
        "\n",
        "    // allocate memory in host RAM, h_cc is used to store CPU result\n",
        "    float *h_a, *h_b, *h_c, *h_cc;\n",
        "    h_a = (float *)malloc((sizeof(float)*m*n)) ;\n",
        "    h_b = (float *)malloc((sizeof(float)*m*n)) ;\n",
        "    h_c = (float *)malloc((sizeof(float)*m*n)) ;\n",
        "    h_cc = (float *)malloc((sizeof(float)*m*n)) ;\n",
        "\n",
        "    // random initialize matrix A\n",
        "    for (int i = 0; i < m; ++i) {\n",
        "        for (int j = 0; j < n; ++j) {\n",
        "            h_a[i * n + j] =random()%5;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // random initialize matrix B\n",
        "    for (int i = 0; i < n; ++i) {\n",
        "        for (int j = 0; j < k; ++j) {\n",
        "            h_b[i * k + j] = random()%5;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    float gpu_elapsed_time_ms, cpu_elapsed_time_ms;\n",
        "\n",
        "    // some events to count the execution time\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    // start to count execution time of GPU version\n",
        "    // Allocate memory space on the device \n",
        "    float *d_a, *d_b, *d_c;\n",
        "    cudaMalloc(&d_a, sizeof(float)*m*n);\n",
        "    cudaMalloc(&d_b, sizeof(float)*n*k);\n",
        "    cudaMalloc(&d_c, sizeof(float)*m*k);\n",
        " \n",
        "    cudaEventRecord(start, 0);\n",
        " \n",
        "    cudaMemcpy(d_a, h_a, sizeof(float)*m*n, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, h_b, sizeof(float)*n*k, cudaMemcpyHostToDevice);\n",
        "\n",
        "    unsigned int grid_rows = m/TILE_WIDTH_Y;\n",
        "    unsigned int grid_cols = k/TILE_WIDTH_X;\n",
        "  \n",
        "\n",
        "    dim3 dimGrid2(grid_cols, grid_rows);\n",
        "    dim3 dimBlock2(16, 8); \n",
        " \n",
        "    cudaEventRecord(start, 0);\n",
        "    gpu_matrix_mult<<<dimGrid2, dimBlock2>>>(d_a, d_b, d_c, m, n, k);\n",
        "    cudaDeviceSynchronize();\n",
        "    cudaEventRecord(stop, 0);\n",
        "    cudaMemcpy(h_c, d_c, sizeof(float)*m*k, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    cudaEventSynchronize(stop);\n",
        " \n",
        "\n",
        "    cudaEventElapsedTime(&gpu_elapsed_time_ms, start, stop);\n",
        "    // Transefer results from device to host\n",
        "    //cudaMemcpy(h_c, d_c, sizeof(float)*m*k, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // compute time elapse on GPU computing\n",
        "    printf(\"Time elapsed on matrix multiplication of %dx%d . %dx%d on GPU: %f ms.\\n\\n\", m, n, n, k, gpu_elapsed_time_ms);\n",
        "\n",
        "    // start the CPU version\n",
        "    cudaEventRecord(start, 0);\n",
        "  \n",
        "    if (check_with_cpu==1){\n",
        "    cpu_matrix_mult(h_a, h_b, h_cc, m, n, k);\n",
        "\n",
        "    cudaEventRecord(stop, 0);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&cpu_elapsed_time_ms, start, stop);\n",
        "    //printf(\"Time elapsed on matrix multiplication of %dx%d . %dx%d on CPU: %f ms.\\n\\n\", m, n, n, k, cpu_elapsed_time_ms);\n",
        "    int all_ok = 1;\n",
        "    for (int i = 0; i < m; ++i)\n",
        "    {\n",
        "        for (int j = 0; j < k; ++j)\n",
        "        {\n",
        "            //printf(\"\\n --%f--, --%f-- \\n\", h_cc[i*k + j], h_c[i*k + j]);\n",
        "            //printf(\"[%f][%f]:%f == [%f][%f]:%f, \", i, j, h_cc[i*k + j], i, j, h_c[i*k + j]);\n",
        "            if(h_cc[i*k + j] != h_c[i*k + j])\n",
        "            {\n",
        "                all_ok = 0;\n",
        "            }\n",
        "        }\n",
        "        //printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    // roughly compute speedup\n",
        "    if(all_ok)\n",
        "    {\n",
        "        printf(\"all results are correct!!!, speedup = %f\\n\", cpu_elapsed_time_ms / gpu_elapsed_time_ms);\n",
        "    }\n",
        "    else\n",
        "    {\n",
        "        printf(\"incorrect results\\n\");\n",
        "    }\n",
        "    }\n",
        "    // free memory\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_c);\n",
        "    free(h_a);\n",
        "    free(h_b);\n",
        "    free(h_c);\n",
        "    free(h_cc);\n",
        "    \n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 16x8_128x128_v3.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgEgQpHc9znS"
      },
      "source": [
        "!nvcc -o 16x8_128x128_v3 16x8_128x128_v3.cu "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ns34tRF29zgs",
        "outputId": "7168f678-7ebb-4f03-c489-82eea8a975c9"
      },
      "source": [
        "!nvprof ./16x8_128x128_v3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==197== NVPROF is profiling process 197, command: ./16x8_128x128_v3\n",
            "==197== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "Time elapsed on matrix multiplication of 8192x8192 . 8192x8192 on GPU: 0.012320 ms.\n",
            "\n",
            "==197== Profiling application: ./16x8_128x128_v3\n",
            "==197== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   68.10%  154.82ms         1  154.82ms  154.82ms  154.82ms  [CUDA memcpy DtoH]\n",
            "                   31.90%  72.529ms         2  36.264ms  36.229ms  36.299ms  [CUDA memcpy HtoD]\n",
            "      API calls:   45.87%  228.58ms         3  76.193ms  36.324ms  155.92ms  cudaMemcpy\n",
            "                   39.85%  198.54ms         2  99.271ms  1.8730us  198.54ms  cudaEventCreate\n",
            "                   13.81%  68.788ms         3  22.929ms  456.32us  34.258ms  cudaFree\n",
            "                    0.29%  1.4576ms         3  485.86us  445.59us  529.40us  cudaMalloc\n",
            "                    0.09%  466.05us         1  466.05us  466.05us  466.05us  cuDeviceTotalMem\n",
            "                    0.04%  194.91us       101  1.9290us     135ns  83.404us  cuDeviceGetAttribute\n",
            "                    0.03%  146.36us         1  146.36us  146.36us  146.36us  cudaDeviceSynchronize\n",
            "                    0.01%  41.462us         1  41.462us  41.462us  41.462us  cuDeviceGetName\n",
            "                    0.01%  40.665us         4  10.166us  1.6730us  22.209us  cudaEventRecord\n",
            "                    0.00%  5.7970us         1  5.7970us  5.7970us  5.7970us  cuDeviceGetPCIBusId\n",
            "                    0.00%  4.8890us         1  4.8890us  4.8890us  4.8890us  cudaEventSynchronize\n",
            "                    0.00%  4.0010us         1  4.0010us  4.0010us  4.0010us  cudaEventElapsedTime\n",
            "                    0.00%  2.0060us         3     668ns     198ns  1.0320us  cuDeviceGetCount\n",
            "                    0.00%  1.2810us         1  1.2810us  1.2810us  1.2810us  cudaLaunchKernel\n",
            "                    0.00%  1.2470us         2     623ns     361ns     886ns  cuDeviceGet\n",
            "                    0.00%     292ns         1     292ns     292ns     292ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    }
  ]
}